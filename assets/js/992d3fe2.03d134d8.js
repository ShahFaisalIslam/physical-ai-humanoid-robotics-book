"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[6021],{2682(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-2-digital-twin/sensor-processing","title":"Sensor Data Processing in Simulation","description":"Processing sensor data in simulation is a critical aspect of robotic systems. While simulated sensors provide idealized data, processing this data effectively requires understanding the characteristics and limitations of both the sensors and the simulated environment.","source":"@site/docs/module-2-digital-twin/sensor-processing.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/sensor-processing","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2-digital-twin/sensor-processing","draft":false,"unlisted":false,"editUrl":"https://github.com/your-organization/physical-ai-humanoid-robotics-book/edit/main/book/docs/module-2-digital-twin/sensor-processing.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"sidebar_position":10}}');var a=s(4848),t=s(8453);const o={sidebar_position:10},r="Sensor Data Processing in Simulation",l={},c=[{value:"Overview of Sensor Data Processing",id:"overview-of-sensor-data-processing",level:2},{value:"Processing Camera Data",id:"processing-camera-data",level:2},{value:"Image Subscription and Processing",id:"image-subscription-and-processing",level:3},{value:"Depth Image Processing",id:"depth-image-processing",level:3},{value:"Processing LiDAR Data",id:"processing-lidar-data",level:2},{value:"LaserScan Message Processing",id:"laserscan-message-processing",level:3},{value:"Point Cloud Processing",id:"point-cloud-processing",level:3},{value:"Processing IMU Data",id:"processing-imu-data",level:2},{value:"Sensor Fusion",id:"sensor-fusion",level:2},{value:"Combining Multiple Sensors",id:"combining-multiple-sensors",level:3},{value:"Filtering and Noise Handling",id:"filtering-and-noise-handling",level:2},{value:"Basic Filtering for Sensor Data",id:"basic-filtering-for-sensor-data",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Efficient Data Processing",id:"efficient-data-processing",level:3},{value:"Memory Management",id:"memory-management",level:3},{value:"Validation Techniques",id:"validation-techniques",level:2},{value:"Comparing Simulated vs. Expected Data",id:"comparing-simulated-vs-expected-data",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"sensor-data-processing-in-simulation",children:"Sensor Data Processing in Simulation"})}),"\n",(0,a.jsx)(n.p,{children:"Processing sensor data in simulation is a critical aspect of robotic systems. While simulated sensors provide idealized data, processing this data effectively requires understanding the characteristics and limitations of both the sensors and the simulated environment."}),"\n",(0,a.jsx)(n.h2,{id:"overview-of-sensor-data-processing",children:"Overview of Sensor Data Processing"}),"\n",(0,a.jsx)(n.p,{children:"Sensor data processing in simulation involves:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Receiving sensor messages from simulated sensors"}),"\n",(0,a.jsx)(n.li,{children:"Filtering and interpreting the data"}),"\n",(0,a.jsx)(n.li,{children:"Using the processed data for perception, navigation, or control"}),"\n",(0,a.jsx)(n.li,{children:"Validating results against expected simulation behavior"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"processing-camera-data",children:"Processing Camera Data"}),"\n",(0,a.jsx)(n.h3,{id:"image-subscription-and-processing",children:"Image Subscription and Processing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass CameraProcessor(Node):\n    def __init__(self):\n        super().__init__('camera_processor')\n        self.subscription = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10)\n        self.bridge = CvBridge()\n        \n    def image_callback(self, msg):\n        # Convert ROS Image message to OpenCV image\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n        \n        # Process the image (example: edge detection)\n        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n        edges = cv2.Canny(gray, 50, 150)\n        \n        # Publish or use processed data\n        self.process_edges(edges)\n        \n    def process_edges(self, edges):\n        # Find contours in the edge image\n        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        # Process contours (e.g., find largest contour)\n        if contours:\n            largest_contour = max(contours, key=cv2.contourArea)\n            # Calculate bounding box\n            x, y, w, h = cv2.boundingRect(largest_contour)\n            self.get_logger().info(f'Object detected at: ({x}, {y}) with size ({w}, {h})')\n"})}),"\n",(0,a.jsx)(n.h3,{id:"depth-image-processing",children:"Depth Image Processing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from sensor_msgs.msg import Image\nimport numpy as np\n\nclass DepthProcessor(Node):\n    def __init__(self):\n        super().__init__('depth_processor')\n        self.subscription = self.create_subscription(\n            Image,\n            '/depth_camera/depth/image_raw',\n            self.depth_callback,\n            10)\n        self.bridge = CvBridge()\n        \n    def depth_callback(self, msg):\n        # Convert depth image to numpy array\n        depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='32FC1')\n        \n        # Find depth at center of image\n        height, width = depth_image.shape\n        center_depth = depth_image[height//2, width//2]\n        \n        if not np.isnan(center_depth) and center_depth > 0:\n            self.get_logger().info(f'Distance to object at center: {center_depth:.2f}m')\n            \n        # Calculate depth statistics\n        valid_depths = depth_image[np.isfinite(depth_image) & (depth_image > 0)]\n        if valid_depths.size > 0:\n            avg_depth = np.mean(valid_depths)\n            min_depth = np.min(valid_depths)\n            max_depth = np.max(valid_depths)\n            \n            self.get_logger().info(\n                f'Depth stats - Avg: {avg_depth:.2f}m, '\n                f'Min: {min_depth:.2f}m, Max: {max_depth:.2f}m'\n            )\n"})}),"\n",(0,a.jsx)(n.h2,{id:"processing-lidar-data",children:"Processing LiDAR Data"}),"\n",(0,a.jsx)(n.h3,{id:"laserscan-message-processing",children:"LaserScan Message Processing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from sensor_msgs.msg import LaserScan\nimport numpy as np\n\nclass LiDARProcessor(Node):\n    def __init__(self):\n        super().__init__('lidar_processor')\n        self.subscription = self.create_subscription(\n            LaserScan,\n            '/scan',\n            self.scan_callback,\n            10)\n        \n    def scan_callback(self, msg):\n        # Convert ranges to numpy array\n        ranges = np.array(msg.ranges)\n        \n        # Filter out invalid ranges (inf, nan)\n        valid_indices = np.isfinite(ranges) & (ranges > msg.range_min) & (ranges < msg.range_max)\n        valid_ranges = ranges[valid_indices]\n        valid_angles = np.linspace(msg.angle_min, msg.angle_max, len(ranges))[valid_indices]\n        \n        if len(valid_ranges) > 0:\n            # Find closest obstacle\n            min_distance_idx = np.argmin(valid_ranges)\n            min_distance = valid_ranges[min_distance_idx]\n            min_angle = valid_angles[min_angle_idx]\n            \n            self.get_logger().info(\n                f'Closest obstacle: {min_distance:.2f}m at angle {min_angle:.2f}rad'\n            )\n            \n            # Detect obstacles in front of robot (e.g., within 30 degrees)\n            front_mask = (valid_angles >= -np.pi/6) & (valid_angles <= np.pi/6)\n            front_ranges = valid_ranges[front_mask]\n            \n            if len(front_ranges) > 0:\n                min_front_distance = np.min(front_ranges)\n                if min_front_distance < 1.0:  # Less than 1m\n                    self.get_logger().warn('Obstacle detected in front! Distance: {:.2f}m'.format(min_front_distance))\n"})}),"\n",(0,a.jsx)(n.h3,{id:"point-cloud-processing",children:"Point Cloud Processing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from sensor_msgs.msg import PointCloud2\nfrom sensor_msgs_py import point_cloud2\nimport numpy as np\n\nclass PointCloudProcessor(Node):\n    def __init__(self):\n        super().__init__('pointcloud_processor')\n        self.subscription = self.create_subscription(\n            PointCloud2,\n            '/velodyne_points',\n            self.pointcloud_callback,\n            10)\n        \n    def pointcloud_callback(self, msg):\n        # Convert PointCloud2 to list of points\n        points = point_cloud2.read_points(msg, field_names=(\"x\", \"y\", \"z\"), skip_nans=True)\n        \n        # Convert to numpy array\n        points_array = np.array(list(points))\n        \n        if len(points_array) > 0:\n            # Calculate bounding box\n            min_vals = np.min(points_array, axis=0)\n            max_vals = np.max(points_array, axis=0)\n            center = (min_vals + max_vals) / 2\n            \n            self.get_logger().info(\n                f'Point cloud bounds: X({min_vals[0]:.2f}, {max_vals[0]:.2f}), '\n                f'Y({min_vals[1]:.2f}, {max_vals[1]:.2f}), '\n                f'Z({min_vals[2]:.2f}, {max_vals[2]:.2f})'\n            )\n            \n            # Cluster detection (simplified approach)\n            self.detect_clusters(points_array)\n    \n    def detect_clusters(self, points):\n        # Simple clustering based on z-height\n        ground_level = np.mean(points[:, 2]) - np.std(points[:, 2])\n        ground_points = points[points[:, 2] < ground_level + 0.1]\n        obstacle_points = points[points[:, 2] >= ground_level + 0.1]\n        \n        self.get_logger().info(\n            f'Ground points: {len(ground_points)}, '\n            f'Obstacle points: {len(obstacle_points)}'\n        )\n"})}),"\n",(0,a.jsx)(n.h2,{id:"processing-imu-data",children:"Processing IMU Data"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from sensor_msgs.msg import Imu\nimport numpy as np\n\nclass IMUProcessor(Node):\n    def __init__(self):\n        super().__init__('imu_processor')\n        self.subscription = self.create_subscription(\n            Imu,\n            '/imu/data',\n            self.imu_callback,\n            10)\n        \n    def imu_callback(self, msg):\n        # Extract orientation (quaternion)\n        orientation = msg.orientation\n        # Convert quaternion to Euler angles (simplified)\n        euler = self.quaternion_to_euler(orientation.x, orientation.y, orientation.z, orientation.w)\n        \n        # Extract angular velocity\n        angular_velocity = msg.angular_velocity\n        \n        # Extract linear acceleration\n        linear_accel = msg.linear_acceleration\n        \n        self.get_logger().info(\n            f'Orientation: Roll={euler[0]:.2f}, Pitch={euler[1]:.2f}, Yaw={euler[2]:.2f}\\n'\n            f'Angular Vel: X={angular_velocity.x:.2f}, Y={angular_velocity.y:.2f}, Z={angular_velocity.z:.2f}\\n'\n            f'Linear Acc: X={linear_accel.x:.2f}, Y={linear_accel.y:.2f}, Z={linear_accel.z:.2f}'\n        )\n        \n    def quaternion_to_euler(self, x, y, z, w):\n        # Simplified conversion (not accounting for all edge cases)\n        t0 = 2.0 * (w * x + y * z)\n        t1 = 1.0 - 2.0 * (x * x + y * y)\n        roll = np.arctan2(t0, t1)\n        \n        t2 = 2.0 * (w * y - z * x)\n        t2 = np.clip(t2, -1.0, 1.0)\n        pitch = np.arcsin(t2)\n        \n        t3 = 2.0 * (w * z + x * y)\n        t4 = 1.0 - 2.0 * (y * y + z * z)\n        yaw = np.arctan2(t3, t4)\n        \n        return [roll, pitch, yaw]\n"})}),"\n",(0,a.jsx)(n.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,a.jsx)(n.h3,{id:"combining-multiple-sensors",children:"Combining Multiple Sensors"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Imu\nfrom geometry_msgs.msg import PoseStamped\nimport numpy as np\n\nclass SensorFusionNode(Node):\n    def __init__(self):\n        super().__init__('sensor_fusion')\n        \n        # Subscriptions\n        self.scan_sub = self.create_subscription(LaserScan, '/scan', self.scan_callback, 10)\n        self.imu_sub = self.create_subscription(Imu, '/imu/data', self.imu_callback, 10)\n        \n        # Publisher\n        self.obstacle_pub = self.create_publisher(PoseStamped, '/obstacle_pose', 10)\n        \n        # State variables\n        self.latest_imu = None\n        self.latest_scan = None\n        \n    def scan_callback(self, msg):\n        self.latest_scan = msg\n        self.process_fusion()\n        \n    def imu_callback(self, msg):\n        self.latest_imu = msg\n        self.process_fusion()\n        \n    def process_fusion(self):\n        if self.latest_scan is None or self.latest_imu is None:\n            return\n            \n        # Find closest obstacle from LiDAR\n        ranges = np.array(self.latest_scan.ranges)\n        valid_ranges = np.isfinite(ranges) & (ranges > 0)\n        \n        if np.any(valid_ranges):\n            min_idx = np.argmin(ranges[valid_ranges])\n            min_range = ranges[valid_ranges][min_idx]\n            \n            # Calculate angle of closest obstacle\n            angle_increment = self.latest_scan.angle_increment\n            obstacle_angle = self.latest_scan.angle_min + min_idx * angle_increment\n            \n            # Use IMU orientation to transform to global frame\n            imu_orientation = self.latest_imu.orientation\n            euler = self.quaternion_to_euler(\n                imu_orientation.x, imu_orientation.y, \n                imu_orientation.z, imu_orientation.w\n            )\n            \n            # Calculate global position of obstacle\n            # (Simplified - in practice, you'd use full TF transforms)\n            global_angle = obstacle_angle + euler[2]  # Add yaw from IMU\n            obstacle_x = min_range * np.cos(global_angle)\n            obstacle_y = min_range * np.sin(global_angle)\n            \n            # Publish obstacle position\n            obstacle_pose = PoseStamped()\n            obstacle_pose.header.stamp = self.get_clock().now().to_msg()\n            obstacle_pose.header.frame_id = 'map'\n            obstacle_pose.pose.position.x = obstacle_x\n            obstacle_pose.pose.position.y = obstacle_y\n            obstacle_pose.pose.position.z = 0.0\n            \n            self.obstacle_pub.publish(obstacle_pose)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"filtering-and-noise-handling",children:"Filtering and Noise Handling"}),"\n",(0,a.jsx)(n.h3,{id:"basic-filtering-for-sensor-data",children:"Basic Filtering for Sensor Data"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class SensorFilter:\n    def __init__(self, window_size=5):\n        self.window_size = window_size\n        self.values = []\n        \n    def update(self, value):\n        self.values.append(value)\n        if len(self.values) > self.window_size:\n            self.values.pop(0)\n        \n        return sum(self.values) / len(self.values)  # Simple moving average\n    \n    def get_filtered_value(self):\n        if not self.values:\n            return None\n        return sum(self.values) / len(self.values)\n\n# Example usage in a sensor processor\nclass FilteredSensorProcessor(Node):\n    def __init__(self):\n        super().__init__('filtered_sensor_processor')\n        self.distance_filter = SensorFilter(window_size=5)\n        \n    def scan_callback(self, msg):\n        # Get minimum distance from scan\n        ranges = np.array(msg.ranges)\n        valid_ranges = ranges[np.isfinite(ranges) & (ranges > 0)]\n        \n        if len(valid_ranges) > 0:\n            min_distance = np.min(valid_ranges)\n            filtered_distance = self.distance_filter.update(min_distance)\n            \n            if filtered_distance is not None:\n                self.get_logger().info(f'Raw min distance: {min_distance:.2f}, Filtered: {filtered_distance:.2f}')\n"})}),"\n",(0,a.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,a.jsx)(n.h3,{id:"efficient-data-processing",children:"Efficient Data Processing"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Use appropriate data types"}),": Use numpy arrays for numerical computations"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Batch processing"}),": Process multiple data points together when possible"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Threading"}),": Use separate threads for computationally intensive tasks"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Message filtering"}),": Process only necessary messages at required rates"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"memory-management",children:"Memory Management"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Limit stored data to prevent memory issues\nclass RingBuffer:\n    def __init__(self, size):\n        self.size = size\n        self.buffer = []\n        \n    def add(self, item):\n        self.buffer.append(item)\n        if len(self.buffer) > self.size:\n            self.buffer.pop(0)\n            \n    def get_all(self):\n        return self.buffer.copy()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"validation-techniques",children:"Validation Techniques"}),"\n",(0,a.jsx)(n.h3,{id:"comparing-simulated-vs-expected-data",children:"Comparing Simulated vs. Expected Data"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class SensorValidator:\n    def __init__(self):\n        self.expected_values = {}\n        self.tolerance = 0.1  # Tolerance for validation\n        \n    def validate_camera_data(self, image, expected_objects):\n        # Process image to detect objects\n        # Compare with expected objects\n        detected_objects = self.detect_objects(image)\n        \n        for expected_obj in expected_objects:\n            found = False\n            for detected_obj in detected_objects:\n                if self.objects_match(expected_obj, detected_obj):\n                    found = True\n                    break\n            \n            if not found:\n                print(f"Expected object {expected_obj} not found in simulation")\n    \n    def objects_match(self, obj1, obj2):\n        # Implement logic to determine if objects match\n        # This would depend on your specific object representation\n        pass\n'})}),"\n",(0,a.jsx)(n.p,{children:"Sensor data processing in simulation requires understanding both the characteristics of the simulated sensors and the algorithms needed to extract meaningful information from the data. Proper processing enables robots to effectively perceive and interact with their simulated environment."})]})}function g(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453(e,n,s){s.d(n,{R:()=>o,x:()=>r});var i=s(6540);const a={},t=i.createContext(a);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);