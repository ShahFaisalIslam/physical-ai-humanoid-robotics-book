"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[2985],{4740(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-4-vision-language-action/llm-cognitive-planning-examples","title":"LLM Cognitive Planning Code Examples","description":"Practical code examples for implementing LLM-based cognitive planning in robotics","source":"@site/docs/module-4-vision-language-action/llm-cognitive-planning-examples.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/llm-cognitive-planning-examples","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/llm-cognitive-planning-examples","draft":false,"unlisted":false,"editUrl":"https://github.com/your-organization/physical-ai-humanoid-robotics-book/edit/main/book/docs/module-4-vision-language-action/llm-cognitive-planning-examples.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"LLM Cognitive Planning Code Examples","description":"Practical code examples for implementing LLM-based cognitive planning in robotics","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"OpenAI Whisper Integration Code Examples","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/openai-whisper-examples"},"next":{"title":"Exercises for Cognitive Planning","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/exercises-cognitive-planning"}}');var o=t(4848),s=t(8453);const r={title:"LLM Cognitive Planning Code Examples",description:"Practical code examples for implementing LLM-based cognitive planning in robotics",sidebar_position:5},i="LLM Cognitive Planning Code Examples",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Basic LLM Integration",id:"basic-llm-integration",level:2},{value:"Simple LLM Planner Node",id:"simple-llm-planner-node",level:3},{value:"Advanced LLM Integration with Context",id:"advanced-llm-integration-with-context",level:2},{value:"Context-Aware LLM Planner",id:"context-aware-llm-planner",level:3},{value:"Local LLM Integration",id:"local-llm-integration",level:2},{value:"Running Open-Source LLMs Locally",id:"running-open-source-llms-locally",level:3},{value:"Multi-Modal LLM Integration",id:"multi-modal-llm-integration",level:2},{value:"Combining Vision and Language for Complex Tasks",id:"combining-vision-and-language-for-complex-tasks",level:3},{value:"Error Handling and Fallback Strategies",id:"error-handling-and-fallback-strategies",level:2},{value:"Robust LLM Planner with Fallbacks",id:"robust-llm-planner-with-fallbacks",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"llm-cognitive-planning-code-examples",children:"LLM Cognitive Planning Code Examples"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"This section provides practical code examples for implementing Large Language Model (LLM) based cognitive planning in robotics applications. These examples demonstrate how to use LLMs to interpret natural language commands and generate executable robotic plans."}),"\n",(0,o.jsx)(n.h2,{id:"basic-llm-integration",children:"Basic LLM Integration"}),"\n",(0,o.jsx)(n.h3,{id:"simple-llm-planner-node",children:"Simple LLM Planner Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nimport openai\nimport json\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\n\nclass SimpleLLMPlanner(Node):\n    def __init__(self):\n        super().__init__(\'simple_llm_planner\')\n        \n        # Set OpenAI API key (in practice, use environment variables)\n        openai.api_key = "YOUR_API_KEY_HERE"\n        \n        # Subscriptions\n        self.command_sub = self.create_subscription(\n            String,\n            \'natural_language_command\',\n            self.command_callback,\n            10\n        )\n        \n        # Publishers\n        self.action_pub = self.create_publisher(String, \'robot_action\', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        \n        # Robot state\n        self.robot_state = {\n            \'position\': {\'x\': 0, \'y\': 0, \'theta\': 0},\n            \'battery_level\': 100,\n            \'attached_object\': None,\n            \'environment\': \'indoor_office\'\n        }\n        \n        self.get_logger().info(\'Simple LLM Planner initialized\')\n\n    def command_callback(self, msg):\n        """Process natural language command and generate robot actions"""\n        command = msg.data\n        self.get_logger().info(f\'Received command: {command}\')\n        \n        # Generate plan using LLM\n        plan = self.generate_plan(command)\n        \n        if plan:\n            # Execute the plan\n            self.execute_plan(plan)\n        else:\n            self.get_logger().warn(\'Could not generate a plan for the command\')\n\n    def generate_plan(self, command):\n        """Generate a step-by-step plan using an LLM"""\n        prompt = f"""\n        You are a cognitive planner for a TurtleBot3 robot operating in an indoor office environment.\n        The robot can perform the following actions:\n        - move_forward(distance_in_meters)\n        - move_backward(distance_in_meters)\n        - turn_left(degrees)\n        - turn_right(degrees)\n        - stop()\n        - pick_up_object()\n        - place_object()\n        - detect_object(object_type)\n        - check_battery()\n        \n        Current robot state:\n        {json.dumps(self.robot_state, indent=2)}\n        \n        Human command: "{command}"\n        \n        Respond with a JSON object containing a step-by-step plan. Each step should be a dictionary with an \'action\' and \'parameters\'.\n        Format:\n        {{\n          "plan": [\n            {{"action": "move_forward", "parameters": {{"distance": 1.0}}}},\n            {{"action": "turn_left", "parameters": {{"degrees": 90}}}},\n            {{"action": "pick_up_object", "parameters": {{}}}}\n          ]\n        }}\n        \n        Be specific with parameters and ensure the plan is executable.\n        """\n        \n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-3.5-turbo",\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.3,\n                max_tokens=500\n            )\n            \n            # Extract the plan from the response\n            content = response.choices[0].message[\'content\'].strip()\n            \n            # Extract JSON from the response\n            start_idx = content.find(\'{\')\n            end_idx = content.rfind(\'}\') + 1\n            \n            if start_idx != -1 and end_idx != 0:\n                json_str = content[start_idx:end_idx]\n                plan_data = json.loads(json_str)\n                return plan_data[\'plan\']\n            else:\n                self.get_logger().error(f\'Could not extract JSON from LLM response: {content}\')\n                return None\n                \n        except Exception as e:\n            self.get_logger().error(f\'Error calling LLM: {str(e)}\')\n            return None\n\n    def execute_plan(self, plan):\n        """Execute the plan step by step"""\n        for step in plan:\n            action = step[\'action\']\n            params = step.get(\'parameters\', {})\n            \n            self.get_logger().info(f\'Executing action: {action} with params: {params}\')\n            \n            if action == \'move_forward\':\n                self.move_forward(params.get(\'distance\', 1.0))\n            elif action == \'move_backward\':\n                self.move_backward(params.get(\'distance\', 1.0))\n            elif action == \'turn_left\':\n                self.turn_left(params.get(\'degrees\', 90))\n            elif action == \'turn_right\':\n                self.turn_right(params.get(\'degrees\', 90))\n            elif action == \'stop\':\n                self.stop_robot()\n            elif action == \'pick_up_object\':\n                self.pick_up_object()\n            elif action == \'place_object\':\n                self.place_object()\n            elif action == \'detect_object\':\n                self.detect_object(params.get(\'object_type\', \'any\'))\n            elif action == \'check_battery\':\n                self.check_battery()\n            else:\n                self.get_logger().warn(f\'Unknown action: {action}\')\n            \n            # Add a small delay between actions\n            self.get_clock().sleep_for(rclpy.duration.Duration(seconds=0.5))\n\n    def move_forward(self, distance):\n        """Move the robot forward by the specified distance"""\n        msg = Twist()\n        msg.linear.x = 0.2  # m/s\n        duration = distance / 0.2\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def move_backward(self, distance):\n        """Move the robot backward by the specified distance"""\n        msg = Twist()\n        msg.linear.x = -0.2  # m/s\n        duration = distance / 0.2\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def turn_left(self, degrees):\n        """Turn the robot left by the specified degrees"""\n        msg = Twist()\n        msg.angular.z = 0.5  # rad/s\n        duration = (degrees * 3.14159 / 180) / 0.5  # Convert degrees to radians and calculate time\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def turn_right(self, degrees):\n        """Turn the robot right by the specified degrees"""\n        msg = Twist()\n        msg.angular.z = -0.5  # rad/s\n        duration = (degrees * 3.14159 / 180) / 0.5  # Convert degrees to radians and calculate time\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def stop_robot(self):\n        """Stop the robot"""\n        msg = Twist()\n        self.cmd_vel_pub.publish(msg)\n\n    def pick_up_object(self):\n        """Simulate picking up an object"""\n        self.get_logger().info(\'Picking up object...\')\n        self.robot_state[\'attached_object\'] = \'object\'\n\n    def place_object(self):\n        """Simulate placing an object"""\n        self.get_logger().info(\'Placing object...\')\n        self.robot_state[\'attached_object\'] = None\n\n    def detect_object(self, object_type):\n        """Simulate object detection"""\n        self.get_logger().info(f\'Detecting {object_type}...\')\n\n    def check_battery(self):\n        """Check robot battery level"""\n        self.get_logger().info(f\'Battery level: {self.robot_state["battery_level"]}%\')\n'})}),"\n",(0,o.jsx)(n.h2,{id:"advanced-llm-integration-with-context",children:"Advanced LLM Integration with Context"}),"\n",(0,o.jsx)(n.h3,{id:"context-aware-llm-planner",children:"Context-Aware LLM Planner"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nimport openai\nimport json\nimport time\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\nfrom collections import deque\n\nclass ContextAwareLLMPlanner(Node):\n    def __init__(self):\n        super().__init__('context_aware_llm_planner')\n        \n        # Set OpenAI API key\n        openai.api_key = \"YOUR_API_KEY_HERE\"\n        \n        # Subscriptions\n        self.command_sub = self.create_subscription(\n            String,\n            'natural_language_command',\n            self.command_callback,\n            10\n        )\n        \n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            'scan',\n            self.scan_callback,\n            10\n        )\n        \n        # Publishers\n        self.action_pub = self.create_publisher(String, 'robot_action', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, 'cmd_vel', 10)\n        \n        # Robot state with more details\n        self.robot_state = {\n            'position': {'x': 0, 'y': 0, 'theta': 0},\n            'battery_level': 100,\n            'attached_object': None,\n            'environment': 'indoor_office',\n            'last_seen_objects': [],\n            'is_moving': False,\n            'safety_status': 'safe'\n        }\n        \n        # Maintain conversation history\n        self.conversation_history = deque(maxlen=10)\n        \n        # Store latest sensor data\n        self.latest_scan = None\n        \n        self.get_logger().info('Context-Aware LLM Planner initialized')\n\n    def command_callback(self, msg):\n        \"\"\"Process natural language command with context\"\"\"\n        command = msg.data\n        self.get_logger().info(f'Received command: {command}')\n        \n        # Add to conversation history\n        self.conversation_history.append({\n            'type': 'user',\n            'content': command,\n            'timestamp': time.time()\n        })\n        \n        # Generate plan using LLM with context\n        plan = self.generate_plan_with_context(command)\n        \n        if plan:\n            # Execute the plan\n            success = self.execute_plan_safely(plan)\n            \n            # Add result to conversation history\n            self.conversation_history.append({\n                'type': 'system',\n                'content': f\"Plan executed successfully: {success}\",\n                'timestamp': time.time()\n            })\n        else:\n            self.get_logger().warn('Could not generate a plan for the command')\n\n    def scan_callback(self, msg):\n        \"\"\"Update robot state with latest sensor data\"\"\"\n        self.latest_scan = msg\n        # Update safety status based on scan data\n        self.update_safety_status()\n\n    def update_safety_status(self):\n        \"\"\"Update robot safety status based on sensor data\"\"\"\n        if self.latest_scan is not None:\n            # Check for obstacles in front of the robot (simplified)\n            front_range = self.latest_scan.ranges[len(self.latest_scan.ranges)//2]  # Front reading\n            if front_range < 0.5:  # Obstacle within 0.5m\n                self.robot_state['safety_status'] = 'obstacle_detected'\n            else:\n                self.robot_state['safety_status'] = 'safe'\n\n    def generate_plan_with_context(self, command):\n        \"\"\"Generate a plan considering conversation history and context\"\"\"\n        # Get recent conversation context\n        context = list(self.conversation_history)[-5:]  # Last 5 interactions\n        \n        prompt = f\"\"\"\n        You are a cognitive planner for a TurtleBot3 robot operating in an indoor office environment.\n        The robot can perform the following actions:\n        - move_forward(distance_in_meters)\n        - move_backward(distance_in_meters)\n        - turn_left(degrees)\n        - turn_right(degrees)\n        - stop()\n        - pick_up_object()\n        - place_object()\n        - detect_object(object_type)\n        - check_battery()\n        \n        Current robot state:\n        {json.dumps(self.robot_state, indent=2)}\n        \n        Recent conversation history:\n        {json.dumps(context, indent=2)}\n        \n        Current sensor data (LIDAR scan):\n        - Min range: {min(self.latest_scan.ranges) if self.latest_scan else 'N/A'}\n        - Max range: {max(self.latest_scan.ranges) if self.latest_scan else 'N/A'}\n        - Front range: {self.latest_scan.ranges[len(self.latest_scan.ranges)//2] if self.latest_scan else 'N/A' }\n        \n        Human command: \"{command}\"\n        \n        Respond with a JSON object containing a step-by-step plan. Each step should be a dictionary with an 'action' and 'parameters'.\n        Format:\n        {{\n          \"plan\": [\n            {{\"action\": \"move_forward\", \"parameters\": {{\"distance\": 1.0}}}},\n            {{\"action\": \"turn_left\", \"parameters\": {{\"degrees\": 90}}}},\n            {{\"action\": \"pick_up_object\", \"parameters\": {{}}}}\n          ]\n        }}\n        \n        Consider the conversation context and current sensor data when generating the plan.\n        Be specific with parameters and ensure the plan is executable and safe.\n        \"\"\"\n        \n        try:\n            response = openai.ChatCompletion.create(\n                model=\"gpt-4\",  # Using GPT-4 for more complex reasoning\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.3,\n                max_tokens=700\n            )\n            \n            content = response.choices[0].message['content'].strip()\n            \n            # Extract JSON from the response\n            start_idx = content.find('{')\n            end_idx = content.rfind('}') + 1\n            \n            if start_idx != -1 and end_idx != 0:\n                json_str = content[start_idx:end_idx]\n                plan_data = json.loads(json_str)\n                return plan_data['plan']\n            else:\n                self.get_logger().error(f'Could not extract JSON from LLM response: {content}')\n                return None\n                \n        except Exception as e:\n            self.get_logger().error(f'Error calling LLM: {str(e)}')\n            return None\n\n    def execute_plan_safely(self, plan):\n        \"\"\"Execute the plan with safety checks\"\"\"\n        for step in plan:\n            # Check if environment is still safe\n            if not self.check_environment_safety():\n                self.get_logger().error('Environment is unsafe, stopping execution')\n                self.stop_robot()\n                return False\n            \n            # Execute the action\n            self.execute_single_action(step)\n            \n            # Small delay between actions for safety\n            time.sleep(0.5)\n        \n        return True\n\n    def check_environment_safety(self):\n        \"\"\"Check if the environment is safe for movement\"\"\"\n        # In a real robot, this would check sensor data\n        # for obstacles, people, etc.\n        return self.robot_state['safety_status'] == 'safe'\n\n    def execute_single_action(self, step):\n        \"\"\"Execute a single action from the plan\"\"\"\n        action = step['action']\n        params = step.get('parameters', {})\n        \n        self.get_logger().info(f'Executing action: {action} with params: {params}')\n        \n        if action == 'move_forward':\n            self.move_forward(params.get('distance', 1.0))\n        elif action == 'move_backward':\n            self.move_backward(params.get('distance', 1.0))\n        elif action == 'turn_left':\n            self.turn_left(params.get('degrees', 90))\n        elif action == 'turn_right':\n            self.turn_right(params.get('degrees', 90))\n        elif action == 'stop':\n            self.stop_robot()\n        elif action == 'pick_up_object':\n            self.pick_up_object()\n        elif action == 'place_object':\n            self.place_object()\n        elif action == 'detect_object':\n            self.detect_object(params.get('object_type', 'any'))\n        elif action == 'check_battery':\n            self.check_battery()\n        else:\n            self.get_logger().warn(f'Unknown action: {action}')\n\n    # Implementation of movement methods would be similar to SimpleLLMPlanner\n    def move_forward(self, distance):\n        \"\"\"Move the robot forward by the specified distance\"\"\"\n        msg = Twist()\n        msg.linear.x = 0.2  # m/s\n        duration = distance / 0.2\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def move_backward(self, distance):\n        \"\"\"Move the robot backward by the specified distance\"\"\"\n        msg = Twist()\n        msg.linear.x = -0.2  # m/s\n        duration = distance / 0.2\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def turn_left(self, degrees):\n        \"\"\"Turn the robot left by the specified degrees\"\"\"\n        msg = Twist()\n        msg.angular.z = 0.5  # rad/s\n        duration = (degrees * 3.14159 / 180) / 0.5  # Convert degrees to radians and calculate time\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def turn_right(self, degrees):\n        \"\"\"Turn the robot right by the specified degrees\"\"\"\n        msg = Twist()\n        msg.angular.z = -0.5  # rad/s\n        duration = (degrees * 3.14159 / 180) / 0.5  # Convert degrees to radians and calculate time\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def stop_robot(self):\n        \"\"\"Stop the robot\"\"\"\n        msg = Twist()\n        self.cmd_vel_pub.publish(msg)\n        self.robot_state['is_moving'] = False\n\n    def pick_up_object(self):\n        \"\"\"Simulate picking up an object\"\"\"\n        self.get_logger().info('Picking up object...')\n        self.robot_state['attached_object'] = 'object'\n\n    def place_object(self):\n        \"\"\"Simulate placing an object\"\"\"\n        self.get_logger().info('Placing object...')\n        self.robot_state['attached_object'] = None\n\n    def detect_object(self, object_type):\n        \"\"\"Simulate object detection\"\"\"\n        self.get_logger().info(f'Detecting {object_type}...')\n\n    def check_battery(self):\n        \"\"\"Check robot battery level\"\"\"\n        self.get_logger().info(f'Battery level: {self.robot_state[\"battery_level\"]}%')\n"})}),"\n",(0,o.jsx)(n.h2,{id:"local-llm-integration",children:"Local LLM Integration"}),"\n",(0,o.jsx)(n.h3,{id:"running-open-source-llms-locally",children:"Running Open-Source LLMs Locally"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport rclpy\nfrom rclpy.node import Node\nimport json\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport time\n\nclass LocalLLMPlanner(Node):\n    def __init__(self):\n        super().__init__(\'local_llm_planner\')\n        \n        # Load a pre-trained model (e.g., Llama, Mistral, or other)\n        model_name = "microsoft/DialoGPT-medium"  # Example model - replace with appropriate model\n        \n        try:\n            # Initialize the tokenizer and model\n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n            self.model = AutoModelForCausalLM.from_pretrained(model_name)\n            \n            # Set pad token if it doesn\'t exist\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            # Initialize text generation pipeline\n            self.generator = pipeline(\n                \'text-generation\',\n                model=self.model,\n                tokenizer=self.tokenizer,\n                device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n            )\n            \n            self.get_logger().info(f\'Loaded model: {model_name}\')\n        except Exception as e:\n            self.get_logger().error(f\'Failed to load model {model_name}: {e}\')\n            return\n        \n        # Subscriptions\n        self.command_sub = self.create_subscription(\n            String,\n            \'natural_language_command\',\n            self.command_callback,\n            10\n        )\n        \n        # Publishers\n        self.action_pub = self.create_publisher(String, \'robot_action\', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        \n        # Robot state\n        self.robot_state = {\n            \'position\': {\'x\': 0, \'y\': 0, \'theta\': 0},\n            \'battery_level\': 100,\n            \'attached_object\': None,\n            \'environment\': \'indoor_office\'\n        }\n        \n        self.get_logger().info(\'Local LLM Planner initialized\')\n\n    def command_callback(self, msg):\n        """Process natural language command using local LLM"""\n        command = msg.data\n        self.get_logger().info(f\'Received command: {command}\')\n        \n        # Generate plan using local LLM\n        plan = self.generate_plan_local(command)\n        \n        if plan:\n            # Execute the plan\n            self.execute_plan(plan)\n        else:\n            self.get_logger().warn(\'Could not generate a plan for the command\')\n\n    def generate_plan_local(self, command):\n        """Generate a plan using a local LLM"""\n        prompt = f"""\n        You are a cognitive planner for a TurtleBot3 robot operating in an indoor office environment.\n        The robot can perform the following actions:\n        - move_forward(distance_in_meters)\n        - move_backward(distance_in_meters)\n        - turn_left(degrees)\n        - turn_right(degrees)\n        - stop()\n        - pick_up_object()\n        - place_object()\n        - detect_object(object_type)\n        - check_battery()\n        \n        Current robot state:\n        {json.dumps(self.robot_state, indent=2)}\n        \n        Human command: "{command}"\n        \n        Respond with a JSON object containing a step-by-step plan. Each step should be a dictionary with an \'action\' and \'parameters\'.\n        Format:\n        {{\n          "plan": [\n            {{"action": "move_forward", "parameters": {{"distance": 1.0}}}},\n            {{"action": "turn_left", "parameters": {{"degrees": 90}}}},\n            {{"action": "pick_up_object", "parameters": {{}}}}\n          ]\n        }}\n        \n        Be specific with parameters and ensure the plan is executable.\n        """\n        \n        try:\n            # Generate response using the local model\n            response = self.generator(\n                prompt,\n                max_length=500,\n                num_return_sequences=1,\n                temperature=0.3,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n            \n            content = response[0][\'generated_text\'][len(prompt):].strip()\n            \n            # Extract JSON from the response\n            start_idx = content.find(\'{\')\n            end_idx = content.rfind(\'}\') + 1\n            \n            if start_idx != -1 and end_idx != 0:\n                json_str = content[start_idx:end_idx]\n                plan_data = json.loads(json_str)\n                return plan_data[\'plan\']\n            else:\n                self.get_logger().error(f\'Could not extract JSON from LLM response: {content}\')\n                return None\n                \n        except Exception as e:\n            self.get_logger().error(f\'Error generating plan with local LLM: {str(e)}\')\n            return None\n\n    def execute_plan(self, plan):\n        """Execute the plan step by step"""\n        for step in plan:\n            action = step[\'action\']\n            params = step.get(\'parameters\', {})\n            \n            self.get_logger().info(f\'Executing action: {action} with params: {params}\')\n            \n            if action == \'move_forward\':\n                self.move_forward(params.get(\'distance\', 1.0))\n            elif action == \'move_backward\':\n                self.move_backward(params.get(\'distance\', 1.0))\n            elif action == \'turn_left\':\n                self.turn_left(params.get(\'degrees\', 90))\n            elif action == \'turn_right\':\n                self.turn_right(params.get(\'degrees\', 90))\n            elif action == \'stop\':\n                self.stop_robot()\n            elif action == \'pick_up_object\':\n                self.pick_up_object()\n            elif action == \'place_object\':\n                self.place_object()\n            elif action == \'detect_object\':\n                self.detect_object(params.get(\'object_type\', \'any\'))\n            elif action == \'check_battery\':\n                self.check_battery()\n            else:\n                self.get_logger().warn(f\'Unknown action: {action}\')\n            \n            # Add a small delay between actions\n            time.sleep(0.5)\n\n    # Movement methods would be similar to previous examples\n    def move_forward(self, distance):\n        """Move the robot forward by the specified distance"""\n        msg = Twist()\n        msg.linear.x = 0.2  # m/s\n        duration = distance / 0.2\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def move_backward(self, distance):\n        """Move the robot backward by the specified distance"""\n        msg = Twist()\n        msg.linear.x = -0.2  # m/s\n        duration = distance / 0.2\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def turn_left(self, degrees):\n        """Turn the robot left by the specified degrees"""\n        msg = Twist()\n        msg.angular.z = 0.5  # rad/s\n        duration = (degrees * 3.14159 / 180) / 0.5  # Convert degrees to radians and calculate time\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def turn_right(self, degrees):\n        """Turn the robot right by the specified degrees"""\n        msg = Twist()\n        msg.angular.z = -0.5  # rad/s\n        duration = (degrees * 3.14159 / 180) / 0.5  # Convert degrees to radians and calculate time\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def stop_robot(self):\n        """Stop the robot"""\n        msg = Twist()\n        self.cmd_vel_pub.publish(msg)\n\n    def pick_up_object(self):\n        """Simulate picking up an object"""\n        self.get_logger().info(\'Picking up object...\')\n        self.robot_state[\'attached_object\'] = \'object\'\n\n    def place_object(self):\n        """Simulate placing an object"""\n        self.get_logger().info(\'Placing object...\')\n        self.robot_state[\'attached_object\'] = None\n\n    def detect_object(self, object_type):\n        """Simulate object detection"""\n        self.get_logger().info(f\'Detecting {object_type}...\')\n\n    def check_battery(self):\n        """Check robot battery level"""\n        self.get_logger().info(f\'Battery level: {self.robot_state["battery_level"]}%\')\n'})}),"\n",(0,o.jsx)(n.h2,{id:"multi-modal-llm-integration",children:"Multi-Modal LLM Integration"}),"\n",(0,o.jsx)(n.h3,{id:"combining-vision-and-language-for-complex-tasks",children:"Combining Vision and Language for Complex Tasks"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nimport openai\nimport json\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport base64\n\nclass MultiModalLLMPlanner(Node):\n    def __init__(self):\n        super().__init__(\'multimodal_llm_planner\')\n        \n        # Set OpenAI API key\n        openai.api_key = "YOUR_API_KEY_HERE"\n        \n        # Initialize OpenCV bridge\n        self.bridge = CvBridge()\n        \n        # Subscriptions\n        self.command_sub = self.create_subscription(\n            String,\n            \'natural_language_command\',\n            self.command_callback,\n            10\n        )\n        \n        self.image_sub = self.create_subscription(\n            Image,\n            \'camera/image_raw\',\n            self.image_callback,\n            10\n        )\n        \n        # Publishers\n        self.action_pub = self.create_publisher(String, \'robot_action\', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        \n        # Robot state\n        self.robot_state = {\n            \'position\': {\'x\': 0, \'y\': 0, \'theta\': 0},\n            \'battery_level\': 100,\n            \'attached_object\': None,\n            \'environment\': \'indoor_office\'\n        }\n        \n        # Store latest image\n        self.latest_image = None\n        \n        self.get_logger().info(\'Multi-Modal LLM Planner initialized\')\n\n    def command_callback(self, msg):\n        """Process natural language command with visual context"""\n        command = msg.data\n        self.get_logger().info(f\'Received command: {command}\')\n        \n        # Generate plan using LLM with both text and image\n        plan = self.generate_plan_with_vision(command)\n        \n        if plan:\n            # Execute the plan\n            self.execute_plan(plan)\n        else:\n            self.get_logger().warn(\'Could not generate a plan for the command\')\n\n    def image_callback(self, msg):\n        """Process incoming image data"""\n        try:\n            # Convert ROS Image message to OpenCV image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n            self.latest_image = cv_image\n        except Exception as e:\n            self.get_logger().error(f\'Error converting image: {e}\')\n\n    def encode_image(self, image):\n        """Encode OpenCV image as base64 string"""\n        _, buffer = cv2.imencode(\'.jpg\', image)\n        image_bytes = buffer.tobytes()\n        base64_image = base64.b64encode(image_bytes).decode(\'utf-8\')\n        return base64_image\n\n    def generate_plan_with_vision(self, command):\n        """Generate a plan using both text command and visual input"""\n        # Encode the latest image\n        if self.latest_image is not None:\n            # Resize image to save on tokens (GPT-4 Vision has limits)\n            height, width = self.latest_image.shape[:2]\n            if height > 480 or width > 640:\n                # Maintain aspect ratio\n                scale = min(480/height, 640/width)\n                new_width = int(width * scale)\n                new_height = int(height * scale)\n                resized_image = cv2.resize(self.latest_image, (new_width, new_height))\n            else:\n                resized_image = self.latest_image\n            \n            base64_image = self.encode_image(resized_image)\n        else:\n            self.get_logger().warn(\'No image available for visual context\')\n            return None\n\n        # Create the message with both text and image\n        messages = [\n            {\n                "role": "user",\n                "content": [\n                    {\n                        "type": "text",\n                        "text": f"""\n                        You are a cognitive planner for a TurtleBot3 robot operating in an indoor office environment.\n                        The robot can perform the following actions:\n                        - move_forward(distance_in_meters)\n                        - move_backward(distance_in_meters)\n                        - turn_left(degrees)\n                        - turn_right(degrees)\n                        - stop()\n                        - pick_up_object()\n                        - place_object()\n                        - detect_object(object_type)\n                        - check_battery()\n\n                        Current robot state:\n                        {json.dumps(self.robot_state, indent=2)}\n\n                        Human command: "{command}"\n\n                        The image shows the robot\'s current view. Use this visual information to understand the environment and execute the command.\n\n                        Respond with a JSON object containing a step-by-step plan. Each step should be a dictionary with an \'action\' and \'parameters\'.\n                        Format:\n                        {{\n                          "plan": [\n                            {{"action": "move_forward", "parameters": {{"distance": 1.0}}}},\n                            {{"action": "turn_left", "parameters": {{"degrees": 90}}}},\n                            {{"action": "pick_up_object", "parameters": {{}}}}\n                          ]\n                        }}\n\n                        Be specific with parameters and ensure the plan is executable.\n                        """\n                    },\n                    {\n                        "type": "image_url",\n                        "image_url": {\n                            "url": f"data:image/jpeg;base64,{base64_image}"\n                        }\n                    }\n                ]\n            }\n        ]\n\n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-4-vision-preview",  # Use GPT-4 with vision capabilities\n                messages=messages,\n                temperature=0.3,\n                max_tokens=700\n            )\n            \n            content = response.choices[0].message[\'content\'].strip()\n            \n            # Extract JSON from the response\n            start_idx = content.find(\'{\')\n            end_idx = content.rfind(\'}\') + 1\n            \n            if start_idx != -1 and end_idx != 0:\n                json_str = content[start_idx:end_idx]\n                plan_data = json.loads(json_str)\n                return plan_data[\'plan\']\n            else:\n                self.get_logger().error(f\'Could not extract JSON from LLM response: {content}\')\n                return None\n                \n        except Exception as e:\n            self.get_logger().error(f\'Error calling multimodal LLM: {str(e)}\')\n            return None\n\n    def execute_plan(self, plan):\n        """Execute the plan step by step"""\n        for step in plan:\n            action = step[\'action\']\n            params = step.get(\'parameters\', {})\n            \n            self.get_logger().info(f\'Executing action: {action} with params: {params}\')\n            \n            if action == \'move_forward\':\n                self.move_forward(params.get(\'distance\', 1.0))\n            elif action == \'move_backward\':\n                self.move_backward(params.get(\'distance\', 1.0))\n            elif action == \'turn_left\':\n                self.turn_left(params.get(\'degrees\', 90))\n            elif action == \'turn_right\':\n                self.turn_right(params.get(\'degrees\', 90))\n            elif action == \'stop\':\n                self.stop_robot()\n            elif action == \'pick_up_object\':\n                self.pick_up_object()\n            elif action == \'place_object\':\n                self.place_object()\n            elif action == \'detect_object\':\n                self.detect_object(params.get(\'object_type\', \'any\'))\n            elif action == \'check_battery\':\n                self.check_battery()\n            else:\n                self.get_logger().warn(f\'Unknown action: {action}\')\n            \n            # Add a small delay between actions\n            time.sleep(0.5)\n\n    # Movement methods would be similar to previous examples\n    def move_forward(self, distance):\n        """Move the robot forward by the specified distance"""\n        msg = Twist()\n        msg.linear.x = 0.2  # m/s\n        duration = distance / 0.2\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def move_backward(self, distance):\n        """Move the robot backward by the specified distance"""\n        msg = Twist()\n        msg.linear.x = -0.2  # m/s\n        duration = distance / 0.2\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def turn_left(self, degrees):\n        """Turn the robot left by the specified degrees"""\n        msg = Twist()\n        msg.angular.z = 0.5  # rad/s\n        duration = (degrees * 3.14159 / 180) / 0.5  # Convert degrees to radians and calculate time\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def turn_right(self, degrees):\n        """Turn the robot right by the specified degrees"""\n        msg = Twist()\n        msg.angular.z = -0.5  # rad/s\n        duration = (degrees * 3.14159 / 180) / 0.5  # Convert degrees to radians and calculate time\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def stop_robot(self):\n        """Stop the robot"""\n        msg = Twist()\n        self.cmd_vel_pub.publish(msg)\n\n    def pick_up_object(self):\n        """Simulate picking up an object"""\n        self.get_logger().info(\'Picking up object...\')\n        self.robot_state[\'attached_object\'] = \'object\'\n\n    def place_object(self):\n        """Simulate placing an object"""\n        self.get_logger().info(\'Placing object...\')\n        self.robot_state[\'attached_object\'] = None\n\n    def detect_object(self, object_type):\n        """Simulate object detection"""\n        self.get_logger().info(f\'Detecting {object_type}...\')\n\n    def check_battery(self):\n        """Check robot battery level"""\n        self.get_logger().info(f\'Battery level: {self.robot_state["battery_level"]}%\')\n'})}),"\n",(0,o.jsx)(n.h2,{id:"error-handling-and-fallback-strategies",children:"Error Handling and Fallback Strategies"}),"\n",(0,o.jsx)(n.h3,{id:"robust-llm-planner-with-fallbacks",children:"Robust LLM Planner with Fallbacks"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nimport openai\nimport json\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport time\nimport random\n\nclass RobustLLMPlanner(Node):\n    def __init__(self):\n        super().__init__('robust_llm_planner')\n        \n        # Set OpenAI API key\n        openai.api_key = \"YOUR_API_KEY_HERE\"\n        \n        # Subscriptions\n        self.command_sub = self.create_subscription(\n            String,\n            'natural_language_command',\n            self.command_callback,\n            10\n        )\n        \n        # Publishers\n        self.action_pub = self.create_publisher(String, 'robot_action', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, 'cmd_vel', 10)\n        \n        # Robot state\n        self.robot_state = {\n            'position': {'x': 0, 'y': 0, 'theta': 0},\n            'battery_level': 100,\n            'attached_object': None,\n            'environment': 'indoor_office'\n        }\n        \n        # Fallback command mappings for common requests\n        self.fallback_commands = {\n            'move forward': [{'action': 'move_forward', 'parameters': {'distance': 1.0}}],\n            'go forward': [{'action': 'move_forward', 'parameters': {'distance': 1.0}}],\n            'move back': [{'action': 'move_backward', 'parameters': {'distance': 1.0}}],\n            'go back': [{'action': 'move_backward', 'parameters': {'distance': 1.0}}],\n            'turn left': [{'action': 'turn_left', 'parameters': {'degrees': 90}}],\n            'turn right': [{'action': 'turn_right', 'parameters': {'degrees': 90}}],\n            'stop': [{'action': 'stop', 'parameters': {}}],\n            'halt': [{'action': 'stop', 'parameters': {}}]\n        }\n        \n        # Statistics for monitoring\n        self.llm_calls = 0\n        self.llm_failures = 0\n        self.fallback_uses = 0\n        \n        # Start statistics timer\n        self.stats_timer = self.create_timer(60.0, self.log_statistics)\n        \n        self.get_logger().info('Robust LLM Planner initialized')\n\n    def command_callback(self, msg):\n        \"\"\"Process natural language command with error handling and fallbacks\"\"\"\n        command = msg.data\n        self.get_logger().info(f'Received command: {command}')\n        \n        # Try to generate plan with LLM first\n        plan = self.generate_plan_with_error_handling(command)\n        \n        if plan:\n            # Execute the plan\n            self.execute_plan(plan)\n        else:\n            self.get_logger().warn('LLM failed to generate plan, trying fallback')\n            \n            # Try fallback method\n            fallback_plan = self.get_fallback_plan(command)\n            if fallback_plan:\n                self.fallback_uses += 1\n                self.get_logger().info('Using fallback plan')\n                self.execute_plan(fallback_plan)\n            else:\n                self.get_logger().error('Both LLM and fallback methods failed')\n                # Provide feedback to user\n                feedback_msg = String()\n                feedback_msg.data = f\"Could not understand command: {command}\"\n                self.action_pub.publish(feedback_msg)\n\n    def generate_plan_with_error_handling(self, command):\n        \"\"\"Generate a plan with comprehensive error handling\"\"\"\n        self.llm_calls += 1\n        \n        try:\n            prompt = f\"\"\"\n            You are a cognitive planner for a TurtleBot3 robot operating in an indoor office environment.\n            The robot can perform the following actions:\n            - move_forward(distance_in_meters)\n            - move_backward(distance_in_meters)\n            - turn_left(degrees)\n            - turn_right(degrees)\n            - stop()\n            - pick_up_object()\n            - place_object()\n            - detect_object(object_type)\n            - check_battery()\n\n            Current robot state:\n            {json.dumps(self.robot_state, indent=2)}\n\n            Human command: \"{command}\"\n\n            Respond with a JSON object containing a step-by-step plan. Each step should be a dictionary with an 'action' and 'parameters'.\n            Format:\n            {{\n              \"plan\": [\n                {{\"action\": \"move_forward\", \"parameters\": {{\"distance\": 1.0}}}},\n                {{\"action\": \"turn_left\", \"parameters\": {{\"degrees\": 90}}}},\n                {{\"action\": \"pick_up_object\", \"parameters\": {{}}}}\n              ]\n            }}\n\n            Be specific with parameters and ensure the plan is executable.\n            \"\"\"\n            \n            response = openai.ChatCompletion.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.3,\n                max_tokens=500,\n                timeout=15  # 15 second timeout\n            )\n            \n            content = response.choices[0].message['content'].strip()\n            \n            # Extract JSON from the response\n            start_idx = content.find('{')\n            end_idx = content.rfind('}') + 1\n            \n            if start_idx != -1 and end_idx != 0:\n                json_str = content[start_idx:end_idx]\n                plan_data = json.loads(json_str)\n                return plan_data['plan']\n            else:\n                self.get_logger().error(f'Could not extract JSON from LLM response: {content}')\n                return None\n                \n        except openai.error.RateLimitError:\n            self.get_logger().error('OpenAI rate limit exceeded')\n            self.llm_failures += 1\n            return None\n        except openai.error.AuthenticationError:\n            self.get_logger().error('OpenAI authentication failed')\n            self.llm_failures += 1\n            return None\n        except openai.error.APIConnectionError:\n            self.get_logger().error('OpenAI connection error')\n            self.llm_failures += 1\n            return None\n        except openai.error.Timeout:\n            self.get_logger().error('OpenAI request timed out')\n            self.llm_failures += 1\n            return None\n        except json.JSONDecodeError:\n            self.get_logger().error('Invalid JSON response from LLM')\n            self.llm_failures += 1\n            return None\n        except Exception as e:\n            self.get_logger().error(f'Unexpected error in LLM call: {str(e)}')\n            self.llm_failures += 1\n            return None\n\n    def get_fallback_plan(self, command):\n        \"\"\"Get a plan using simple keyword matching as fallback\"\"\"\n        command_lower = command.lower()\n        \n        # Check for exact matches first\n        if command_lower in self.fallback_commands:\n            return self.fallback_commands[command_lower]\n        \n        # Check for partial matches\n        for key, plan in self.fallback_commands.items():\n            if key in command_lower:\n                return plan\n        \n        # If no match found, try to parse simple commands\n        if 'forward' in command_lower or 'ahead' in command_lower:\n            # Extract distance if mentioned\n            distance = 1.0  # default\n            for word in command_lower.split():\n                if word.replace('.', '').isdigit():\n                    distance = float(word)\n                    break\n            return [{'action': 'move_forward', 'parameters': {'distance': distance}}]\n        elif 'backward' in command_lower or 'back' in command_lower:\n            distance = 1.0  # default\n            for word in command_lower.split():\n                if word.replace('.', '').isdigit():\n                    distance = float(word)\n                    break\n            return [{'action': 'move_backward', 'parameters': {'distance': distance}}]\n        elif 'left' in command_lower:\n            degrees = 90  # default\n            for word in command_lower.split():\n                if word.replace('.', '').isdigit():\n                    degrees = float(word)\n                    break\n            return [{'action': 'turn_left', 'parameters': {'degrees': degrees}}]\n        elif 'right' in command_lower:\n            degrees = 90  # default\n            for word in command_lower.split():\n                if word.replace('.', '').isdigit():\n                    degrees = float(word)\n                    break\n            return [{'action': 'turn_right', 'parameters': {'degrees': degrees}}]\n        \n        # No fallback plan available\n        return None\n\n    def execute_plan(self, plan):\n        \"\"\"Execute the plan step by step\"\"\"\n        for step in plan:\n            action = step['action']\n            params = step.get('parameters', {})\n            \n            self.get_logger().info(f'Executing action: {action} with params: {params}')\n            \n            if action == 'move_forward':\n                self.move_forward(params.get('distance', 1.0))\n            elif action == 'move_backward':\n                self.move_backward(params.get('distance', 1.0))\n            elif action == 'turn_left':\n                self.turn_left(params.get('degrees', 90))\n            elif action == 'turn_right':\n                self.turn_right(params.get('degrees', 90))\n            elif action == 'stop':\n                self.stop_robot()\n            elif action == 'pick_up_object':\n                self.pick_up_object()\n            elif action == 'place_object':\n                self.place_object()\n            elif action == 'detect_object':\n                self.detect_object(params.get('object_type', 'any'))\n            elif action == 'check_battery':\n                self.check_battery()\n            else:\n                self.get_logger().warn(f'Unknown action: {action}')\n            \n            # Add a small delay between actions\n            time.sleep(0.5)\n\n    def log_statistics(self):\n        \"\"\"Log LLM usage statistics\"\"\"\n        success_rate = 0\n        if self.llm_calls > 0:\n            success_rate = 100 * (self.llm_calls - self.llm_failures) / self.llm_calls\n            \n        self.get_logger().info(\n            f'LLM Statistics: {self.llm_calls} calls, '\n            f'{self.llm_failures} failures, '\n            f'{self.fallback_uses} fallbacks used, '\n            f'{success_rate:.1f}% success rate'\n        )\n\n    # Movement methods would be similar to previous examples\n    def move_forward(self, distance):\n        \"\"\"Move the robot forward by the specified distance\"\"\"\n        msg = Twist()\n        msg.linear.x = 0.2  # m/s\n        duration = distance / 0.2\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def move_backward(self, distance):\n        \"\"\"Move the robot backward by the specified distance\"\"\"\n        msg = Twist()\n        msg.linear.x = -0.2  # m/s\n        duration = distance / 0.2\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def turn_left(self, degrees):\n        \"\"\"Turn the robot left by the specified degrees\"\"\"\n        msg = Twist()\n        msg.angular.z = 0.5  # rad/s\n        duration = (degrees * 3.14159 / 180) / 0.5  # Convert degrees to radians and calculate time\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def turn_right(self, degrees):\n        \"\"\"Turn the robot right by the specified degrees\"\"\"\n        msg = Twist()\n        msg.angular.z = -0.5  # rad/s\n        duration = (degrees * 3.14159 / 180) / 0.5  # Convert degrees to radians and calculate time\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def stop_robot(self):\n        \"\"\"Stop the robot\"\"\"\n        msg = Twist()\n        self.cmd_vel_pub.publish(msg)\n\n    def pick_up_object(self):\n        \"\"\"Simulate picking up an object\"\"\"\n        self.get_logger().info('Picking up object...')\n        self.robot_state['attached_object'] = 'object'\n\n    def place_object(self):\n        \"\"\"Simulate placing an object\"\"\"\n        self.get_logger().info('Placing object...')\n        self.robot_state['attached_object'] = None\n\n    def detect_object(self, object_type):\n        \"\"\"Simulate object detection\"\"\"\n        self.get_logger().info(f'Detecting {object_type}...')\n\n    def check_battery(self):\n        \"\"\"Check robot battery level\"\"\"\n        self.get_logger().info(f'Battery level: {self.robot_state[\"battery_level\"]}%')\n"})}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"These code examples demonstrate various approaches to implementing LLM-based cognitive planning in robotics:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Basic Integration"}),": Simple command-to-action mapping using LLMs"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Context-Aware Planning"}),": Using conversation history and sensor data"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Local LLMs"}),": Running open-source models locally for privacy/offline capability"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multi-Modal"}),": Combining vision and language for complex tasks"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robust Implementation"}),": Error handling and fallback strategies"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Each approach has its trade-offs in terms of complexity, resource usage, privacy, and real-time performance. Choose the approach that best fits your specific robotics application requirements."})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>r,x:()=>i});var a=t(6540);const o={},s=a.createContext(o);function r(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);