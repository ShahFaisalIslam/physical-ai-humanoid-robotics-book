---
title: Robotics Glossary
description: Comprehensive glossary of robotics terms and definitions
sidebar_position: 4
---

# Robotics Glossary

## A

**Actuator**: A mechanical device that converts energy (usually electrical, hydraulic, or pneumatic) into physical motion. Common examples include motors, servos, and pneumatic cylinders.

**Adaptive Control**: A control system that adjusts its parameters in real-time based on changes in the system or environment to maintain optimal performance.

**Adaptive Monte Carlo Localization (AMCL)**: A probabilistic localization algorithm that uses particle filters to estimate a robot's position and orientation in a known map.

**Artificial Intelligence (AI)**: The simulation of human intelligence in machines that are programmed to think and learn like humans, particularly relevant in robotics for perception, planning, and decision-making.

**Autonomous Mobile Robot (AMR)**: A robot that can navigate and perform tasks without human intervention, using sensors and AI to perceive and interact with its environment.

## B

**Behavior-Based Robotics**: An approach to robotics that structures robot control as a collection of behaviors, each responsible for a specific task, which are then coordinated to achieve complex behaviors.

**Bipedal Robot**: A robot with two legs designed to walk upright, mimicking human locomotion.

**Blob Detection**: A computer vision technique used to identify connected regions of pixels in an image that share similar properties, often used for object detection.

**Brushless DC Motor**: An electric motor that uses electronic commutation instead of mechanical brushes, offering higher efficiency and longer life than brushed motors.

## C

**Cartesian Space**: The three-dimensional space defined by X, Y, and Z coordinates, used to describe positions and movements in the physical world.

**Cognitive Robotics**: A field that combines robotics with cognitive science to create robots that can perceive, reason, learn, and interact with humans and environments in human-like ways.

**Computer Vision**: A field of artificial intelligence that trains computers to interpret and understand the visual world, using digital images, video, and deep learning models.

**Control Theory**: The mathematical study of dynamic systems and how to influence their behavior to achieve desired outcomes, fundamental to robot control systems.

**Convolutional Neural Network (CNN)**: A class of deep neural networks most commonly applied to analyzing visual imagery, widely used in robotics for perception tasks.

**Covariance**: In robotics, a measure of how much two random variables change together, commonly used in localization and mapping to represent uncertainty.

**Cyber-Physical System (CPS)**: A system with tight integration between computational and physical elements, of which robots are a prime example.

## D

**Deep Learning**: A subset of machine learning based on artificial neural networks with representation learning, enabling robots to learn complex patterns from data.

**Differential Drive**: A type of wheeled robot drivetrain that uses two independently controlled wheels on the same axis to achieve motion and steering.

**Dijkstra's Algorithm**: A graph search algorithm that solves the single-source shortest path problem for a graph with non-negative edge path costs, used in robot path planning.

**Dynamic Movement Primitives (DMP)**: A method for learning and reproducing complex movements in robotics, representing trajectories as a combination of stable dynamical systems.

**Dynamic Window Approach (DWA)**: A local path planning algorithm that selects robot velocities by evaluating trajectories within a constraint window to avoid obstacles while approaching a target.

## E

**Encoder**: A sensor that measures the rotational position of a shaft, commonly used in robotics to track wheel rotation and joint angles.

**End Effector**: The device at the end of a robotic manipulator designed to interact with the environment, such as a gripper, tool, or sensor.

**Epuck Robot**: A small mobile robot platform designed for education and research in swarm robotics and artificial intelligence.

**Extended Kalman Filter (EKF)**: A nonlinear version of the Kalman filter that linearizes the system model around the current estimate, used for state estimation in robotics.

**Embodied Cognition**: The theory that cognitive processes are deeply influenced by the body's interactions with the environment, relevant to how robots should be designed and controlled.

## F

**Forward Kinematics**: The use of joint parameters to compute the position and orientation of the end-effector of a robotic manipulator.

**Fiducial Marker**: A visual marker with a known pattern used for camera pose estimation, often used in robotics for localization and navigation.

**Fuzzy Logic**: A mathematical approach to handle uncertainty by allowing partial membership in sets, useful in robotics for control systems that need to handle imprecise sensor data.

## G

**Gazebo**: A 3D simulation environment for robotics that provides realistic physics simulation and sensor models for testing robot algorithms.

**Gaussian Distribution**: A continuous probability distribution that is symmetric around the mean, fundamental to many robotics algorithms for representing uncertainty.

**Gimbal Lock**: A loss of one degree of freedom in a three-dimensional, three-gimbal mechanism, relevant to robot orientation representation.

**Global Navigation Satellite System (GNSS)**: A satellite-based system that provides geospatial positioning, including GPS, GLONASS, Galileo, and BeiDou.

**Graph SLAM**: A formulation of the Simultaneous Localization and Mapping problem that represents the environment and robot trajectory as a graph optimization problem.

## H

**Haptic Feedback**: The use of touch and motion feedback to provide information to a user, important for teleoperated robots and human-robot interaction.

**Heuristic**: A practical approach to problem-solving that is not guaranteed to be optimal but is sufficient for immediate goals, such as in A* path planning.

**Human-Robot Interaction (HRI)**: The study of interactions between humans and robots, focusing on design, development, and evaluation of robotic systems for human use.

**Husky Robot**: A medium-sized outdoor mobile robot platform developed by Clearpath Robotics, commonly used for research and industrial applications.

## I

**Inverse Kinematics**: The mathematical process of determining the joint parameters needed to place the end-effector of a robotic manipulator at a desired position and orientation.

**Inertial Measurement Unit (IMU)**: A device that measures and reports a body's specific force, angular rate, and sometimes magnetic field, using accelerometers, gyroscopes, and magnetometers.

**Iterative Closest Point (ICP)**: An algorithm used to minimize the distance between two point clouds, commonly used in 3D mapping and localization.

**Intelligent Agent**: An autonomous entity that perceives its environment and takes actions to achieve its goals, a fundamental concept in AI and robotics.

## J

**Jacobian Matrix**: A matrix of partial derivatives that describes the relationship between joint velocities and end-effector velocities in robotic manipulators.

**Joint Space**: The space defined by the joint angles or positions of a robotic manipulator, as opposed to Cartesian space.

## K

**Kalman Filter**: An algorithm that uses a series of measurements observed over time to estimate unknown variables, widely used in robotics for sensor fusion and state estimation.

**Kinematics**: The study of motion without considering the forces that cause it, fundamental to robot motion planning and control.

**Kinodynamic Planning**: Motion planning that considers both kinematic and dynamic constraints of the robot.

**Kobuki Robot**: A mobile robot platform developed by Yujin Robot, commonly used for ROS tutorials and research applications.

## L

**Laser Range Finder**: A device that measures distance by illuminating a target with a laser and analyzing the reflected light, commonly used in robotics for mapping and navigation.

**LIDAR (Light Detection and Ranging)**: A remote sensing method that uses light in the form of a pulsed laser to measure distances, creating high-resolution maps.

**Localization**: The process of determining the position and orientation of a robot within a known or unknown environment.

**Logistics Robot**: An autonomous robot designed to transport goods in warehouses, factories, or other facilities, improving efficiency and reducing labor costs.

**Long Short-Term Memory (LSTM)**: A type of recurrent neural network architecture that can learn long-term dependencies, useful in robotics for sequence modeling and prediction.

## M

**Machine Learning**: A branch of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed, crucial for robot perception and decision-making.

**Manipulator**: A robot arm designed to manipulate objects in the environment, consisting of links connected by joints.

**Mapping**: The process of creating a representation of the environment, typically as a 2D or 3D model, used for navigation and localization.

**Marker Detection**: The process of identifying and locating predefined visual markers in images, used for robot localization and augmented reality applications.

**Middleware**: Software that provides common services and capabilities to applications beyond what's offered by the operating system, such as ROS for robotics.

**Mobile Robot**: A robot that can move around in its environment, as opposed to fixed manipulators or stationary systems.

**Monte Carlo Localization (MCL)**: A probabilistic algorithm for robot localization that uses particle filters to represent the probability distribution of the robot's pose.

**Motion Planning**: The computational problem of finding a valid sequence of configurations to move an object from a start to a goal without collisions.

## N

**Navigation**: The process of planning and executing a path for a robot to move from one location to another while avoiding obstacles.

**Neural Network**: A computing system inspired by the human brain, consisting of interconnected nodes that process information, fundamental to AI in robotics.

**Node**: In ROS, a process that performs computation, the basic building block of ROS programs.

**Non-Holonomic Constraint**: A constraint on a robot's motion that cannot be integrated to give a constraint on position alone, such as the constraint that a wheeled robot cannot move sideways.

## O

**Occupancy Grid**: A probabilistic representation of space where each cell in a grid contains the probability that the cell is occupied by an obstacle.

**Odometry**: The use of data from motion sensors to estimate change in position over time, commonly used for robot localization.

**Open Source Robotics Foundation (OSRF)**: The organization that develops and maintains Gazebo and supports the open-source robotics community.

**Operational Space**: The space in which a robot's end-effector operates, typically Cartesian space for manipulation tasks.

**Optimization**: The mathematical process of finding the best solution according to some criterion, widely used in robotics for control and planning.

**Otsu's Method**: An algorithm for automatic image thresholding, used in robotics for segmenting objects from backgrounds in computer vision applications.

## P

**Path Planning**: The computational process of finding a path from a start point to a goal point, considering obstacles and other constraints.

**Particle Filter**: A recursive Bayesian estimation algorithm that represents the posterior distribution of the estimated state using a set of random particles, used in robot localization.

**Perception**: The process by which robots acquire and interpret sensory information about their environment.

**PID Controller**: A control loop feedback mechanism widely used in robotics to minimize the error between a desired value and the actual value.

**Point Cloud**: A set of data points in space, typically representing the external surface of an object, used in 3D mapping and perception.

**Pose**: The position and orientation of an object in space, typically represented as a position vector and rotation matrix or quaternion.

**Proportional-Integral-Derivative (PID) Control**: A control algorithm that uses three terms to adjust the control output based on the error, its integral, and its derivative.

## Q

**Quaternion**: A mathematical representation of rotation in 3D space, used in robotics to avoid gimbal lock and provide smooth interpolation between orientations.

**Q-Learning**: A reinforcement learning algorithm that learns a policy telling an agent what action to take under what circumstances, applicable to robot decision-making.

## R

**RANSAC (Random Sample Consensus)**: An iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, used in robotics for model fitting.

**Range Sensor**: A sensor that measures distances to objects, including ultrasonic sensors, infrared sensors, and LIDAR.

**Real-Time Operating System (RTOS)**: An operating system designed to process data with very little latency, important for robot control systems.

**Reinforcement Learning**: A type of machine learning where an agent learns to make decisions by performing actions and receiving rewards or penalties.

**Robot Operating System (ROS)**: A flexible framework for writing robot software, providing services for hardware abstraction, device drivers, and message passing.

**Robotics Middleware**: Software infrastructure that provides common services and capabilities for robot applications, such as ROS.

**ROS 2**: The second generation of the Robot Operating System, designed for production environments with improved security and real-time capabilities.

**RRT (Rapidly-exploring Random Tree)**: A path planning algorithm that builds a tree of possible paths by randomly sampling the configuration space.

## S

**Sensor Fusion**: The process of combining data from multiple sensors to improve the accuracy and reliability of information, crucial for robot perception.

**Simultaneous Localization and Mapping (SLAM)**: The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it.

**Singularity**: A configuration of a robotic manipulator where the robot loses one or more degrees of freedom, making it unable to move in certain directions.

**Social Robot**: A robot designed to interact with humans in a socially acceptable way, often featuring human-like characteristics and social behaviors.

**Software Development Kit (SDK)**: A collection of software tools and documentation to assist developers in creating applications for specific platforms, such as robotics platforms.

**State Estimation**: The process of estimating the internal state of a system from noisy measurements, fundamental to robot control and navigation.

**Stereovision**: The process of extracting 3D information from 2D images captured by two or more cameras, mimicking human depth perception.

**Sweep**: In robotics, a complete scan of the environment using a range sensor, often used in mapping and navigation.

## T

**Teleoperation**: The remote operation of a robot by a human operator, often used in dangerous or inaccessible environments.

**Trajectory**: A time-parameterized path that includes position, velocity, and acceleration information, used for smooth robot motion execution.

**TurtleBot**: A low-cost, personal robot kit that provides a platform for learning and research in robotics and computer science.

**Twist Message**: In ROS, a message type that represents the velocity of a robot in free space, with linear and angular components.

## U

**Unmanned Aerial Vehicle (UAV)**: An aircraft without a human pilot aboard, commonly known as a drone, used in various robotics applications.

**Unmanned Ground Vehicle (UGV)**: A land vehicle without a human operator aboard, used for various robotics applications including exploration and logistics.

**Ultrasonic Sensor**: A sensor that uses sound waves above the range of human hearing to measure distances, commonly used in robotics for obstacle detection.

**Unicycle Model**: A kinematic model of robot motion that treats the robot as a unicycle with forward velocity and angular velocity, commonly used in path planning.

**URDF (Unified Robot Description Format)**: An XML format for representing a robot model in ROS, including kinematic and dynamic properties.

## V

**Velocity Obstacle**: A method for collision avoidance that defines regions in velocity space that would lead to collisions, used in robot navigation.

**Visual Servoing**: A control strategy that uses visual feedback to control robot motion, commonly used in manipulation tasks.

**Volatile Memory**: Memory that loses its contents when power is removed, such as RAM, important for understanding robot system design.

**VSLAM (Visual SLAM)**: Simultaneous Localization and Mapping using visual sensors such as cameras instead of other range sensors.

## W

**Wheel Odometry**: The use of wheel encoders to estimate a robot's position and orientation based on wheel rotation measurements.

**Wheeled Mobile Robot**: A robot that moves using wheels, the most common type of ground mobile robot.

**Wi-Fi**: A technology for wireless local area networking, commonly used in robotics for communication and control.

**World Coordinate System**: A fixed reference frame used to describe positions and orientations of objects in the environment.

## X, Y, Z

**X, Y, Z Axes**: The three perpendicular axes that define 3D space, with X typically pointing forward, Y pointing left, and Z pointing up in robotics.

**Yaw**: The rotation around the vertical (Z) axis, one of the three rotational degrees of freedom in 3D space.

**ZMP (Zero Moment Point)**: A concept in bipedal robotics that describes the point on the ground where the sum of all moments of the active forces equals zero, important for balance control.

## Greek Letters Commonly Used

**Δ (Delta)**: Often represents change or difference in robotics equations.

**θ (Theta)**: Commonly used to represent angles, particularly joint angles in robotic manipulators.

**λ (Lambda)**: Often used to represent wavelengths in sensor models or eigenvalues in optimization problems.

**μ (Mu)**: Represents friction coefficients or mean values in probabilistic robotics.

**σ (Sigma)**: Represents standard deviation in probabilistic models or conductivity in physical models.

**τ (Tau)**: Represents torque in robotics equations or time constants in control systems.

**φ (Phi)**: Often represents angles, particularly azimuth angles in 3D coordinate systems.

**ψ (Psi)**: Commonly used to represent the yaw angle in 3D orientation representations.

**ω (Omega)**: Represents angular velocity in robotics kinematics and dynamics.

This glossary provides definitions for essential robotics terminology, from basic concepts to advanced topics, serving as a reference for readers throughout their robotics education journey.