"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[4523],{4881(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"capstone-project/autonomous-humanoid","title":"Autonomous Humanoid Capstone Project","description":"A comprehensive capstone project integrating all concepts from the book","source":"@site/docs/capstone-project/autonomous-humanoid.md","sourceDirName":"capstone-project","slug":"/capstone-project/autonomous-humanoid","permalink":"/physical-ai-humanoid-robotics-book/docs/capstone-project/autonomous-humanoid","draft":false,"unlisted":false,"editUrl":"https://github.com/your-organization/physical-ai-humanoid-robotics-book/edit/main/book/docs/capstone-project/autonomous-humanoid.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Autonomous Humanoid Capstone Project","description":"A comprehensive capstone project integrating all concepts from the book","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Assessment Questions for Conversational Robotics","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/assessment-questions"},"next":{"title":"Capstone Architecture Design","permalink":"/physical-ai-humanoid-robotics-book/docs/capstone-project/capstone-architecture"}}');var s=t(4848),i=t(8453);const a={title:"Autonomous Humanoid Capstone Project",description:"A comprehensive capstone project integrating all concepts from the book",sidebar_position:1},r="Autonomous Humanoid Capstone Project",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Project Objectives",id:"project-objectives",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"Capstone Project Components",id:"capstone-project-components",level:2},{value:"1. Voice Command Processing System",id:"1-voice-command-processing-system",level:3},{value:"2. Integrated Navigation and Path Planning",id:"2-integrated-navigation-and-path-planning",level:3},{value:"3. Perception and Object Recognition System",id:"3-perception-and-object-recognition-system",level:3},{value:"4. Manipulation and Control System",id:"4-manipulation-and-control-system",level:3},{value:"System Integration",id:"system-integration",level:2},{value:"Main Orchestration Node",id:"main-orchestration-node",level:3},{value:"Simulation Environment Setup",id:"simulation-environment-setup",level:2},{value:"Gazebo World for Capstone Project",id:"gazebo-world-for-capstone-project",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Unit Tests",id:"unit-tests",level:3},{value:"Integration Tests",id:"integration-tests",level:3},{value:"Deployment Considerations",id:"deployment-considerations",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"autonomous-humanoid-capstone-project",children:"Autonomous Humanoid Capstone Project"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"The Autonomous Humanoid Capstone Project synthesizes all concepts learned throughout this book into a comprehensive application. In this project, you'll design and implement an autonomous humanoid robot that can receive voice commands, plan paths, navigate obstacles, identify objects using computer vision, and manipulate them appropriately."}),"\n",(0,s.jsx)(n.p,{children:"This capstone project demonstrates the integration of:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"ROS 2 fundamentals for robot communication"}),"\n",(0,s.jsx)(n.li,{children:"Gazebo simulation for testing and validation"}),"\n",(0,s.jsx)(n.li,{children:"NVIDIA Isaac platform for AI-powered perception and navigation"}),"\n",(0,s.jsx)(n.li,{children:"Conversational AI for voice command processing"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"project-objectives",children:"Project Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By completing this capstone project, you will:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate all major components learned in previous modules"}),"\n",(0,s.jsx)(n.li,{children:"Design a complete autonomous robotic system"}),"\n",(0,s.jsx)(n.li,{children:"Implement voice-controlled navigation and manipulation"}),"\n",(0,s.jsx)(n.li,{children:"Create a system that demonstrates embodied intelligence"}),"\n",(0,s.jsx)(n.li,{children:"Validate your system in simulation before real-world deployment"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,s.jsx)(n.p,{children:"The autonomous humanoid system consists of several interconnected modules:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Voice Input   \u2502\u2500\u2500\u2500\u25b6\u2502  NLP & Planning  \u2502\u2500\u2500\u2500\u25b6\u2502  Navigation &   \u2502\n\u2502   Processing    \u2502    \u2502     Module       \u2502    \u2502  Path Planning  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                        \u2502                       \u2502\n         \u25bc                        \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Whisper ASR    \u2502    \u2502  LLM Cognitive   \u2502    \u2502   ROS 2 Action  \u2502\n\u2502  Integration    \u2502    \u2502   Planner        \u2502    \u2502   Execution     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                        \u2502                       \u2502\n         \u25bc                        \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Audio Capture  \u2502    \u2502  Task Manager    \u2502    \u2502  Manipulation   \u2502\n\u2502   & Preproc.    \u2502    \u2502     & Safety     \u2502    \u2502   Controller    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h2,{id:"capstone-project-components",children:"Capstone Project Components"}),"\n",(0,s.jsx)(n.h3,{id:"1-voice-command-processing-system",children:"1. Voice Command Processing System"}),"\n",(0,s.jsx)(n.p,{children:"The voice command processing system handles natural language input and converts it to actionable robot commands."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Key Features:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Real-time speech recognition using OpenAI Whisper"}),"\n",(0,s.jsx)(n.li,{children:"Natural language understanding for command interpretation"}),"\n",(0,s.jsx)(n.li,{children:"Context awareness for multi-turn conversations"}),"\n",(0,s.jsx)(n.li,{children:"Error handling and clarification requests"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Implementation:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nimport openai\nimport whisper\nimport json\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import Image, LaserScan\n\nclass VoiceCommandProcessor(Node):\n    def __init__(self):\n        super().__init__(\'voice_command_processor\')\n        \n        # Initialize Whisper model for speech recognition\n        self.whisper_model = whisper.load_model("base")\n        \n        # Initialize OpenAI API for natural language processing\n        openai.api_key = "YOUR_API_KEY_HERE"\n        \n        # Subscriptions\n        self.audio_sub = self.create_subscription(\n            String,  # In practice, you\'d use audio data\n            \'audio_input\',\n            self.audio_callback,\n            10\n        )\n        \n        # Publishers\n        self.command_pub = self.create_publisher(String, \'parsed_command\', 10)\n        self.speech_pub = self.create_publisher(String, \'robot_speech\', 10)\n        \n        # Robot state tracking\n        self.robot_state = {\n            \'position\': {\'x\': 0, \'y\': 0, \'theta\': 0},\n            \'battery_level\': 100,\n            \'attached_object\': None,\n            \'environment\': \'indoor_office\',\n            \'last_command_time\': 0\n        }\n        \n        self.get_logger().info(\'Voice Command Processor initialized\')\n\n    def audio_callback(self, msg):\n        """Process audio input and convert to robot command"""\n        # In a real implementation, this would process actual audio data\n        # For this example, we\'ll simulate the process\n        \n        # Simulate Whisper transcription\n        transcribed_text = msg.data  # This would come from Whisper processing\n        self.get_logger().info(f\'Transcribed: {transcribed_text}\')\n        \n        # Process with LLM for command interpretation\n        command_plan = self.interpret_command(transcribed_text)\n        \n        if command_plan:\n            # Publish the interpreted command\n            cmd_msg = String()\n            cmd_msg.data = json.dumps(command_plan)\n            self.command_pub.publish(cmd_msg)\n            \n            self.get_logger().info(f\'Command plan generated: {command_plan}\')\n        else:\n            # Request clarification\n            self.request_clarification(transcribed_text)\n\n    def interpret_command(self, command_text):\n        """Interpret natural language command using LLM"""\n        prompt = f"""\n        You are a command interpreter for an autonomous humanoid robot.\n        The robot can perform these actions:\n        - navigate_to(location)\n        - pick_up_object(object_type)\n        - place_object(location)\n        - detect_object(object_type)\n        - follow_person(person_name)\n        - avoid_obstacle(obstacle_type)\n        - return_to_charging_station()\n        \n        Current robot state:\n        {json.dumps(self.robot_state, indent=2)}\n        \n        User command: "{command_text}"\n\n        Respond with a JSON object containing the action plan:\n        {% raw %}\n        {{\n          "action": "action_name",\n          "parameters": {{"param1": "value1", "param2": "value2"}},\n          "confidence": 0.0-1.0\n        }}\n        {% endraw %}\n        \n        If the command is ambiguous, return empty parameters and low confidence.\n        """\n        \n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-3.5-turbo",\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.3,\n                max_tokens=300\n            )\n            \n            content = response.choices[0].message[\'content\'].strip()\n            start_idx = content.find(\'{\')\n            end_idx = content.rfind(\'}\') + 1\n            \n            if start_idx != -1 and end_idx != 0:\n                json_str = content[start_idx:end_idx]\n                plan = json.loads(json_str)\n                return plan\n            else:\n                self.get_logger().error(f\'Could not parse LLM response: {content}\')\n                return None\n                \n        except Exception as e:\n            self.get_logger().error(f\'Error interpreting command: {e}\')\n            return None\n\n    def request_clarification(self, original_command):\n        """Request clarification for ambiguous commands"""\n        clarification_msg = String()\n        clarification_msg.data = f"I didn\'t understand your command: \'{original_command}\'. Could you please rephrase?"\n        self.speech_pub.publish(clarification_msg)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-integrated-navigation-and-path-planning",children:"2. Integrated Navigation and Path Planning"}),"\n",(0,s.jsx)(n.p,{children:"The navigation system combines perception data with path planning algorithms to enable safe and efficient movement."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Key Features:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integration with NVIDIA Isaac for perception"}),"\n",(0,s.jsx)(n.li,{children:"SLAM for localization and mapping"}),"\n",(0,s.jsx)(n.li,{children:"Dynamic path planning with obstacle avoidance"}),"\n",(0,s.jsx)(n.li,{children:"Multi-floor navigation support"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Implementation:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom nav_msgs.msg import OccupancyGrid, Path\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom sensor_msgs.msg import LaserScan, Image\nfrom std_msgs.msg import String\nimport numpy as np\nimport cv2\nfrom scipy.spatial import distance\n\nclass IntegratedNavigationPlanner(Node):\n    def __init__(self):\n        super().__init__(\'integrated_navigation_planner\')\n        \n        # Subscriptions\n        self.map_sub = self.create_subscription(\n            OccupancyGrid,\n            \'map\',\n            self.map_callback,\n            10\n        )\n        \n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            \'scan\',\n            self.scan_callback,\n            10\n        )\n        \n        self.command_sub = self.create_subscription(\n            String,\n            \'parsed_command\',\n            self.command_callback,\n            10\n        )\n        \n        # Publishers\n        self.path_pub = self.create_publisher(Path, \'global_plan\', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.goal_pub = self.create_publisher(PoseStamped, \'goal_pose\', 10)\n        \n        # Navigation state\n        self.map_data = None\n        self.current_pose = None\n        self.goal_pose = None\n        self.local_obstacles = []\n        \n        # Navigation parameters\n        self.linear_vel = 0.5  # m/s\n        self.angular_vel = 0.5  # rad/s\n        self.safe_distance = 0.5  # meters\n        \n        self.get_logger().info(\'Integrated Navigation Planner initialized\')\n\n    def map_callback(self, msg):\n        """Update internal map representation"""\n        self.map_data = {\n            \'data\': np.array(msg.data).reshape(msg.info.height, msg.info.width),\n            \'resolution\': msg.info.resolution,\n            \'origin\': msg.origin\n        }\n\n    def scan_callback(self, msg):\n        """Process laser scan for local obstacle detection"""\n        # Convert laser scan to obstacle positions\n        angles = np.linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        valid_ranges = np.array(msg.ranges)\n        valid_ranges[valid_ranges > msg.range_max] = np.inf\n        \n        # Calculate obstacle positions in robot frame\n        x = valid_ranges * np.cos(angles)\n        y = valid_ranges * np.sin(angles)\n        \n        # Filter out infinite ranges\n        finite_mask = np.isfinite(valid_ranges)\n        self.local_obstacles = np.column_stack((x[finite_mask], y[finite_mask]))\n\n    def command_callback(self, msg):\n        """Process navigation commands"""\n        try:\n            command_plan = json.loads(msg.data)\n            action = command_plan[\'action\']\n            \n            if action == \'navigate_to\':\n                location = command_plan[\'parameters\'].get(\'location\')\n                self.navigate_to_location(location)\n            elif action == \'return_to_charging_station\':\n                self.return_to_charging_station()\n                \n        except json.JSONDecodeError as e:\n            self.get_logger().error(f\'Error parsing command: {e}\')\n\n    def navigate_to_location(self, location):\n        """Navigate to a specified location"""\n        # In a real implementation, this would:\n        # 1. Convert location name to coordinates\n        # 2. Plan a global path using A* or similar algorithm\n        # 3. Execute the path with local obstacle avoidance\n        \n        # For this example, we\'ll simulate navigation to a position\n        target_x, target_y = self.get_coordinates_for_location(location)\n        \n        if target_x is not None and target_y is not None:\n            self.get_logger().info(f\'Navigating to {location} at ({target_x}, {target_y})\')\n            self.move_to_position(target_x, target_y)\n        else:\n            self.get_logger().warn(f\'Unknown location: {location}\')\n\n    def get_coordinates_for_location(self, location):\n        """Convert location name to coordinates"""\n        # In a real system, this would use a map or semantic localization\n        location_map = {\n            \'kitchen\': (5.0, 3.0),\n            \'living_room\': (2.0, 8.0),\n            \'office\': (8.0, 2.0),\n            \'bedroom\': (7.0, 7.0),\n            \'charging_station\': (0.0, 0.0)\n        }\n        \n        return location_map.get(location, (None, None))\n\n    def move_to_position(self, target_x, target_y):\n        """Move robot to target position with obstacle avoidance"""\n        # Simple proportional controller for demonstration\n        while rclpy.ok():\n            # Get current position (in practice, from localization system)\n            current_x, current_y = self.get_current_position()\n            \n            # Calculate distance and angle to target\n            dx = target_x - current_x\n            dy = target_y - current_y\n            distance_to_target = np.sqrt(dx**2 + dy**2)\n            \n            # Check if we\'re close enough to target\n            if distance_to_target < 0.2:  # 20 cm tolerance\n                self.get_logger().info(\'Reached target position\')\n                self.stop_robot()\n                break\n            \n            # Calculate desired heading\n            desired_theta = np.arctan2(dy, dx)\n            current_theta = self.get_current_orientation()  # Implement this method\n            \n            # Calculate heading error\n            heading_error = desired_theta - current_theta\n            # Normalize angle to [-\u03c0, \u03c0]\n            while heading_error > np.pi:\n                heading_error -= 2 * np.pi\n            while heading_error < -np.pi:\n                heading_error += 2 * np.pi\n            \n            # Create velocity command\n            cmd = Twist()\n            \n            # Proportional controller for linear velocity\n            cmd.linear.x = min(self.linear_vel, distance_to_target * 0.5)\n            \n            # Proportional controller for angular velocity\n            cmd.angular.z = min(self.angular_vel, max(-self.angular_vel, heading_error * 1.0))\n            \n            # Check for obstacles before moving\n            if self.has_obstacle_ahead():\n                cmd.linear.x = 0.0  # Stop if obstacle detected\n                self.get_logger().warn(\'Obstacle detected ahead, stopping\')\n            \n            # Publish command\n            self.cmd_vel_pub.publish(cmd)\n            \n            # Small delay\n            self.get_clock().sleep_for(rclpy.duration.Duration(seconds=0.1))\n\n    def has_obstacle_ahead(self):\n        """Check if there\'s an obstacle in front of the robot"""\n        # Check if any local obstacle is within safe distance and in front of robot\n        for obs_x, obs_y in self.local_obstacles:\n            # Only consider obstacles in front of the robot (positive x direction)\n            if obs_x > 0 and abs(obs_y) < 0.5:  # Within 50cm laterally\n                if obs_x < self.safe_distance:  # Within safe distance\n                    return True\n        return False\n\n    def get_current_position(self):\n        """Get current robot position (implement with localization system)"""\n        # Placeholder implementation\n        # In a real system, this would come from AMCL, odometry, or other localization\n        return (0.0, 0.0)\n\n    def get_current_orientation(self):\n        """Get current robot orientation (implement with localization system)"""\n        # Placeholder implementation\n        return 0.0\n\n    def stop_robot(self):\n        """Stop robot movement"""\n        cmd = Twist()\n        self.cmd_vel_pub.publish(cmd)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-perception-and-object-recognition-system",children:"3. Perception and Object Recognition System"}),"\n",(0,s.jsx)(n.p,{children:"The perception system uses computer vision and NVIDIA Isaac to identify and locate objects in the environment."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Key Features:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Real-time object detection and classification"}),"\n",(0,s.jsx)(n.li,{children:"3D pose estimation for manipulation"}),"\n",(0,s.jsx)(n.li,{children:"Integration with Isaac for AI-powered perception"}),"\n",(0,s.jsx)(n.li,{children:"Multi-camera support for enhanced perception"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Implementation:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport json\n\nclass PerceptionSystem(Node):\n    def __init__(self):\n        super().__init__('perception_system')\n        \n        # Initialize OpenCV bridge\n        self.bridge = CvBridge()\n        \n        # Subscriptions\n        self.image_sub = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.image_callback,\n            10\n        )\n        \n        self.command_sub = self.create_subscription(\n            String,\n            'parsed_command',\n            self.command_callback,\n            10\n        )\n        \n        # Publishers\n        self.object_detection_pub = self.create_publisher(String, 'detected_objects', 10)\n        self.manipulation_pub = self.create_publisher(String, 'manipulation_command', 10)\n        \n        # Perception state\n        self.latest_image = None\n        self.detected_objects = []\n        \n        # Object detection model (in practice, use Isaac or other AI model)\n        # For this example, we'll use a simple color-based detection\n        self.object_colors = {\n            'red_cup': ([0, 50, 50], [10, 255, 255]),\n            'blue_bottle': ([100, 50, 50], [130, 255, 255]),\n            'green_box': ([50, 50, 50], [70, 255, 255])\n        }\n        \n        self.get_logger().info('Perception System initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera image\"\"\"\n        try:\n            # Convert ROS Image message to OpenCV image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n            self.latest_image = cv_image\n            \n            # Perform object detection\n            detected_objects = self.detect_objects(cv_image)\n            \n            # Update internal state\n            self.detected_objects = detected_objects\n            \n            # Publish detected objects\n            detection_msg = String()\n            detection_msg.data = json.dumps(detected_objects)\n            self.object_detection_pub.publish(detection_msg)\n            \n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def detect_objects(self, image):\n        \"\"\"Detect objects in the image using color-based detection\"\"\"\n        objects = []\n        \n        for obj_name, (lower_color, upper_color) in self.object_colors.items():\n            # Create mask for the color range\n            lower = np.array(lower_color)\n            upper = np.array(upper_color)\n            mask = cv2.inRange(image, lower, upper)\n            \n            # Find contours in the mask\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            \n            for contour in contours:\n                # Filter by size to avoid noise\n                area = cv2.contourArea(contour)\n                if area > 500:  # Minimum area threshold\n                    # Calculate bounding box\n                    x, y, w, h = cv2.boundingRect(contour)\n                    \n                    # Calculate center position\n                    center_x = x + w // 2\n                    center_y = y + h // 2\n                    \n                    # Calculate image coordinates (0-1 normalized)\n                    img_width, img_height = image.shape[1], image.shape[0]\n                    norm_x = center_x / img_width\n                    norm_y = center_y / img_height\n                    \n                    # Add detected object\n                    objects.append({\n                        'name': obj_name,\n                        'center': {'x': center_x, 'y': center_y},\n                        'normalized_center': {'x': norm_x, 'y': norm_y},\n                        'bbox': {'x': x, 'y': y, 'width': w, 'height': h},\n                        'area': area\n                    })\n        \n        return objects\n\n    def command_callback(self, msg):\n        \"\"\"Process commands related to object detection and manipulation\"\"\"\n        try:\n            command_plan = json.loads(msg.data)\n            action = command_plan['action']\n            \n            if action == 'detect_object':\n                obj_type = command_plan['parameters'].get('object_type', 'any')\n                self.handle_detect_object_command(obj_type)\n            elif action == 'pick_up_object':\n                self.handle_pick_up_command()\n                \n        except json.JSONDecodeError as e:\n            self.get_logger().error(f'Error parsing command: {e}')\n\n    def handle_detect_object_command(self, obj_type):\n        \"\"\"Handle object detection command\"\"\"\n        if self.latest_image is None:\n            self.get_logger().warn('No image available for object detection')\n            return\n        \n        # If specific object type requested, filter results\n        if obj_type != 'any' and obj_type != 'all':\n            matching_objects = [obj for obj in self.detected_objects if obj['name'] == obj_type]\n        else:\n            matching_objects = self.detected_objects\n        \n        # Publish results\n        result_msg = String()\n        result_msg.data = json.dumps({\n            'command': 'detect_object',\n            'object_type': obj_type,\n            'objects_found': matching_objects,\n            'timestamp': self.get_clock().now().nanoseconds\n        })\n        \n        self.object_detection_pub.publish(result_msg)\n\n    def handle_pick_up_command(self):\n        \"\"\"Handle object pickup command\"\"\"\n        if not self.detected_objects:\n            self.get_logger().warn('No objects detected for pickup')\n            return\n        \n        # For this example, pick up the largest detected object\n        largest_obj = max(self.detected_objects, key=lambda obj: obj['area'])\n        \n        # Create manipulation command\n        manipulation_cmd = {\n            'action': 'grasp_object',\n            'object': largest_obj,\n            'approach_vector': [0, 0, -1],  # Approach from above\n            'grasp_type': 'top_grasp'\n        }\n        \n        cmd_msg = String()\n        cmd_msg.data = json.dumps(manipulation_cmd)\n        self.manipulation_pub.publish(cmd_msg)\n        \n        self.get_logger().info(f'Attempting to pick up {largest_obj[\"name\"]}')\n"})}),"\n",(0,s.jsx)(n.h3,{id:"4-manipulation-and-control-system",children:"4. Manipulation and Control System"}),"\n",(0,s.jsx)(n.p,{children:"The manipulation system handles the physical interaction with objects in the environment."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Key Features:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Precise control of robotic arms and grippers"}),"\n",(0,s.jsx)(n.li,{children:"Integration with perception for object manipulation"}),"\n",(0,s.jsx)(n.li,{children:"Safety checks and force control"}),"\n",(0,s.jsx)(n.li,{children:"Grasp planning and execution"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Implementation:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose, Point, Quaternion\nfrom sensor_msgs.msg import JointState\nimport json\nimport numpy as np\n\nclass ManipulationController(Node):\n    def __init__(self):\n        super().__init__('manipulation_controller')\n        \n        # Subscriptions\n        self.manipulation_sub = self.create_subscription(\n            String,\n            'manipulation_command',\n            self.manipulation_callback,\n            10\n        )\n        \n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            'joint_states',\n            self.joint_state_callback,\n            10\n        )\n        \n        # Publishers\n        self.joint_cmd_pub = self.create_publisher(JointState, 'joint_commands', 10)\n        self.status_pub = self.create_publisher(String, 'manipulation_status', 10)\n        \n        # Manipulation state\n        self.joint_positions = {}\n        self.is_gripper_open = True\n        \n        # Robot arm parameters (example for a simple 6-DOF arm)\n        self.joint_names = ['joint_1', 'joint_2', 'joint_3', \n                           'joint_4', 'joint_5', 'joint_6', 'gripper_joint']\n        \n        self.get_logger().info('Manipulation Controller initialized')\n\n    def joint_state_callback(self, msg):\n        \"\"\"Update current joint positions\"\"\"\n        for i, name in enumerate(msg.name):\n            if i < len(msg.position):\n                self.joint_positions[name] = msg.position[i]\n\n    def manipulation_callback(self, msg):\n        \"\"\"Process manipulation commands\"\"\"\n        try:\n            command = json.loads(msg.data)\n            action = command['action']\n            \n            if action == 'grasp_object':\n                obj_info = command['object']\n                grasp_type = command.get('grasp_type', 'top_grasp')\n                \n                self.execute_grasp(obj_info, grasp_type)\n            elif action == 'release_object':\n                self.execute_release()\n            elif action == 'move_to_pose':\n                pose = command['pose']\n                self.move_to_pose(pose)\n                \n        except json.JSONDecodeError as e:\n            self.get_logger().error(f'Error parsing manipulation command: {e}')\n\n    def execute_grasp(self, object_info, grasp_type='top_grasp'):\n        \"\"\"Execute grasping of an object\"\"\"\n        self.get_logger().info(f'Executing grasp for {object_info[\"name\"]}')\n        \n        # In a real system, this would:\n        # 1. Plan approach trajectory based on object pose\n        # 2. Execute approach motion\n        # 3. Close gripper with appropriate force\n        # 4. Verify grasp success\n        \n        # For this example, we'll simulate the process\n        if grasp_type == 'top_grasp':\n            # Approach from above\n            self.approach_from_above(object_info)\n        \n        # Close gripper\n        self.close_gripper()\n        \n        # Lift object\n        self.lift_object()\n        \n        # Publish status\n        status_msg = String()\n        status_msg.data = json.dumps({\n            'action': 'grasp',\n            'object': object_info['name'],\n            'status': 'completed',\n            'timestamp': self.get_clock().now().nanoseconds\n        })\n        \n        self.status_pub.publish(status_msg)\n\n    def approach_from_above(self, object_info):\n        \"\"\"Approach object from above for top grasp\"\"\"\n        # Calculate approach position (slightly above object)\n        approach_z = 0.2  # 20cm above object\n        \n        # In a real system, this would calculate inverse kinematics\n        # to move end-effector to the calculated position\n        self.get_logger().info(f'Approaching {object_info[\"name\"]} from above')\n\n    def close_gripper(self):\n        \"\"\"Close the gripper to grasp object\"\"\"\n        # Create joint command to close gripper\n        cmd = JointState()\n        cmd.name = ['gripper_joint']\n        cmd.position = [0.0]  # Closed position\n        cmd.velocity = [0.1]  # Closing velocity\n        cmd.effort = [50.0]   # Maximum effort\n        \n        self.joint_cmd_pub.publish(cmd)\n        self.is_gripper_open = False\n        \n        # Wait for gripper to close\n        self.get_clock().sleep_for(rclpy.duration.Duration(seconds=1.0))\n\n    def lift_object(self):\n        \"\"\"Lift the grasped object\"\"\"\n        # In a real system, this would move the arm upward\n        self.get_logger().info('Lifting object')\n\n    def execute_release(self):\n        \"\"\"Release the currently grasped object\"\"\"\n        self.get_logger().info('Releasing object')\n        \n        # Open gripper\n        self.open_gripper()\n        \n        # Publish status\n        status_msg = String()\n        status_msg.data = json.dumps({\n            'action': 'release',\n            'status': 'completed',\n            'timestamp': self.get_clock().now().nanoseconds\n        })\n        \n        self.status_pub.publish(status_msg)\n\n    def open_gripper(self):\n        \"\"\"Open the gripper\"\"\"\n        # Create joint command to open gripper\n        cmd = JointState()\n        cmd.name = ['gripper_joint']\n        cmd.position = [0.8]  # Open position\n        cmd.velocity = [0.1]  # Opening velocity\n        cmd.effort = [50.0]   # Maximum effort\n        \n        self.joint_cmd_pub.publish(cmd)\n        self.is_gripper_open = True\n        \n        # Wait for gripper to open\n        self.get_clock().sleep_for(rclpy.duration.Duration(seconds=1.0))\n\n    def move_to_pose(self, pose):\n        \"\"\"Move end-effector to specified pose\"\"\"\n        # In a real system, this would calculate inverse kinematics\n        # and generate joint trajectories\n        self.get_logger().info(f'Moving to pose: {pose}')\n"})}),"\n",(0,s.jsx)(n.h2,{id:"system-integration",children:"System Integration"}),"\n",(0,s.jsx)(n.h3,{id:"main-orchestration-node",children:"Main Orchestration Node"}),"\n",(0,s.jsx)(n.p,{children:"The main orchestration node ties all components together and manages the overall system behavior:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\n\nclass CapstoneOrchestrator(Node):\n    def __init__(self):\n        super().__init__('capstone_orchestrator')\n        \n        # Subscriptions for all subsystems\n        self.status_sub = self.create_subscription(\n            String,\n            'system_status',\n            self.status_callback,\n            10\n        )\n        \n        # Publishers\n        self.status_pub = self.create_publisher(String, 'system_status', 10)\n        \n        # System state\n        self.system_state = {\n            'voice_processing': 'idle',\n            'navigation': 'idle',\n            'perception': 'idle',\n            'manipulation': 'idle',\n            'overall_status': 'ready'\n        }\n        \n        # Start system status timer\n        self.status_timer = self.create_timer(1.0, self.publish_system_status)\n        \n        self.get_logger().info('Capstone Orchestrator initialized')\n\n    def status_callback(self, msg):\n        \"\"\"Update system status from subsystems\"\"\"\n        try:\n            status_data = json.loads(msg.data)\n            component = status_data.get('component')\n            status = status_data.get('status', 'unknown')\n            \n            if component in self.system_state:\n                self.system_state[component] = status\n                self.update_overall_status()\n                \n        except json.JSONDecodeError:\n            self.get_logger().error('Error parsing status message')\n\n    def update_overall_status(self):\n        \"\"\"Update overall system status based on component states\"\"\"\n        # Determine overall status based on critical components\n        critical_components = ['navigation', 'perception', 'manipulation']\n        \n        if any(self.system_state[comp] == 'error' for comp in critical_components):\n            self.system_state['overall_status'] = 'error'\n        elif any(self.system_state[comp] == 'busy' for comp in critical_components):\n            self.system_state['overall_status'] = 'executing_task'\n        else:\n            self.system_state['overall_status'] = 'ready'\n\n    def publish_system_status(self):\n        \"\"\"Publish overall system status\"\"\"\n        status_msg = String()\n        status_msg.data = json.dumps(self.system_state)\n        self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    \n    # Create all nodes\n    voice_processor = VoiceCommandProcessor()\n    navigation_planner = IntegratedNavigationPlanner()\n    perception_system = PerceptionSystem()\n    manipulation_controller = ManipulationController()\n    orchestrator = CapstoneOrchestrator()\n    \n    # Create executor and add nodes\n    executor = rclpy.executors.MultiThreadedExecutor()\n    executor.add_node(voice_processor)\n    executor.add_node(navigation_planner)\n    executor.add_node(perception_system)\n    executor.add_node(manipulation_controller)\n    executor.add_node(orchestrator)\n    \n    try:\n        executor.spin()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        # Cleanup\n        executor.shutdown()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"simulation-environment-setup",children:"Simulation Environment Setup"}),"\n",(0,s.jsx)(n.h3,{id:"gazebo-world-for-capstone-project",children:"Gazebo World for Capstone Project"}),"\n",(0,s.jsx)(n.p,{children:"Create a Gazebo world file that represents a home environment with various rooms, furniture, and objects:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0" ?>\n<sdf version="1.6">\n  <world name="capstone_world">\n    \x3c!-- Include the outdoor world --\x3e\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n    \n    <include>\n      <uri>model://sun</uri>\n    </include>\n    \n    \x3c!-- Home environment with furniture --\x3e\n    \x3c!-- Kitchen area --\x3e\n    <model name="kitchen_counter">\n      <pose>5 3 0 0 0 0</pose>\n      <include>\n        <uri>model://table</uri>\n      </include>\n    </model>\n    \n    <model name="kitchen_cup">\n      <pose>5.2 3.1 0.8 0 0 0</pose>\n      <include>\n        <uri>model://coke_can</uri>\n      </include>\n    </model>\n    \n    \x3c!-- Living room area --\x3e\n    <model name="sofa">\n      <pose>2 8 0 0 0 1.57</pose>\n      <include>\n        <uri>model://sofa</uri>\n      </include>\n    </model>\n    \n    <model name="coffee_table">\n      <pose>2.5 7.5 0 0 0 0</pose>\n      <include>\n        <uri>model://table</uri>\n      </include>\n    </model>\n    \n    \x3c!-- Office area --\x3e\n    <model name="office_desk">\n      <pose>8 2 0 0 0 0</pose>\n      <include>\n        <uri>model://table</uri>\n      </include>\n    </model>\n    \n    <model name="office_bottle">\n      <pose>8.2 2.1 0.8 0 0 0</pose>\n      <include>\n        <uri>model://water_bottle</uri>\n      </include>\n    </model>\n    \n    \x3c!-- Charging station --\x3e\n    <model name="charging_station">\n      <pose>0.5 0.5 0 0 0 0</pose>\n      <include>\n        <uri>model://charger</uri>\n      </include>\n    </model>\n    \n    \x3c!-- Walls to define rooms --\x3e\n    <model name="wall_1">\n      <pose>0 5 0 0 0 0</pose>\n      <link name="link">\n        <collision name="collision">\n          <geometry>\n            <box>\n              <size>10 0.2 2</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name="visual">\n          <geometry>\n            <box>\n              <size>10 0.2 2</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>0.8 0.8 0.8 1</ambient>\n            <diffuse>0.8 0.8 0.8 1</diffuse>\n          </material>\n        </visual>\n      </link>\n    </model>\n    \n    \x3c!-- Add more walls as needed --\x3e\n  </world>\n</sdf>\n'})}),"\n",(0,s.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,s.jsx)(n.h3,{id:"unit-tests",children:"Unit Tests"}),"\n",(0,s.jsx)(n.p,{children:"Create unit tests for each component of the system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import unittest\nimport rclpy\nfrom rclpy.executors import SingleThreadedExecutor\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport json\n\nclass TestVoiceCommandProcessor(unittest.TestCase):\n    def setUp(self):\n        rclpy.init()\n        self.node = VoiceCommandProcessor()\n        self.executor = SingleThreadedExecutor()\n        self.executor.add_node(self.node)\n\n    def tearDown(self):\n        self.node.destroy_node()\n        rclpy.shutdown()\n\n    def test_command_interpretation(self):\n        """Test that natural language commands are correctly interpreted"""\n        # This would test the interpret_command method with various inputs\n        command_text = "move to the kitchen"\n        result = self.node.interpret_command(command_text)\n        \n        self.assertIsNotNone(result)\n        self.assertEqual(result[\'action\'], \'navigate_to\')\n        self.assertEqual(result[\'parameters\'][\'location\'], \'kitchen\')\n\nclass TestNavigationPlanner(unittest.TestCase):\n    def setUp(self):\n        rclpy.init()\n        self.node = IntegratedNavigationPlanner()\n        self.executor = SingleThreadedExecutor()\n        self.executor.add_node(self.node)\n\n    def tearDown(self):\n        self.node.destroy_node()\n        rclpy.shutdown()\n\n    def test_obstacle_detection(self):\n        """Test that obstacles are properly detected"""\n        # Mock some obstacles in front of the robot\n        self.node.local_obstacles = [[0.3, 0.0], [0.4, 0.1]]  # Obstacles at 30cm and 40cm\n        has_obstacle = self.node.has_obstacle_ahead()\n        \n        self.assertTrue(has_obstacle)\n\nclass TestPerceptionSystem(unittest.TestCase):\n    def setUp(self):\n        rclpy.init()\n        self.node = PerceptionSystem()\n        self.executor = SingleThreadedExecutor()\n        self.executor.add_node(self.node)\n\n    def tearDown(self):\n        self.node.destroy_node()\n        rclpy.shutdown()\n\n    def test_object_detection(self):\n        """Test that objects are detected in sample images"""\n        # This would test the detect_objects method with sample images\n        # For now, we\'ll just ensure the method exists and doesn\'t crash\n        import numpy as np\n        test_image = np.zeros((480, 640, 3), dtype=np.uint8)\n        test_image[200:250, 300:350] = (0, 0, 255)  # Red square\n        \n        objects = self.node.detect_objects(test_image)\n        \n        # Check that at least one red object was detected\n        red_objects = [obj for obj in objects if \'red\' in obj[\'name\']]\n        self.assertGreaterEqual(len(red_objects), 1)\n\nif __name__ == \'__main__\':\n    unittest.main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"integration-tests",children:"Integration Tests"}),"\n",(0,s.jsx)(n.p,{children:"Create integration tests that verify the interaction between components:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import unittest\nimport rclpy\nfrom rclpy.executors import SingleThreadedExecutor\nfrom std_msgs.msg import String\nimport json\nimport threading\nimport time\n\nclass TestSystemIntegration(unittest.TestCase):\n    def setUp(self):\n        rclpy.init()\n        \n        # Create all nodes\n        self.voice_processor = VoiceCommandProcessor()\n        self.navigation_planner = IntegratedNavigationPlanner()\n        self.perception_system = PerceptionSystem()\n        self.manipulation_controller = ManipulationController()\n        self.orchestrator = CapstoneOrchestrator()\n        \n        # Create executor and add nodes\n        self.executor = SingleThreadedExecutor()\n        self.executor.add_node(self.voice_processor)\n        self.executor.add_node(self.navigation_planner)\n        self.executor.add_node(self.perception_system)\n        self.executor.add_node(self.manipulation_controller)\n        self.executor.add_node(self.orchestrator)\n        \n        # Start executor in a separate thread\n        self.executor_thread = threading.Thread(target=self.executor.spin)\n        self.executor_thread.start()\n\n    def tearDown(self):\n        self.executor.shutdown()\n        rclpy.shutdown()\n        self.executor_thread.join()\n\n    def test_voice_to_navigation(self):\n        """Test the complete pipeline from voice command to navigation"""\n        # Create a publisher for voice commands\n        voice_cmd_pub = self.voice_processor.create_publisher(\n            String, \'audio_input\', 10\n        )\n        \n        # Wait a bit for connections to establish\n        time.sleep(1.0)\n        \n        # Publish a navigation command\n        cmd_msg = String()\n        cmd_msg.data = "navigate to the kitchen"\n        voice_cmd_pub.publish(cmd_msg)\n        \n        # Wait for processing\n        time.sleep(2.0)\n        \n        # Verify that navigation commands were generated\n        # This would require checking if navigation commands were published\n        # For this test, we\'ll just verify that the system is running without errors\n\nif __name__ == \'__main__\':\n    unittest.main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"deployment-considerations",children:"Deployment Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,s.jsx)(n.p,{children:"For deploying the autonomous humanoid system on real hardware:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Computing Platform"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"NVIDIA Jetson AGX Xavier or Orin for edge AI processing"}),"\n",(0,s.jsx)(n.li,{children:"Sufficient RAM (16GB+) and storage (256GB+ SSD)"}),"\n",(0,s.jsx)(n.li,{children:"Real-time capable processor for control tasks"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Sensors"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"RGB-D camera (Intel RealSense, ZED, etc.)"}),"\n",(0,s.jsx)(n.li,{children:"LIDAR for navigation (2D or 3D depending on application)"}),"\n",(0,s.jsx)(n.li,{children:"IMU for orientation and balance"}),"\n",(0,s.jsx)(n.li,{children:"Microphone array for voice commands"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Actuators"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Robotic arm with 6+ DOF for manipulation"}),"\n",(0,s.jsx)(n.li,{children:"Gripper for object handling"}),"\n",(0,s.jsx)(n.li,{children:"Mobile base with differential or omni-directional drive"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Communication"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"WiFi for cloud services (optional)"}),"\n",(0,s.jsx)(n.li,{children:"Ethernet for reliable local communication"}),"\n",(0,s.jsx)(n.li,{children:"Real-time capable network for control loops"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,s.jsx)(n.p,{children:"When deploying the autonomous system:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Physical Safety"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Emergency stop mechanisms"}),"\n",(0,s.jsx)(n.li,{children:"Collision detection and avoidance"}),"\n",(0,s.jsx)(n.li,{children:"Force limiting on manipulators"}),"\n",(0,s.jsx)(n.li,{children:"Speed limitations in human environments"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Operational Safety"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Fail-safe behaviors when components fail"}),"\n",(0,s.jsx)(n.li,{children:"Graceful degradation of capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Human override capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Monitoring and logging systems"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Data Safety"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Privacy protection for voice and visual data"}),"\n",(0,s.jsx)(n.li,{children:"Secure communication channels"}),"\n",(0,s.jsx)(n.li,{children:"Data encryption at rest and in transit"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"The Autonomous Humanoid Capstone Project demonstrates the integration of multiple advanced robotics concepts into a cohesive system. By completing this project, you will have gained experience in:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Developing complex, multi-component robotic systems"}),"\n",(0,s.jsx)(n.li,{children:"Integrating perception, planning, and control"}),"\n",(0,s.jsx)(n.li,{children:"Working with state-of-the-art AI technologies"}),"\n",(0,s.jsx)(n.li,{children:"Implementing safety and validation procedures"}),"\n",(0,s.jsx)(n.li,{children:"Testing and validation in simulation before real-world deployment"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This project serves as a foundation for developing more advanced autonomous robotic systems and provides a framework for extending capabilities in specific application domains."})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>a,x:()=>r});var o=t(6540);const s={},i=o.createContext(s);function a(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);