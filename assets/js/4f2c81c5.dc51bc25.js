"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[4961],{4086(e,n,i){i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>u,frontMatter:()=>t,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"module-4-vision-language-action/openai-whisper-examples","title":"OpenAI Whisper Integration Code Examples","description":"Practical code examples for integrating OpenAI Whisper in robotics applications","source":"@site/docs/module-4-vision-language-action/openai-whisper-examples.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/openai-whisper-examples","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/openai-whisper-examples","draft":false,"unlisted":false,"editUrl":"https://github.com/your-organization/physical-ai-humanoid-robotics-book/edit/main/book/docs/module-4-vision-language-action/openai-whisper-examples.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"OpenAI Whisper Integration Code Examples","description":"Practical code examples for integrating OpenAI Whisper in robotics applications","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action Pipeline","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/voice-to-action"},"next":{"title":"LLM Cognitive Planning Code Examples","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/llm-cognitive-planning-examples"}}');var s=i(4848),o=i(8453);const t={title:"OpenAI Whisper Integration Code Examples",description:"Practical code examples for integrating OpenAI Whisper in robotics applications",sidebar_position:4},a="OpenAI Whisper Integration Code Examples",d={},l=[{value:"Overview",id:"overview",level:2},{value:"Basic Audio Capture and Processing",id:"basic-audio-capture-and-processing",level:2},{value:"Simple Audio Capture Node",id:"simple-audio-capture-node",level:3},{value:"Advanced Audio Processing",id:"advanced-audio-processing",level:2},{value:"Audio Preprocessing for Better Recognition",id:"audio-preprocessing-for-better-recognition",level:3},{value:"Streaming Audio Processing",id:"streaming-audio-processing",level:2},{value:"Continuous Audio Processing with VAD (Voice Activity Detection)",id:"continuous-audio-processing-with-vad-voice-activity-detection",level:3},{value:"Whisper with ROS 2 Actions",id:"whisper-with-ros-2-actions",level:2},{value:"Using ROS 2 Actions for Long-Running Transcription",id:"using-ros-2-actions-for-long-running-transcription",level:3},{value:"Error Handling and Robustness",id:"error-handling-and-robustness",level:2},{value:"Whisper Node with Comprehensive Error Handling",id:"whisper-node-with-comprehensive-error-handling",level:3},{value:"Summary",id:"summary",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"openai-whisper-integration-code-examples",children:"OpenAI Whisper Integration Code Examples"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This section provides practical code examples for integrating OpenAI Whisper into robotics applications. These examples demonstrate how to capture audio, process it through Whisper, and use the results for robotic control."}),"\n",(0,s.jsx)(n.h2,{id:"basic-audio-capture-and-processing",children:"Basic Audio Capture and Processing"}),"\n",(0,s.jsx)(n.h3,{id:"simple-audio-capture-node",children:"Simple Audio Capture Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nimport pyaudio\nimport numpy as np\nimport whisper\nimport threading\nfrom std_msgs.msg import String\n\nclass SimpleWhisperNode(Node):\n    def __init__(self):\n        super().__init__(\'simple_whisper_node\')\n        \n        # Publisher for recognized text\n        self.text_publisher = self.create_publisher(String, \'recognized_speech\', 10)\n        \n        # Load Whisper model\n        self.model = whisper.load_model("base")\n        \n        # Audio parameters\n        self.rate = 16000\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        \n        # Initialize PyAudio\n        self.audio = pyaudio.PyAudio()\n        \n        # Start audio recording\n        self.start_recording()\n        \n        self.get_logger().info(\'Simple Whisper Node initialized\')\n\n    def start_recording(self):\n        """Start audio recording in a separate thread"""\n        self.recording_thread = threading.Thread(target=self.record_audio)\n        self.recording_thread.daemon = True\n        self.recording_thread.start()\n\n    def record_audio(self):\n        """Record audio and process with Whisper"""\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n        \n        # Buffer to hold audio chunks\n        audio_buffer = []\n        buffer_duration = 3  # Process every 3 seconds\n        max_chunks = int(buffer_duration * self.rate / self.chunk)\n        \n        while rclpy.ok():\n            # Read audio chunk\n            data = stream.read(self.chunk)\n            audio_buffer.append(data)\n            \n            # If buffer is full, process the audio\n            if len(audio_buffer) >= max_chunks:\n                # Convert buffer to numpy array\n                audio_data = b\'\'.join(audio_buffer)\n                audio_np = np.frombuffer(audio_data, dtype=np.int16)\n                \n                # Normalize to [-1, 1]\n                audio_float = audio_np.astype(np.float32) / 32768.0\n                \n                # Process with Whisper\n                result = self.model.transcribe(audio_float)\n                recognized_text = result[\'text\'].strip()\n                \n                if recognized_text:  # Only publish if there\'s text\n                    msg = String()\n                    msg.data = recognized_text\n                    self.text_publisher.publish(msg)\n                    self.get_logger().info(f\'Recognized: "{recognized_text}"\')\n                \n                # Clear buffer\n                audio_buffer = []\n\n    def destroy_node(self):\n        """Clean up resources"""\n        self.audio.terminate()\n        super().destroy_node()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"advanced-audio-processing",children:"Advanced Audio Processing"}),"\n",(0,s.jsx)(n.h3,{id:"audio-preprocessing-for-better-recognition",children:"Audio Preprocessing for Better Recognition"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import librosa\nimport soundfile as sf\nimport io\nimport numpy as np\n\nclass AdvancedWhisperNode(SimpleWhisperNode):\n    def __init__(self):\n        super().__init__()\n        \n        # Add noise reduction parameters\n        self.noise_floor = 0.01  # Minimum signal level to consider as speech\n        self.silence_threshold = 0.02  # Threshold for silence detection\n        \n        self.get_logger().info(\'Advanced Whisper Node initialized\')\n\n    def preprocess_audio(self, raw_audio):\n        """\n        Preprocess raw audio for optimal Whisper performance\n        """\n        # Resample if needed\n        if len(raw_audio) > 0 and isinstance(raw_audio, np.ndarray):\n            # Normalize audio\n            raw_audio = raw_audio / max(np.max(np.abs(raw_audio)), 1e-9)  # Avoid division by zero\n            \n            # Apply pre-emphasis filter to boost high frequencies\n            pre_emphasis = 0.97\n            audio_pre = np.append(raw_audio[0], raw_audio[1:] - pre_emphasis * raw_audio[:-1])\n            \n            # Apply windowing to reduce spectral leakage\n            window = np.hanning(len(audio_pre))\n            audio_windowed = audio_pre * window\n            \n            return audio_windowed\n        else:\n            return raw_audio\n\n    def is_silence(self, audio_chunk, threshold=None):\n        """Check if an audio chunk contains silence"""\n        if threshold is None:\n            threshold = self.silence_threshold\n            \n        # Calculate RMS energy of the audio chunk\n        rms = np.sqrt(np.mean(np.square(audio_chunk)))\n        return rms < threshold\n\n    def record_audio(self):\n        """Record audio with silence detection and preprocessing"""\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n        \n        # Buffer to hold audio chunks\n        audio_buffer = []\n        buffer_duration = 3  # Process every 3 seconds\n        max_chunks = int(buffer_duration * self.rate / self.chunk)\n        \n        while rclpy.ok():\n            # Read audio chunk\n            data = stream.read(self.chunk)\n            audio_np = np.frombuffer(data, dtype=np.int16)\n            audio_float = audio_np.astype(np.float32) / 32768.0\n            \n            # Check if this chunk is silence\n            if not self.is_silence(audio_float):\n                audio_buffer.append(data)\n            \n            # If buffer is full, process the audio\n            if len(audio_buffer) >= max_chunks and len(audio_buffer) > 0:\n                # Convert buffer to numpy array\n                audio_data = b\'\'.join(audio_buffer)\n                audio_np = np.frombuffer(audio_data, dtype=np.int16)\n                \n                # Preprocess the audio\n                audio_processed = self.preprocess_audio(audio_np.astype(np.float32) / 32768.0)\n                \n                # Process with Whisper\n                result = self.model.transcribe(audio_processed)\n                recognized_text = result[\'text\'].strip()\n                \n                if recognized_text:  # Only publish if there\'s text\n                    msg = String()\n                    msg.data = recognized_text\n                    self.text_publisher.publish(msg)\n                    self.get_logger().info(f\'Recognized: "{recognized_text}"\')\n                \n                # Clear buffer\n                audio_buffer = []\n\n    def destroy_node(self):\n        """Clean up resources"""\n        self.audio.terminate()\n        super().destroy_node()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"streaming-audio-processing",children:"Streaming Audio Processing"}),"\n",(0,s.jsx)(n.h3,{id:"continuous-audio-processing-with-vad-voice-activity-detection",children:"Continuous Audio Processing with VAD (Voice Activity Detection)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import collections\nimport webrtcvad\n\nclass StreamingWhisperNode(AdvancedWhisperNode):\n    def __init__(self):\n        super().__init__()\n        \n        # Initialize WebRTC VAD (Voice Activity Detection)\n        self.vad = webrtcvad.Vad()\n        # Set VAD aggressiveness (0-3, 3 is most aggressive)\n        self.vad.set_mode(1)\n        \n        # Audio parameters for VAD (must be 8000, 16000, 32000, or 48000 Hz)\n        self.vad_sample_rate = 16000\n        # Frame size for VAD (10, 20, or 30 ms)\n        self.frame_duration = 20  # ms\n        self.frame_size = int(self.vad_sample_rate * self.frame_duration / 1000) * 2  # 2 bytes per sample\n        \n        # Ring buffer for audio chunks\n        self.ring_buffer = collections.deque(maxlen=int(30 * self.vad_sample_rate / self.frame_size))  # 30 frames = 600ms\n        self.triggered = False\n        self.vad_frames = []\n        self.ring_buffer_count = 0\n        self.speech_start = 0\n        self.speech_end = 0\n        self.silence_duration = 0\n        \n        self.get_logger().info(\'Streaming Whisper Node with VAD initialized\')\n\n    def is_speech(self, frame):\n        """Check if a frame contains speech using WebRTC VAD"""\n        try:\n            return self.vad.is_speech(frame, self.vad_sample_rate)\n        except:\n            return False\n\n    def record_audio(self):\n        """Record audio with VAD and streaming processing"""\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.frame_size//2  # Divide by 2 because our frame size is in bytes but chunk is in samples\n        )\n        \n        while rclpy.ok():\n            # Read audio chunk (10ms worth)\n            data = stream.read(self.frame_size//2, exception_on_overflow=False)\n            \n            # Add to ring buffer\n            self.ring_buffer.append(data)\n            \n            # Check if this frame contains speech\n            is_speech = self.is_speech(data)\n            \n            if not self.triggered:\n                # No speech detected yet\n                self.ring_buffer_count += 1\n                if is_speech:\n                    # Start of speech detected\n                    self.triggered = True\n                    self.speech_start = self.ring_buffer_count - self.ring_buffer.maxlen\n                    self.vad_frames = self.ring_buffer.copy()\n            else:\n                # Speech already detected\n                if is_speech:\n                    # Continue collecting speech frames\n                    self.vad_frames.append(data)\n                    self.silence_duration = 0\n                else:\n                    # Silence detected\n                    self.silence_duration += 1\n                    self.vad_frames.append(data)\n                    \n                    # If silence continues for 20 frames (200ms), consider speech ended\n                    if self.silence_duration > 20:\n                        # Process collected speech\n                        self.process_speech_segment()\n                        \n                        # Reset for next speech segment\n                        self.triggered = False\n                        self.silence_duration = 0\n                        self.vad_frames = []\n\n    def process_speech_segment(self):\n        """Process a complete speech segment"""\n        if len(self.vad_frames) < 10:  # Less than 100ms of speech, probably noise\n            return\n            \n        # Convert collected frames to numpy array\n        audio_data = b\'\'.join(self.vad_frames)\n        audio_np = np.frombuffer(audio_data, dtype=np.int16)\n        \n        # Preprocess the audio\n        audio_processed = self.preprocess_audio(audio_np.astype(np.float32) / 32768.0)\n        \n        # Process with Whisper\n        result = self.model.transcribe(audio_processed)\n        recognized_text = result[\'text\'].strip()\n        \n        if recognized_text:  # Only publish if there\'s text\n            msg = String()\n            msg.data = recognized_text\n            self.text_publisher.publish(msg)\n            self.get_logger().info(f\'Recognized: "{recognized_text}"\')\n\n    def destroy_node(self):\n        """Clean up resources"""\n        self.audio.terminate()\n        super().destroy_node()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"whisper-with-ros-2-actions",children:"Whisper with ROS 2 Actions"}),"\n",(0,s.jsx)(n.h3,{id:"using-ros-2-actions-for-long-running-transcription",children:"Using ROS 2 Actions for Long-Running Transcription"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.action import ActionServer\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\n# Define a custom action interface (you would create this in your package)\n# For this example, we\'ll define a simple structure\nclass TranscriptionAction:\n    class Goal:\n        def __init__(self):\n            self.duration = 0  # Duration in seconds to record\n    \n    class Result:\n        def __init__(self):\n            self.transcript = ""\n    \n    class Feedback:\n        def __init__(self):\n            self.interim_transcript = ""\n\nclass ActionWhisperNode(Node):\n    def __init__(self):\n        super().__init__(\'action_whisper_node\')\n        \n        # Publisher for real-time updates\n        self.interim_publisher = self.create_publisher(String, \'interim_transcript\', 10)\n        \n        # Initialize Whisper model\n        self.model = whisper.load_model("base")\n        \n        # Audio parameters\n        self.rate = 16000\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        \n        # Initialize PyAudio\n        self.audio = pyaudio.PyAudio()\n        \n        # Create action server\n        self._action_server = ActionServer(\n            self,\n            TranscriptionAction,\n            \'transcribe_audio\',\n            self.execute_callback\n        )\n        \n        self.get_logger().info(\'Action Whisper Node initialized\')\n\n    def execute_callback(self, goal_handle):\n        """Execute the transcription action"""\n        self.get_logger().info(f\'Executing transcription for {goal_handle.request.duration} seconds\')\n        \n        # Record audio for the specified duration\n        audio_data = self.record_for_duration(goal_handle.request.duration)\n        \n        # Process with Whisper\n        result = self.model.transcribe(audio_data)\n        transcript = result[\'text\'].strip()\n        \n        # Create result message\n        result_msg = TranscriptionAction.Result()\n        result_msg.transcript = transcript\n        \n        # Publish final result\n        final_msg = String()\n        final_msg.data = transcript\n        self.interim_publisher.publish(final_msg)\n        \n        self.get_logger().info(f\'Transcription completed: "{transcript}"\')\n        \n        goal_handle.succeed()\n        return result_msg\n\n    def record_for_duration(self, duration_sec):\n        """Record audio for a specific duration"""\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n        \n        frames = []\n        total_chunks = int(duration_sec * self.rate / self.chunk)\n        \n        for _ in range(total_chunks):\n            data = stream.read(self.chunk)\n            frames.append(data)\n        \n        # Convert to numpy array\n        audio_data = b\'\'.join(frames)\n        audio_np = np.frombuffer(audio_data, dtype=np.int16)\n        audio_float = audio_np.astype(np.float32) / 32768.0\n        \n        stream.stop_stream()\n        stream.close()\n        \n        return audio_float\n\n    def destroy_node(self):\n        """Clean up resources"""\n        self.audio.terminate()\n        super().destroy_node()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"error-handling-and-robustness",children:"Error Handling and Robustness"}),"\n",(0,s.jsx)(n.h3,{id:"whisper-node-with-comprehensive-error-handling",children:"Whisper Node with Comprehensive Error Handling"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import time\nfrom collections import deque\n\nclass RobustWhisperNode(Node):\n    def __init__(self):\n        super().__init__('robust_whisper_node')\n        \n        # Publisher for recognized text\n        self.text_publisher = self.create_publisher(String, 'recognized_speech', 10)\n        \n        # Parameters\n        self.declare_parameter('whisper_model_size', 'base')\n        self.declare_parameter('audio_rate', 16000)\n        self.declare_parameter('audio_chunk', 1024)\n        \n        model_size = self.get_parameter('whisper_model_size').get_parameter_value().string_value\n        self.rate = self.get_parameter('audio_rate').get_parameter_value().integer_value\n        self.chunk = self.get_parameter('audio_chunk').get_parameter_value().integer_value\n        \n        # Initialize Whisper model with error handling\n        try:\n            self.model = whisper.load_model(model_size)\n        except Exception as e:\n            self.get_logger().error(f'Failed to load Whisper model: {e}')\n            self.model = None\n            return\n        \n        # Audio parameters\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        \n        # Initialize PyAudio\n        try:\n            self.audio = pyaudio.PyAudio()\n        except Exception as e:\n            self.get_logger().error(f'Failed to initialize PyAudio: {e}')\n            return\n        \n        # Audio buffer with timeout\n        self.audio_queue = queue.Queue()\n        self.is_recording = True\n        \n        # Statistics for monitoring\n        self.processed_chunks = 0\n        self.failed_transcriptions = 0\n        self.start_time = time.time()\n        \n        # Start recording thread\n        self.recording_thread = threading.Thread(target=self.record_audio)\n        self.recording_thread.daemon = True\n        self.recording_thread.start()\n        \n        # Start processing thread\n        self.processing_thread = threading.Thread(target=self.process_audio)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n        \n        # Timer for periodic statistics\n        self.stats_timer = self.create_timer(30.0, self.log_statistics)\n        \n        self.get_logger().info('Robust Whisper Node initialized')\n\n    def record_audio(self):\n        \"\"\"Record audio with error handling\"\"\"\n        try:\n            stream = self.audio.open(\n                format=self.format,\n                channels=self.channels,\n                rate=self.rate,\n                input=True,\n                frames_per_buffer=self.chunk\n            )\n        except Exception as e:\n            self.get_logger().error(f'Failed to open audio stream: {e}')\n            return\n        \n        while self.is_recording:\n            try:\n                data = stream.read(self.chunk, exception_on_overflow=False)\n                self.audio_queue.put(data)\n            except Exception as e:\n                self.get_logger().warn(f'Error reading audio: {e}')\n                # Continue despite error\n        \n        stream.stop_stream()\n        stream.close()\n\n    def process_audio(self):\n        \"\"\"Process audio from queue with error handling\"\"\"\n        audio_buffer = []\n        buffer_duration = 3  # Process every 3 seconds\n        max_chunks = int(buffer_duration * self.rate / self.chunk)\n        \n        while self.is_recording:\n            try:\n                # Get audio chunk from queue with timeout\n                data = self.audio_queue.get(timeout=1.0)\n                audio_buffer.append(data)\n                \n                # If buffer is full, process the audio\n                if len(audio_buffer) >= max_chunks:\n                    self.process_audio_buffer(audio_buffer)\n                    audio_buffer = []  # Clear buffer\n                    \n            except queue.Empty:\n                continue  # Continue if no audio in queue\n            except Exception as e:\n                self.get_logger().error(f'Error processing audio: {e}')\n                self.failed_transcriptions += 1\n                audio_buffer = []  # Clear buffer on error\n\n    def process_audio_buffer(self, audio_buffer):\n        \"\"\"Process a buffer of audio chunks\"\"\"\n        try:\n            # Convert buffer to numpy array\n            audio_data = b''.join(audio_buffer)\n            audio_np = np.frombuffer(audio_data, dtype=np.int16)\n            \n            # Normalize to [-1, 1]\n            audio_float = audio_np.astype(np.float32) / 32768.0\n            \n            # Process with Whisper\n            result = self.model.transcribe(audio_float)\n            recognized_text = result['text'].strip()\n            \n            if recognized_text:  # Only publish if there's text\n                msg = String()\n                msg.data = recognized_text\n                self.text_publisher.publish(msg)\n                self.get_logger().info(f'Recognized: \"{recognized_text}\"')\n            \n            self.processed_chunks += 1\n            \n        except Exception as e:\n            self.get_logger().error(f'Error in Whisper transcription: {e}')\n            self.failed_transcriptions += 1\n\n    def log_statistics(self):\n        \"\"\"Log processing statistics\"\"\"\n        elapsed_time = time.time() - self.start_time\n        success_rate = 0\n        if self.processed_chunks > 0:\n            success_rate = 100 * (self.processed_chunks - self.failed_transcriptions) / self.processed_chunks\n            \n        self.get_logger().info(\n            f'Audio processing stats: {self.processed_chunks} processed, '\n            f'{self.failed_transcriptions} failed, '\n            f'{success_rate:.1f}% success rate over {elapsed_time/60:.1f} minutes'\n        )\n\n    def destroy_node(self):\n        \"\"\"Clean up resources\"\"\"\n        self.is_recording = False\n        if hasattr(self, 'audio'):\n            self.audio.terminate()\n        super().destroy_node()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"These code examples demonstrate various approaches to integrating OpenAI Whisper in robotics applications:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Basic Integration"}),": Simple audio capture and transcription"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advanced Processing"}),": Audio preprocessing for better recognition"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Streaming"}),": Continuous processing with voice activity detection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 Actions"}),": Using actions for long-running transcription tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robust Implementation"}),": Error handling and monitoring"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Each approach has its trade-offs in terms of complexity, resource usage, and real-time performance. Choose the approach that best fits your specific robotics application requirements."})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453(e,n,i){i.d(n,{R:()=>t,x:()=>a});var r=i(6540);const s={},o=r.createContext(s);function t(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);