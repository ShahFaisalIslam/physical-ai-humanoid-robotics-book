"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[4845],{1184(e,n,i){i.r(n),i.d(n,{assets:()=>t,contentTitle:()=>l,default:()=>c,frontMatter:()=>r,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module-2-digital-twin/sensor-simulation","title":"Sensor Simulation","description":"Sensor simulation is a critical component of robotic simulation, enabling robots to perceive their environment in a realistic way. Accurate sensor simulation allows for testing perception algorithms, sensor fusion, and robot behaviors before deployment to real hardware.","source":"@site/docs/module-2-digital-twin/sensor-simulation.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/sensor-simulation","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2-digital-twin/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/your-organization/physical-ai-humanoid-robotics-book/edit/main/book/docs/module-2-digital-twin/sensor-simulation.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Physics Modeling in Simulation","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2-digital-twin/physics-modeling"},"next":{"title":"Unity Integration for High-Fidelity Visualization","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2-digital-twin/unity-integration"}}');var s=i(4848),o=i(8453);const r={sidebar_position:3},l="Sensor Simulation",t={},d=[{value:"Overview of Sensor Simulation",id:"overview-of-sensor-simulation",level:2},{value:"Camera Simulation",id:"camera-simulation",level:2},{value:"RGB Cameras",id:"rgb-cameras",level:3},{value:"Depth Cameras",id:"depth-cameras",level:3},{value:"Stereo Cameras",id:"stereo-cameras",level:3},{value:"LiDAR Simulation",id:"lidar-simulation",level:2},{value:"2D LiDAR (Laser Range Finder)",id:"2d-lidar-laser-range-finder",level:3},{value:"3D LiDAR (Lidar)",id:"3d-lidar-lidar",level:3},{value:"IMU Simulation",id:"imu-simulation",level:2},{value:"GPS Simulation",id:"gps-simulation",level:2},{value:"Force/Torque Sensors",id:"forcetorque-sensors",level:2},{value:"Sensor Noise and Realism",id:"sensor-noise-and-realism",level:2},{value:"Noise Models",id:"noise-models",level:3},{value:"Common Noise Sources",id:"common-noise-sources",level:3},{value:"Sensor Fusion Simulation",id:"sensor-fusion-simulation",level:2},{value:"Multi-Sensor Integration",id:"multi-sensor-integration",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Sensor Update Rates",id:"sensor-update-rates",level:3},{value:"Rendering Overhead",id:"rendering-overhead",level:3},{value:"Data Processing",id:"data-processing",level:3},{value:"Validation of Sensor Simulation",id:"validation-of-sensor-simulation",level:2},{value:"Sensor Data Comparison",id:"sensor-data-comparison",level:3},{value:"Perception Algorithm Testing",id:"perception-algorithm-testing",level:3},{value:"Special Considerations for Humanoid Robots",id:"special-considerations-for-humanoid-robots",level:2},{value:"Sensor Placement",id:"sensor-placement",level:3},{value:"Multi-Sensory Integration",id:"multi-sensory-integration",level:3}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"sensor-simulation",children:"Sensor Simulation"})}),"\n",(0,s.jsx)(n.p,{children:"Sensor simulation is a critical component of robotic simulation, enabling robots to perceive their environment in a realistic way. Accurate sensor simulation allows for testing perception algorithms, sensor fusion, and robot behaviors before deployment to real hardware."}),"\n",(0,s.jsx)(n.h2,{id:"overview-of-sensor-simulation",children:"Overview of Sensor Simulation"}),"\n",(0,s.jsx)(n.p,{children:"In Gazebo, sensors are simulated by:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Rendering the environment from the sensor's perspective"}),"\n",(0,s.jsx)(n.li,{children:"Applying realistic noise and distortion models"}),"\n",(0,s.jsx)(n.li,{children:"Converting the rendered data to the appropriate sensor format"}),"\n",(0,s.jsx)(n.li,{children:"Publishing the data to ROS topics"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"camera-simulation",children:"Camera Simulation"}),"\n",(0,s.jsx)(n.h3,{id:"rgb-cameras",children:"RGB Cameras"}),"\n",(0,s.jsx)(n.p,{children:"RGB cameras simulate standard visual sensors:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<sensor name="camera" type="camera">\n  <camera name="head">\n    <horizontal_fov>1.3962634</horizontal_fov> \x3c!-- 80 degrees --\x3e\n    <image>\n      <width>800</width>\n      <height>600</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>100</far>\n    </clip>\n  </camera>\n  <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n    <frame_name>camera_frame</frame_name>\n    <topic_name>image_raw</topic_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"depth-cameras",children:"Depth Cameras"}),"\n",(0,s.jsx)(n.p,{children:"Depth cameras provide both RGB and depth information:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<sensor name="depth_camera" type="depth">\n  <camera name="depth_cam">\n    <horizontal_fov>1.047</horizontal_fov> \x3c!-- 60 degrees --\x3e\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10</far>\n    </clip>\n  </camera>\n  <plugin name="depth_camera_controller" filename="libgazebo_ros_openni_kinect.so">\n    <baseline>0.2</baseline>\n    <distortion_k1>0.0</distortion_k1>\n    <distortion_k2>0.0</distortion_k2>\n    <distortion_k3>0.0</distortion_k3>\n    <distortion_t1>0.0</distortion_t1>\n    <distortion_t2>0.0</distortion_t2>\n    <point_cloud_cutoff>0.1</point_cloud_cutoff>\n    <frame_name>depth_camera_frame</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"stereo-cameras",children:"Stereo Cameras"}),"\n",(0,s.jsx)(n.p,{children:"Stereo cameras provide depth perception through two RGB cameras:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<sensor name="stereo_camera" type="multicamera">\n  <camera name="left_cam">\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>800</width>\n      <height>600</height>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10</far>\n    </clip>\n  </camera>\n  <camera name="right_cam">\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>800</width>\n      <height>600</height>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10</far>\n    </clip>\n    <pose>0.2 0 0 0 0 0</pose> \x3c!-- Baseline offset --\x3e\n  </camera>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.h2,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,s.jsx)(n.h3,{id:"2d-lidar-laser-range-finder",children:"2D LiDAR (Laser Range Finder)"}),"\n",(0,s.jsx)(n.p,{children:"2D LiDAR sensors provide a 2D scan of the environment:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<sensor name="laser" type="ray">\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>720</samples>\n        <resolution>1</resolution>\n        <min_angle>-1.570796</min_angle> \x3c!-- -90 degrees --\x3e\n        <max_angle>1.570796</max_angle>   \x3c!-- 90 degrees --\x3e\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>30.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <plugin name="laser_controller" filename="libgazebo_ros_laser.so">\n    <topic_name>scan</topic_name>\n    <frame_name>laser_frame</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3d-lidar-lidar",children:"3D LiDAR (Lidar)"}),"\n",(0,s.jsx)(n.p,{children:"3D LiDAR sensors provide a 3D point cloud:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<sensor name="velodyne" type="ray">\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>800</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle> \x3c!-- -180 degrees --\x3e\n        <max_angle>3.14159</max_angle>   \x3c!-- 180 degrees --\x3e\n      </horizontal>\n      <vertical>\n        <samples>32</samples>\n        <resolution>1</resolution>\n        <min_angle>-0.5236</min_angle>  \x3c!-- -30 degrees --\x3e\n        <max_angle>0.2618</max_angle>    \x3c!-- 15 degrees --\x3e\n      </vertical>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>100.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <plugin name="velodyne_controller" filename="libgazebo_ros_velodyne_laser.so">\n    <topic_name>velodyne_points</topic_name>\n    <frame_name>velodyne_frame</frame_name>\n    <min_range>0.1</min_range>\n    <max_range>100.0</max_range>\n    <gaussian_noise>0.01</gaussian_noise>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.h2,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,s.jsx)(n.p,{children:"IMU (Inertial Measurement Unit) sensors provide acceleration and angular velocity:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<sensor name="imu_sensor" type="imu">\n  <always_on>1</always_on>\n  <update_rate>100</update_rate>\n  <imu>\n    <angular_velocity>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.0017</stddev> \x3c!-- rad/s --\x3e\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.0017</stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.0017</stddev>\n        </noise>\n      </z>\n    </angular_velocity>\n    <linear_acceleration>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.017</stddev> \x3c!-- m/s\xb2 --\x3e\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.017</stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.017</stddev>\n        </noise>\n      </z>\n    </linear_acceleration>\n  </imu>\n  <plugin name="imu_plugin" filename="libgazebo_ros_imu.so">\n    <topic_name>imu/data</topic_name>\n    <body_name>imu_link</body_name>\n    <frame_name>imu_frame</frame_name>\n    <update_rate>100.0</update_rate>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.h2,{id:"gps-simulation",children:"GPS Simulation"}),"\n",(0,s.jsx)(n.p,{children:"GPS sensors provide global position information:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<sensor name="gps" type="gps">\n  <always_on>1</always_on>\n  <update_rate>10</update_rate>\n  <plugin name="gps_plugin" filename="libgazebo_ros_gps.so">\n    <topic_name>fix</topic_name>\n    <frame_name>gps_frame</frame_name>\n    <update_rate>10.0</update_rate>\n    <gaussian_noise>0.1</gaussian_noise> \x3c!-- Position noise in meters --\x3e\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.h2,{id:"forcetorque-sensors",children:"Force/Torque Sensors"}),"\n",(0,s.jsx)(n.p,{children:"Force/torque sensors measure forces and torques applied to joints:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<sensor name="ft_sensor" type="force_torque">\n  <always_on>true</always_on>\n  <update_rate>100</update_rate>\n  <plugin name="ft_sensor_plugin" filename="libgazebo_ros_ft_sensor.so">\n    <topic_name>wrench</topic_name>\n    <frame_name>ft_sensor_frame</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.h2,{id:"sensor-noise-and-realism",children:"Sensor Noise and Realism"}),"\n",(0,s.jsx)(n.h3,{id:"noise-models",children:"Noise Models"}),"\n",(0,s.jsx)(n.p,{children:"Real sensors have inherent noise that should be simulated:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<noise type="gaussian">\n  <mean>0.0</mean>           \x3c!-- Mean of the noise distribution --\x3e\n  <stddev>0.01</stddev>      \x3c!-- Standard deviation --\x3e\n  <bias_mean>0.001</bias_mean> \x3c!-- Bias mean --\x3e\n  <bias_stddev>0.001</bias_stddev> \x3c!-- Bias standard deviation --\x3e\n</noise>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"common-noise-sources",children:"Common Noise Sources"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Thermal noise"}),": Electronic noise in sensor circuits"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Quantization noise"}),": Discretization of continuous signals"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Environmental noise"}),": External factors affecting measurements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mechanical noise"}),": Vibrations and mechanical imperfections"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"sensor-fusion-simulation",children:"Sensor Fusion Simulation"}),"\n",(0,s.jsx)(n.h3,{id:"multi-sensor-integration",children:"Multi-Sensor Integration"}),"\n",(0,s.jsx)(n.p,{children:"Combine data from multiple sensors to improve perception:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Example: Fusing IMU and GPS for better localization --\x3e\n# In ROS launch file:\n<node pkg="robot_localization" type="ekf_localization_node" name="ekf_se_odom">\n  <param name="frequency" value="50"/>\n  <param name="sensor_timeout" value="0.1"/>\n  <param name="two_d_mode" value="true"/>\n  \n  <param name="odom0" value="/wheel/odometry"/>\n  <param name="imu0" value="/imu/data"/>\n  \n  <rosparam param="odom0_config">[true, true, false, false, false, true, false, false, true, false, false, true, false, false, true]</rosparam>\n  <rosparam param="imu0_config">[false, false, false, true, true, true, true, true, true, false, false, false, true, true, true]</rosparam>\n</node>\n'})}),"\n",(0,s.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"sensor-update-rates",children:"Sensor Update Rates"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Higher update rates provide more data but consume more CPU"}),"\n",(0,s.jsx)(n.li,{children:"Match update rates to real hardware capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Consider sensor fusion requirements when setting rates"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"rendering-overhead",children:"Rendering Overhead"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Camera sensors require rendering, which can be computationally expensive"}),"\n",(0,s.jsx)(n.li,{children:"Reduce resolution or field of view if performance is an issue"}),"\n",(0,s.jsx)(n.li,{children:"Use lower quality rendering for distant sensors"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"data-processing",children:"Data Processing"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Simulated sensor data still requires processing in ROS nodes"}),"\n",(0,s.jsx)(n.li,{children:"Consider the computational load of perception algorithms"}),"\n",(0,s.jsx)(n.li,{children:"Optimize algorithms for real-time performance"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"validation-of-sensor-simulation",children:"Validation of Sensor Simulation"}),"\n",(0,s.jsx)(n.h3,{id:"sensor-data-comparison",children:"Sensor Data Comparison"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Compare simulated sensor data with real sensor data"}),"\n",(0,s.jsx)(n.li,{children:"Validate noise characteristics match real sensors"}),"\n",(0,s.jsx)(n.li,{children:"Check that sensor ranges and accuracies are appropriate"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"perception-algorithm-testing",children:"Perception Algorithm Testing"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Test perception algorithms in both simulation and reality"}),"\n",(0,s.jsx)(n.li,{children:"Validate that algorithms perform similarly in both environments"}),"\n",(0,s.jsx)(n.li,{children:"Identify and address sim-to-real gaps"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"special-considerations-for-humanoid-robots",children:"Special Considerations for Humanoid Robots"}),"\n",(0,s.jsx)(n.h3,{id:"sensor-placement",children:"Sensor Placement"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Position sensors to match human-like perception capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Consider field of view for cameras and LiDAR"}),"\n",(0,s.jsx)(n.li,{children:"Place IMUs appropriately for balance control"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"multi-sensory-integration",children:"Multi-Sensory Integration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Combine visual, proprioceptive, and vestibular-like sensors"}),"\n",(0,s.jsx)(n.li,{children:"Implement sensor fusion for robust perception"}),"\n",(0,s.jsx)(n.li,{children:"Consider redundancy for safety-critical functions"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Sensor simulation is a powerful tool for developing and testing robotic systems. By accurately modeling sensors and their limitations, you can create simulations that effectively prepare robots for real-world deployment."})]})}function c(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>l});var a=i(6540);const s={},o=a.createContext(s);function r(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);