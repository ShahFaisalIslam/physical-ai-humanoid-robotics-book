"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[3196],{4632(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"capstone-project/exercises-system-integration","title":"Capstone Exercises for System Integration","description":"Exercises to understand system integration in the capstone project","source":"@site/docs/capstone-project/exercises-system-integration.md","sourceDirName":"capstone-project","slug":"/capstone-project/exercises-system-integration","permalink":"/physical-ai-humanoid-robotics-book/docs/capstone-project/exercises-system-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/your-organization/physical-ai-humanoid-robotics-book/edit/main/book/docs/capstone-project/exercises-system-integration.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Capstone Exercises for System Integration","description":"Exercises to understand system integration in the capstone project","sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Capstone Troubleshooting Guide","permalink":"/physical-ai-humanoid-robotics-book/docs/capstone-project/troubleshooting-guide"},"next":{"title":"Capstone Assessment Questions","permalink":"/physical-ai-humanoid-robotics-book/docs/capstone-project/assessment-capstone"}}');var o=t(4848),i=t(8453);const a={title:"Capstone Exercises for System Integration",description:"Exercises to understand system integration in the capstone project",sidebar_position:6},r="Capstone Exercises for System Integration",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Exercise 1: Basic Component Communication",id:"exercise-1-basic-component-communication",level:2},{value:"Objective",id:"objective",level:3},{value:"Task",id:"task",level:3},{value:"Implementation Steps",id:"implementation-steps",level:3},{value:"Sample Code Structure",id:"sample-code-structure",level:3},{value:"Validation",id:"validation",level:3},{value:"Exercise 2: Multi-Component Coordination",id:"exercise-2-multi-component-coordination",level:2},{value:"Objective",id:"objective-1",level:3},{value:"Task",id:"task-1",level:3},{value:"Implementation Steps",id:"implementation-steps-1",level:3},{value:"Sample Code Structure",id:"sample-code-structure-1",level:3},{value:"Validation",id:"validation-1",level:3},{value:"Exercise 3: Perception-Navigation Integration",id:"exercise-3-perception-navigation-integration",level:2},{value:"Objective",id:"objective-2",level:3},{value:"Task",id:"task-2",level:3},{value:"Implementation Steps",id:"implementation-steps-2",level:3},{value:"Sample Code Structure",id:"sample-code-structure-2",level:3},{value:"Validation",id:"validation-2",level:3},{value:"Exercise 4: Voice-Perception-Manipulation Pipeline",id:"exercise-4-voice-perception-manipulation-pipeline",level:2},{value:"Objective",id:"objective-3",level:3},{value:"Task",id:"task-3",level:3},{value:"Implementation Steps",id:"implementation-steps-3",level:3},{value:"Sample Code Structure",id:"sample-code-structure-3",level:3},{value:"Validation",id:"validation-3",level:3},{value:"Exercise 5: System State Management",id:"exercise-5-system-state-management",level:2},{value:"Objective",id:"objective-4",level:3},{value:"Task",id:"task-4",level:3},{value:"Implementation Steps",id:"implementation-steps-4",level:3},{value:"Sample Code Structure",id:"sample-code-structure-4",level:3},{value:"Validation",id:"validation-4",level:3},{value:"Exercise 6: Complete Integration Challenge",id:"exercise-6-complete-integration-challenge",level:2},{value:"Objective",id:"objective-5",level:3},{value:"Task",id:"task-5",level:3},{value:"Implementation Steps",id:"implementation-steps-5",level:3},{value:"Sample Code Structure",id:"sample-code-structure-5",level:3},{value:"Validation",id:"validation-5",level:3},{value:"Exercise 7: Performance and Optimization",id:"exercise-7-performance-and-optimization",level:2},{value:"Objective",id:"objective-6",level:3},{value:"Task",id:"task-6",level:3},{value:"Implementation Steps",id:"implementation-steps-6",level:3},{value:"Sample Code Structure",id:"sample-code-structure-6",level:3},{value:"Validation",id:"validation-6",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"capstone-exercises-for-system-integration",children:"Capstone Exercises for System Integration"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"This section provides practical exercises to help you understand and implement system integration in the Autonomous Humanoid Capstone Project. These exercises build on all previous modules and focus on connecting voice processing, navigation, perception, and manipulation components into a cohesive system."}),"\n",(0,o.jsx)(n.h2,{id:"exercise-1-basic-component-communication",children:"Exercise 1: Basic Component Communication"}),"\n",(0,o.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Implement basic communication between two components using ROS 2 topics."}),"\n",(0,o.jsx)(n.h3,{id:"task",children:"Task"}),"\n",(0,o.jsx)(n.p,{children:"Create a simple publisher-subscriber pair where one node publishes voice commands and another node receives and processes them."}),"\n",(0,o.jsx)(n.h3,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:'Create a publisher node that publishes simple commands like "move forward", "turn left", "stop"'}),"\n",(0,o.jsx)(n.li,{children:"Create a subscriber node that receives these commands and logs them"}),"\n",(0,o.jsx)(n.li,{children:"Test the communication between nodes"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"sample-code-structure",children:"Sample Code Structure"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Publisher node\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport time\n\nclass VoiceCommandPublisher(Node):\n    def __init__(self):\n        super().__init__('voice_command_publisher')\n        self.publisher = self.create_publisher(String, 'voice_commands', 10)\n        self.timer = self.create_timer(2.0, self.publish_command)\n        self.commands = [\"move forward\", \"turn left\", \"stop\"]\n        self.index = 0\n\n    def publish_command(self):\n        msg = String()\n        msg.data = self.commands[self.index % len(self.commands)]\n        self.publisher.publish(msg)\n        self.get_logger().info(f'Publishing: {msg.data}')\n        self.index += 1\n\n# Subscriber node\nclass CommandProcessor(Node):\n    def __init__(self):\n        super().__init__('command_processor')\n        self.subscription = self.create_subscription(\n            String,\n            'voice_commands',\n            self.listener_callback,\n            10\n        )\n        self.subscription  # prevent unused variable warning\n\n    def listener_callback(self, msg):\n        self.get_logger().info(f'Received command: {msg.data}')\n        # Process the command here\n\ndef main(args=None):\n    rclpy.init(args=args)\n    \n    publisher = VoiceCommandPublisher()\n    processor = CommandProcessor()\n    \n    executor = rclpy.executors.MultiThreadedExecutor()\n    executor.add_node(publisher)\n    executor.add_node(processor)\n    \n    try:\n        executor.spin()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        executor.shutdown()\n        rclpy.shutdown()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"validation",children:"Validation"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Verify that commands are published and received correctly"}),"\n",(0,o.jsx)(n.li,{children:"Check that both nodes can run simultaneously"}),"\n",(0,o.jsx)(n.li,{children:"Test with different message frequencies"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"exercise-2-multi-component-coordination",children:"Exercise 2: Multi-Component Coordination"}),"\n",(0,o.jsx)(n.h3,{id:"objective-1",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Coordinate three components: voice processing, navigation, and system monitoring."}),"\n",(0,o.jsx)(n.h3,{id:"task-1",children:"Task"}),"\n",(0,o.jsx)(n.p,{children:"Create a system where voice commands trigger navigation actions and system status is monitored."}),"\n",(0,o.jsx)(n.h3,{id:"implementation-steps-1",children:"Implementation Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Implement a voice command interpreter that converts natural language to navigation goals"}),"\n",(0,o.jsx)(n.li,{children:"Create a navigation controller that receives goals and executes them"}),"\n",(0,o.jsx)(n.li,{children:"Implement a system monitor that tracks component status"}),"\n",(0,o.jsx)(n.li,{children:"Connect all components using appropriate ROS 2 interfaces"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"sample-code-structure-1",children:"Sample Code Structure"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import Odometry\nimport json\n\nclass VoiceNavigationCoordinator(Node):\n    def __init__(self):\n        super().__init__('voice_navigation_coordinator')\n        \n        # Subscriptions\n        self.voice_sub = self.create_subscription(\n            String, 'voice_commands', self.voice_callback, 10)\n        \n        self.odom_sub = self.create_subscription(\n            Odometry, 'odom', self.odom_callback, 10)\n        \n        # Publishers\n        self.goal_pub = self.create_publisher(PoseStamped, 'goal_pose', 10)\n        self.status_pub = self.create_publisher(String, 'system_status', 10)\n        \n        # Internal state\n        self.robot_position = None\n        self.current_goal = None\n        \n        self.get_logger().info('Voice Navigation Coordinator initialized')\n\n    def voice_callback(self, msg):\n        \"\"\"Process voice command and generate navigation goal\"\"\"\n        command = msg.data.lower()\n        \n        # Simple command parsing\n        if 'kitchen' in command:\n            self.navigate_to('kitchen')\n        elif 'living room' in command:\n            self.navigate_to('living_room')\n        elif 'stop' in command:\n            self.stop_navigation()\n        else:\n            self.get_logger().info(f'Unknown command: {command}')\n\n    def navigate_to(self, location):\n        \"\"\"Navigate to predefined location\"\"\"\n        # Define location coordinates\n        locations = {\n            'kitchen': (5.0, 3.0, 0.0),\n            'living_room': (2.0, 8.0, 0.0),\n            'office': (8.0, 2.0, 0.0)\n        }\n        \n        if location in locations:\n            x, y, theta = locations[location]\n            goal_msg = PoseStamped()\n            goal_msg.header.frame_id = 'map'\n            goal_msg.header.stamp = self.get_clock().now().to_msg()\n            goal_msg.pose.position.x = x\n            goal_msg.pose.position.y = y\n            goal_msg.pose.orientation.z = theta\n            \n            self.goal_pub.publish(goal_msg)\n            self.current_goal = (x, y)\n            \n            self.get_logger().info(f'Navigating to {location} at ({x}, {y})')\n        else:\n            self.get_logger().warn(f'Unknown location: {location}')\n\n    def stop_navigation(self):\n        \"\"\"Stop current navigation\"\"\"\n        # Implementation would send stop command\n        self.get_logger().info('Navigation stopped')\n\n    def odom_callback(self, msg):\n        \"\"\"Update robot position from odometry\"\"\"\n        self.robot_position = (\n            msg.pose.pose.position.x,\n            msg.pose.pose.position.y\n        )\n        \n        # Check if reached goal\n        if self.current_goal:\n            dx = self.current_goal[0] - self.robot_position[0]\n            dy = self.current_goal[1] - self.robot_position[1]\n            distance = (dx**2 + dy**2)**0.5\n            \n            if distance < 0.5:  # Within 50cm of goal\n                self.get_logger().info('Reached goal')\n                self.current_goal = None\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceNavigationCoordinator()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"validation-1",children:"Validation"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Test voice commands that trigger navigation"}),"\n",(0,o.jsx)(n.li,{children:"Verify that robot position updates correctly"}),"\n",(0,o.jsx)(n.li,{children:"Check that system status is published appropriately"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"exercise-3-perception-navigation-integration",children:"Exercise 3: Perception-Navigation Integration"}),"\n",(0,o.jsx)(n.h3,{id:"objective-2",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Integrate perception data with navigation planning to enable obstacle-aware navigation."}),"\n",(0,o.jsx)(n.h3,{id:"task-2",children:"Task"}),"\n",(0,o.jsx)(n.p,{children:"Create a system that uses simulated perception data to update navigation plans in real-time."}),"\n",(0,o.jsx)(n.h3,{id:"implementation-steps-2",children:"Implementation Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Create a perception simulator that publishes obstacle locations"}),"\n",(0,o.jsx)(n.li,{children:"Implement an obstacle avoidance system that modifies navigation plans"}),"\n",(0,o.jsx)(n.li,{children:"Integrate with the navigation system from Exercise 2"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"sample-code-structure-2",children:"Sample Code Structure"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import LaserScan\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom nav_msgs.msg import OccupancyGrid\nimport numpy as np\n\nclass PerceptionNavigationIntegrator(Node):\n    def __init__(self):\n        super().__init__(\'perception_navigation_integrator\')\n        \n        # Subscriptions\n        self.laser_sub = self.create_subscription(\n            LaserScan, \'scan\', self.laser_callback, 10)\n        \n        self.goal_sub = self.create_subscription(\n            PoseStamped, \'goal_pose\', self.goal_callback, 10)\n        \n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.local_map_pub = self.create_publisher(OccupancyGrid, \'local_costmap\', 10)\n        \n        # Internal state\n        self.current_goal = None\n        self.obstacles = []\n        self.is_navigating = False\n        \n        self.get_logger().info(\'Perception Navigation Integrator initialized\')\n\n    def laser_callback(self, msg):\n        """Process laser scan data to detect obstacles"""\n        # Convert laser ranges to obstacle positions\n        angles = np.linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        valid_ranges = np.array(msg.ranges)\n        \n        # Filter out invalid ranges\n        valid_mask = (valid_ranges > msg.range_min) & (valid_ranges < msg.range_max)\n        valid_angles = angles[valid_mask]\n        valid_ranges = valid_ranges[valid_mask]\n        \n        # Calculate obstacle positions in robot frame\n        obstacle_x = valid_ranges * np.cos(valid_angles)\n        obstacle_y = valid_ranges * np.sin(valid_angles)\n        \n        # Store obstacle positions\n        self.obstacles = list(zip(obstacle_x, obstacle_y))\n        \n        # Update local costmap\n        self.update_local_costmap()\n\n    def goal_callback(self, msg):\n        """Receive navigation goal and start navigation"""\n        self.current_goal = (msg.pose.position.x, msg.pose.position.y)\n        self.is_navigating = True\n        self.get_logger().info(f\'New goal received: {self.current_goal}\')\n\n    def update_local_costmap(self):\n        """Update local costmap with detected obstacles"""\n        # Create a simple occupancy grid for local map\n        resolution = 0.1  # 10cm resolution\n        width = 20  # 2m wide\n        height = 20  # 2m high\n        \n        # Initialize grid (all free space)\n        grid_data = [0] * (width * height)  # 0 = free, 100 = occupied\n        \n        # Mark obstacle positions\n        for obs_x, obs_y in self.obstacles:\n            if -1.0 <= obs_x <= 1.0 and -1.0 <= obs_y <= 1.0:\n                grid_x = int((obs_x + 1.0) / resolution)\n                grid_y = int((obs_y + 1.0) / resolution)\n                \n                if 0 <= grid_x < width and 0 <= grid_y < height:\n                    grid_idx = grid_y * width + grid_x\n                    if 0 <= grid_idx < len(grid_data):\n                        grid_data[grid_idx] = 100  # Mark as occupied\n\n        # Create and publish occupancy grid\n        map_msg = OccupancyGrid()\n        map_msg.header.stamp = self.get_clock().now().to_msg()\n        map_msg.header.frame_id = \'base_link\'\n        map_msg.info.resolution = resolution\n        map_msg.info.width = width\n        map_msg.info.height = height\n        map_msg.info.origin.position.x = -1.0\n        map_msg.info.origin.position.y = -1.0\n        map_msg.data = grid_data\n        \n        self.local_map_pub.publish(map_msg)\n\n    def navigate_with_obstacle_avoidance(self):\n        """Navigate while avoiding obstacles"""\n        if not self.current_goal or not self.is_navigating:\n            return\n            \n        # Simple proportional controller with obstacle avoidance\n        cmd = Twist()\n        \n        # Calculate direction to goal\n        goal_x, goal_y = self.current_goal\n        to_goal_angle = np.arctan2(goal_y, goal_x)\n        \n        # Check for obstacles in front\n        front_obstacles = [(x, y) for x, y in self.obstacles if x > 0 and abs(y) < 0.5]\n        \n        if front_obstacles:\n            # If obstacles in front, turn away\n            cmd.angular.z = 0.5  # Turn right\n            cmd.linear.x = 0.0   # Stop forward motion\n        else:\n            # Head toward goal\n            cmd.angular.z = 0.5 * to_goal_angle\n            cmd.linear.x = 0.2   # Move forward slowly\n        \n        self.cmd_vel_pub.publish(cmd)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PerceptionNavigationIntegrator()\n    \n    # Timer to run navigation control\n    node.create_timer(0.1, node.navigate_with_obstacle_avoidance)\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"validation-2",children:"Validation"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Verify that obstacles are detected from laser scan data"}),"\n",(0,o.jsx)(n.li,{children:"Check that navigation adapts to avoid obstacles"}),"\n",(0,o.jsx)(n.li,{children:"Test with different obstacle configurations"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"exercise-4-voice-perception-manipulation-pipeline",children:"Exercise 4: Voice-Perception-Manipulation Pipeline"}),"\n",(0,o.jsx)(n.h3,{id:"objective-3",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Create a complete pipeline from voice command to object manipulation using perception data."}),"\n",(0,o.jsx)(n.h3,{id:"task-3",children:"Task"}),"\n",(0,o.jsx)(n.p,{children:'Implement a system that takes a voice command like "pick up the red cup" and coordinates perception and manipulation to execute the task.'}),"\n",(0,o.jsx)(n.h3,{id:"implementation-steps-3",children:"Implementation Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Create a voice command parser that identifies manipulation tasks"}),"\n",(0,o.jsx)(n.li,{children:"Implement object detection to find the requested object"}),"\n",(0,o.jsx)(n.li,{children:"Create a manipulation controller to execute the grasp"}),"\n",(0,o.jsx)(n.li,{children:"Coordinate all components with appropriate feedback"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"sample-code-structure-3",children:"Sample Code Structure"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image, JointState\nfrom geometry_msgs.msg import Point\nfrom cv_bridge import CvBridge\nimport json\nimport numpy as np\n\nclass VoicePerceptionManipulationPipeline(Node):\n    def __init__(self):\n        super().__init__('voice_perception_manipulation_pipeline')\n        \n        # Initialize OpenCV bridge\n        self.bridge = CvBridge()\n        \n        # Subscriptions\n        self.voice_sub = self.create_subscription(\n            String, 'voice_commands', self.voice_callback, 10)\n        \n        self.image_sub = self.create_subscription(\n            Image, 'camera/image_raw', self.image_callback, 10)\n        \n        self.joint_state_sub = self.create_subscription(\n            JointState, 'joint_states', self.joint_state_callback, 10)\n        \n        # Publishers\n        self.manipulation_pub = self.create_publisher(String, 'manipulation_commands', 10)\n        self.object_detection_pub = self.create_publisher(String, 'detected_objects', 10)\n        \n        # Internal state\n        self.latest_image = None\n        self.joint_positions = {}\n        self.requested_object = None\n        self.detected_objects = []\n        \n        # Object color definitions\n        self.object_colors = {\n            'red_cup': ([0, 50, 50], [10, 255, 255]),\n            'blue_bottle': ([100, 50, 50], [130, 255, 255]),\n            'green_box': ([50, 50, 50], [70, 255, 255])\n        }\n        \n        self.get_logger().info('Voice Perception Manipulation Pipeline initialized')\n\n    def voice_callback(self, msg):\n        \"\"\"Process voice command for manipulation\"\"\"\n        command = msg.data.lower()\n        \n        # Check if command is for manipulation\n        if 'pick up' in command or 'grasp' in command or 'take' in command:\n            # Extract object type\n            obj_type = self.extract_object_type(command)\n            if obj_type:\n                self.requested_object = obj_type\n                self.get_logger().info(f'Requested to pick up: {obj_type}')\n                \n                # If we have recent image data, try to detect the object\n                if self.latest_image is not None:\n                    self.process_current_image_for_object(obj_type)\n\n    def extract_object_type(self, command):\n        \"\"\"Extract object type from voice command\"\"\"\n        # Simple keyword matching\n        if 'red cup' in command or 'red_cup' in command:\n            return 'red_cup'\n        elif 'blue bottle' in command or 'blue_bottle' in command:\n            return 'blue_bottle'\n        elif 'green box' in command or 'green_box' in command:\n            return 'green_box'\n        else:\n            # Try to match any known object type\n            for obj_type in self.object_colors.keys():\n                if obj_type.replace('_', ' ') in command:\n                    return obj_type\n        return None\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera image\"\"\"\n        try:\n            self.latest_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n            \n            # If we're looking for a specific object, process immediately\n            if self.requested_object:\n                self.process_current_image_for_object(self.requested_object)\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def process_current_image_for_object(self, obj_type):\n        \"\"\"Process current image to find the requested object\"\"\"\n        if self.latest_image is None:\n            self.get_logger().warn('No image available to process')\n            return\n        \n        # Detect objects in image\n        detected_objects = self.detect_objects_in_image(self.latest_image)\n        \n        # Filter for requested object type\n        target_objects = [obj for obj in detected_objects if obj['name'] == obj_type]\n        \n        if target_objects:\n            # Sort by size (largest first - likely closest)\n            target_objects.sort(key=lambda x: x['area'], reverse=True)\n            target_obj = target_objects[0]\n            \n            self.get_logger().info(f'Found {obj_type}: {target_obj}')\n            \n            # Send manipulation command\n            self.execute_manipulation_for_object(target_obj)\n        else:\n            self.get_logger().info(f'{obj_type} not found in current view')\n\n    def detect_objects_in_image(self, image):\n        \"\"\"Detect objects in an image using color-based detection\"\"\"\n        objects = []\n        \n        for obj_name, (lower_color, upper_color) in self.object_colors.items():\n            # Create mask for the color range\n            lower = np.array(lower_color)\n            upper = np.array(upper_color)\n            mask = cv2.inRange(image, lower, upper)\n            \n            # Find contours in the mask\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            \n            for contour in contours:\n                # Filter by size to avoid noise\n                area = cv2.contourArea(contour)\n                if area > 300:  # Minimum area threshold\n                    # Calculate bounding box\n                    x, y, w, h = cv2.boundingRect(contour)\n                    \n                    # Calculate center position\n                    center_x = x + w // 2\n                    center_y = y + h // 2\n                    \n                    # Calculate image coordinates (0-1 normalized)\n                    img_width, img_height = image.shape[1], image.shape[0]\n                    norm_x = center_x / img_width\n                    norm_y = center_y / img_height\n                    \n                    # Add detected object\n                    objects.append({\n                        'name': obj_name,\n                        'center': {'x': center_x, 'y': center_y},\n                        'normalized_center': {'x': norm_x, 'y': norm_y},\n                        'bbox': {'x': x, 'y': y, 'width': w, 'height': h},\n                        'area': area\n                    })\n        \n        return objects\n\n    def execute_manipulation_for_object(self, obj):\n        \"\"\"Execute manipulation to pick up the detected object\"\"\"\n        # Create manipulation command\n        manipulation_cmd = {\n            'action': 'grasp_object',\n            'object_info': obj,\n            'approach_vector': [0, 0, -1],  # Approach from above\n            'grasp_type': 'top_grasp'\n        }\n        \n        cmd_msg = String()\n        cmd_msg.data = json.dumps(manipulation_cmd)\n        self.manipulation_pub.publish(cmd_msg)\n        \n        self.get_logger().info(f'Sent manipulation command for {obj[\"name\"]}')\n\n    def joint_state_callback(self, msg):\n        \"\"\"Update joint positions\"\"\"\n        for i, name in enumerate(msg.name):\n            if i < len(msg.position):\n                self.joint_positions[name] = msg.position[i]\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoicePerceptionManipulationPipeline()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"validation-3",children:"Validation"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Test with voice commands requesting specific objects"}),"\n",(0,o.jsx)(n.li,{children:"Verify object detection works correctly"}),"\n",(0,o.jsx)(n.li,{children:"Check that manipulation commands are generated appropriately"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"exercise-5-system-state-management",children:"Exercise 5: System State Management"}),"\n",(0,o.jsx)(n.h3,{id:"objective-4",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Implement a system state manager that coordinates all components and handles transitions between operational states."}),"\n",(0,o.jsx)(n.h3,{id:"task-4",children:"Task"}),"\n",(0,o.jsx)(n.p,{children:"Create a state machine that manages the robot's operational states and coordinates component activities based on the current state."}),"\n",(0,o.jsx)(n.h3,{id:"implementation-steps-4",children:"Implementation Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Define robot operational states (idle, navigating, manipulating, error, etc.)"}),"\n",(0,o.jsx)(n.li,{children:"Implement state transition logic"}),"\n",(0,o.jsx)(n.li,{children:"Coordinate component activities based on current state"}),"\n",(0,o.jsx)(n.li,{children:"Handle error states and recovery"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"sample-code-structure-4",children:"Sample Code Structure"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom enum import Enum\nimport json\n\nclass RobotState(Enum):\n    IDLE = \"idle\"\n    NAVIGATING = \"navigating\"\n    MANIPULATING = \"manipulating\"\n    ERROR = \"error\"\n    SAFETY_STOP = \"safety_stop\"\n    WAITING_FOR_COMMAND = \"waiting_for_command\"\n\nclass SystemStateManager(Node):\n    def __init__(self):\n        super().__init__('system_state_manager')\n        \n        # Subscriptions\n        self.voice_sub = self.create_subscription(\n            String, 'voice_commands', self.voice_callback, 10)\n        \n        self.status_sub = self.create_subscription(\n            String, 'component_status', self.status_callback, 10)\n        \n        self.error_sub = self.create_subscription(\n            String, 'error_reports', self.error_callback, 10)\n        \n        # Publishers\n        self.state_pub = self.create_publisher(String, 'system_state', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, 'cmd_vel', 10)\n        \n        # Initialize state\n        self.current_state = RobotState.WAITING_FOR_COMMAND\n        self.state_history = [self.current_state]\n        \n        # Component status tracking\n        self.component_status = {\n            'voice_processor': 'unknown',\n            'navigator': 'unknown', \n            'manipulator': 'unknown',\n            'perception': 'unknown'\n        }\n        \n        # Start state monitoring\n        self.state_timer = self.create_timer(1.0, self.monitor_state)\n        \n        self.get_logger().info('System State Manager initialized')\n\n    def voice_callback(self, msg):\n        \"\"\"Handle voice commands based on current state\"\"\"\n        if self.current_state == RobotState.WAITING_FOR_COMMAND:\n            command = msg.data.lower()\n            \n            if 'navigate' in command or 'go to' in command:\n                self.transition_to_state(RobotState.NAVIGATING)\n            elif 'pick up' in command or 'grasp' in command:\n                self.transition_to_state(RobotState.MANIPULATING)\n            else:\n                self.get_logger().info(f'Received command in {self.current_state.value}: {command}')\n        else:\n            self.get_logger().info(f'Ignoring command in state {self.current_state.value}')\n\n    def status_callback(self, msg):\n        \"\"\"Update component status\"\"\"\n        try:\n            status_data = json.loads(msg.data)\n            component = status_data.get('component')\n            status = status_data.get('status')\n            \n            if component in self.component_status:\n                self.component_status[component] = status\n        except json.JSONDecodeError:\n            self.get_logger().error('Invalid status message format')\n\n    def error_callback(self, msg):\n        \"\"\"Handle error reports\"\"\"\n        self.get_logger().error(f'Error reported: {msg.data}')\n        self.transition_to_state(RobotState.ERROR)\n\n    def transition_to_state(self, new_state):\n        \"\"\"Safely transition to a new state\"\"\"\n        # Define valid state transitions\n        valid_transitions = {\n            RobotState.WAITING_FOR_COMMAND: [RobotState.NAVIGATING, RobotState.MANIPULATING, RobotState.SAFETY_STOP],\n            RobotState.NAVIGATING: [RobotState.IDLE, RobotState.SAFETY_STOP, RobotState.ERROR],\n            RobotState.MANIPULATING: [RobotState.IDLE, RobotState.SAFETY_STOP, RobotState.ERROR],\n            RobotState.IDLE: [RobotState.NAVIGATING, RobotState.MANIPULATING, RobotState.WAITING_FOR_COMMAND, RobotState.SAFETY_STOP],\n            RobotState.ERROR: [RobotState.SAFETY_STOP, RobotState.IDLE],\n            RobotState.SAFETY_STOP: [RobotState.IDLE, RobotState.ERROR]\n        }\n        \n        if new_state in valid_transitions.get(self.current_state, []):\n            old_state = self.current_state\n            self.current_state = new_state\n            self.state_history.append(new_state)\n            \n            self.get_logger().info(f'State transition: {old_state.value} -> {new_state.value}')\n            \n            # Execute state-specific actions\n            self.execute_state_actions(new_state)\n            \n            # Publish new state\n            state_msg = String()\n            state_msg.data = json.dumps({\n                'current_state': new_state.value,\n                'timestamp': self.get_clock().now().nanoseconds\n            })\n            self.state_pub.publish(state_msg)\n            \n            return True\n        else:\n            self.get_logger().warn(f'Invalid state transition: {self.current_state.value} -> {new_state.value}')\n            return False\n\n    def execute_state_actions(self, state):\n        \"\"\"Execute actions specific to the current state\"\"\"\n        if state == RobotState.SAFETY_STOP:\n            # Emergency stop - halt all motion\n            stop_cmd = Twist()\n            self.cmd_vel_pub.publish(stop_cmd)\n        elif state == RobotState.IDLE:\n            # In idle state, prepare for next command\n            self.get_logger().info('Robot is idle, waiting for command')\n        elif state == RobotState.NAVIGATING:\n            # Navigation-specific actions would go here\n            self.get_logger().info('Robot is navigating')\n        elif state == RobotState.MANIPULATING:\n            # Manipulation-specific actions would go here\n            self.get_logger().info('Robot is manipulating')\n\n    def monitor_state(self):\n        \"\"\"Monitor system state and handle any necessary transitions\"\"\"\n        # Check component health\n        for component, status in self.component_status.items():\n            if status == 'error':\n                self.get_logger().warn(f'{component} reported error status')\n                if self.current_state != RobotState.ERROR:\n                    self.transition_to_state(RobotState.ERROR)\n        \n        # Check for timeout in certain states\n        if self.current_state in [RobotState.NAVIGATING, RobotState.MANIPULATING]:\n            # Implementation would check for task completion or timeout\n            pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SystemStateManager()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"validation-4",children:"Validation"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Test state transitions with different commands"}),"\n",(0,o.jsx)(n.li,{children:"Verify that appropriate actions are taken in each state"}),"\n",(0,o.jsx)(n.li,{children:"Check error handling and recovery"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"exercise-6-complete-integration-challenge",children:"Exercise 6: Complete Integration Challenge"}),"\n",(0,o.jsx)(n.h3,{id:"objective-5",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Combine all previous exercises into a complete integrated system."}),"\n",(0,o.jsx)(n.h3,{id:"task-5",children:"Task"}),"\n",(0,o.jsx)(n.p,{children:"Create a main orchestrator node that integrates voice processing, navigation, perception, manipulation, and state management into a cohesive system."}),"\n",(0,o.jsx)(n.h3,{id:"implementation-steps-5",children:"Implementation Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Create an orchestrator node that manages all components"}),"\n",(0,o.jsx)(n.li,{children:"Implement proper message routing between components"}),"\n",(0,o.jsx)(n.li,{children:"Add system-level error handling and recovery"}),"\n",(0,o.jsx)(n.li,{children:"Test complete end-to-end functionality"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"sample-code-structure-5",children:"Sample Code Structure"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom sensor_msgs.msg import Image, LaserScan, JointState\nfrom nav_msgs.msg import Odometry\nfrom cv_bridge import CvBridge\nimport json\nimport numpy as np\n\nclass CapstoneOrchestrator(Node):\n    def __init__(self):\n        super().__init__('capstone_orchestrator')\n        \n        # Initialize OpenCV bridge\n        self.bridge = CvBridge()\n        \n        # Subscriptions\n        self.voice_sub = self.create_subscription(\n            String, 'voice_commands', self.voice_callback, 10)\n        \n        self.laser_sub = self.create_subscription(\n            LaserScan, 'scan', self.laser_callback, 10)\n        \n        self.image_sub = self.create_subscription(\n            Image, 'camera/image_raw', self.image_callback, 10)\n        \n        self.odom_sub = self.create_subscription(\n            Odometry, 'odom', self.odom_callback, 10)\n        \n        self.joint_state_sub = self.create_subscription(\n            JointState, 'joint_states', self.joint_state_callback, 10)\n        \n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, 'cmd_vel', 10)\n        self.goal_pub = self.create_publisher(PoseStamped, 'goal_pose', 10)\n        self.manipulation_pub = self.create_publisher(String, 'manipulation_commands', 10)\n        self.system_status_pub = self.create_publisher(String, 'system_status', 10)\n        \n        # Internal state\n        self.robot_position = None\n        self.joint_positions = {}\n        self.latest_image = None\n        self.obstacles = []\n        self.detected_objects = []\n        \n        # System state\n        self.current_task = None\n        self.task_queue = []\n        \n        # Object detection parameters\n        self.object_colors = {\n            'red_cup': ([0, 50, 50], [10, 255, 255]),\n            'blue_bottle': ([100, 50, 50], [130, 255, 255]),\n            'green_box': ([50, 50, 50], [70, 255, 255])\n        }\n        \n        # Start system monitoring\n        self.monitor_timer = self.create_timer(0.5, self.monitor_system)\n        \n        self.get_logger().info('Capstone Orchestrator initialized')\n\n    def voice_callback(self, msg):\n        \"\"\"Process voice command and queue appropriate task\"\"\"\n        command = msg.data.lower()\n        self.get_logger().info(f'Received voice command: {command}')\n        \n        # Parse command and create appropriate task\n        if 'navigate to' in command or 'go to' in command:\n            location = self.extract_location(command)\n            if location:\n                task = {\n                    'type': 'navigation',\n                    'location': location,\n                    'priority': 1\n                }\n                self.task_queue.append(task)\n                \n        elif 'pick up' in command or 'grasp' in command:\n            obj_type = self.extract_object_type(command)\n            if obj_type:\n                task = {\n                    'type': 'manipulation', \n                    'object_type': obj_type,\n                    'priority': 2\n                }\n                self.task_queue.append(task)\n        \n        elif 'stop' in command:\n            self.emergency_stop()\n        \n        # Publish system status\n        self.publish_system_status()\n\n    def extract_location(self, command):\n        \"\"\"Extract location from navigation command\"\"\"\n        locations = ['kitchen', 'living room', 'office', 'bedroom', 'charging station']\n        for loc in locations:\n            if loc in command:\n                return loc.replace(' ', '_')\n        return None\n\n    def extract_object_type(self, command):\n        \"\"\"Extract object type from manipulation command\"\"\"\n        for obj_type in self.object_colors.keys():\n            if obj_type.replace('_', ' ') in command:\n                return obj_type\n        return None\n\n    def laser_callback(self, msg):\n        \"\"\"Process laser scan for obstacle detection\"\"\"\n        # Convert to obstacle positions (same as previous exercise)\n        angles = np.linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        valid_ranges = np.array(msg.ranges)\n        valid_mask = (valid_ranges > msg.range_min) & (valid_ranges < msg.range_max)\n        valid_angles = angles[valid_mask]\n        valid_ranges = valid_ranges[valid_mask]\n        \n        obstacle_x = valid_ranges * np.cos(valid_angles)\n        obstacle_y = valid_ranges * np.sin(valid_angles)\n        \n        self.obstacles = list(zip(obstacle_x, obstacle_y))\n\n    def image_callback(self, msg):\n        \"\"\"Process camera image for object detection\"\"\"\n        try:\n            self.latest_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n            self.detected_objects = self.detect_objects_in_image(self.latest_image)\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def odom_callback(self, msg):\n        \"\"\"Update robot position\"\"\"\n        self.robot_position = (\n            msg.pose.pose.position.x,\n            msg.pose.pose.position.y,\n            msg.pose.pose.position.z\n        )\n\n    def joint_state_callback(self, msg):\n        \"\"\"Update joint positions\"\"\"\n        for i, name in enumerate(msg.name):\n            if i < len(msg.position):\n                self.joint_positions[name] = msg.position[i]\n\n    def monitor_system(self):\n        \"\"\"Monitor system and execute tasks\"\"\"\n        # Execute highest priority task if system is ready\n        if self.task_queue:\n            # Sort by priority (lower number = higher priority)\n            self.task_queue.sort(key=lambda x: x['priority'])\n            current_task = self.task_queue[0]\n            \n            if current_task['type'] == 'navigation':\n                self.execute_navigation_task(current_task)\n            elif current_task['type'] == 'manipulation':\n                self.execute_manipulation_task(current_task)\n            \n            # Remove completed task\n            self.task_queue.pop(0)\n\n    def execute_navigation_task(self, task):\n        \"\"\"Execute navigation task\"\"\"\n        location = task['location']\n        \n        # Define location coordinates\n        location_map = {\n            'kitchen': (5.0, 3.0, 0.0),\n            'living_room': (2.0, 8.0, 0.0),\n            'office': (8.0, 2.0, 0.0),\n            'bedroom': (7.0, 7.0, 0.0),\n            'charging_station': (0.0, 0.0, 0.0)\n        }\n        \n        if location in location_map:\n            x, y, theta = location_map[location]\n            goal_msg = PoseStamped()\n            goal_msg.header.frame_id = 'map'\n            goal_msg.header.stamp = self.get_clock().now().to_msg()\n            goal_msg.pose.position.x = x\n            goal_msg.pose.position.y = y\n            goal_msg.pose.orientation.z = theta\n            \n            self.goal_pub.publish(goal_msg)\n            self.get_logger().info(f'Navigating to {location}')\n        else:\n            self.get_logger().warn(f'Unknown location: {location}')\n\n    def execute_manipulation_task(self, task):\n        \"\"\"Execute manipulation task\"\"\"\n        obj_type = task['object_type']\n        \n        # Find the requested object in detected objects\n        target_objects = [obj for obj in self.detected_objects if obj['name'] == obj_type]\n        \n        if target_objects:\n            # Use the largest (closest) object\n            target_obj = max(target_objects, key=lambda x: x['area'])\n            \n            # Create manipulation command\n            manipulation_cmd = {\n                'action': 'grasp_object',\n                'object_info': target_obj,\n                'approach_vector': [0, 0, -1],\n                'grasp_type': 'top_grasp'\n            }\n            \n            cmd_msg = String()\n            cmd_msg.data = json.dumps(manipulation_cmd)\n            self.manipulation_pub.publish(cmd_msg)\n            \n            self.get_logger().info(f'Attempting to grasp {obj_type}')\n        else:\n            self.get_logger().info(f'{obj_type} not detected, cannot grasp')\n\n    def detect_objects_in_image(self, image):\n        \"\"\"Detect objects in image (same as previous exercise)\"\"\"\n        objects = []\n        \n        for obj_name, (lower_color, upper_color) in self.object_colors.items():\n            lower = np.array(lower_color)\n            upper = np.array(upper_color)\n            mask = cv2.inRange(image, lower, upper)\n            \n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            \n            for contour in contours:\n                area = cv2.contourArea(contour)\n                if area > 300:\n                    x, y, w, h = cv2.boundingRect(contour)\n                    center_x, center_y = x + w // 2, y + h // 2\n                    \n                    img_width, img_height = image.shape[1], image.shape[0]\n                    norm_x, norm_y = center_x / img_width, center_y / img_height\n                    \n                    objects.append({\n                        'name': obj_name,\n                        'center': {'x': center_x, 'y': center_y},\n                        'normalized_center': {'x': norm_x, 'y': norm_y},\n                        'bbox': {'x': x, 'y': y, 'width': w, 'height': h},\n                        'area': area\n                    })\n        \n        return objects\n\n    def emergency_stop(self):\n        \"\"\"Emergency stop all robot motion\"\"\"\n        stop_cmd = Twist()\n        self.cmd_vel_pub.publish(stop_cmd)\n        \n        # Clear task queue\n        self.task_queue.clear()\n        \n        self.get_logger().info('Emergency stop activated')\n\n    def publish_system_status(self):\n        \"\"\"Publish comprehensive system status\"\"\"\n        status = {\n            'timestamp': self.get_clock().now().nanoseconds,\n            'robot_position': self.robot_position,\n            'joint_positions': self.joint_positions,\n            'detected_objects_count': len(self.detected_objects),\n            'obstacles_count': len(self.obstacles),\n            'task_queue_size': len(self.task_queue),\n            'current_task': self.current_task\n        }\n        \n        status_msg = String()\n        status_msg.data = json.dumps(status)\n        self.system_status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CapstoneOrchestrator()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"validation-5",children:"Validation"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Test complete end-to-end functionality"}),"\n",(0,o.jsx)(n.li,{children:"Verify all components work together"}),"\n",(0,o.jsx)(n.li,{children:"Check system response to various commands"}),"\n",(0,o.jsx)(n.li,{children:"Validate error handling and recovery"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"exercise-7-performance-and-optimization",children:"Exercise 7: Performance and Optimization"}),"\n",(0,o.jsx)(n.h3,{id:"objective-6",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Analyze and optimize the integrated system for better performance."}),"\n",(0,o.jsx)(n.h3,{id:"task-6",children:"Task"}),"\n",(0,o.jsx)(n.p,{children:"Implement performance monitoring and optimization techniques for the integrated system."}),"\n",(0,o.jsx)(n.h3,{id:"implementation-steps-6",children:"Implementation Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Add performance monitoring to track system metrics"}),"\n",(0,o.jsx)(n.li,{children:"Implement optimization techniques for resource usage"}),"\n",(0,o.jsx)(n.li,{children:"Add quality of service (QoS) configurations"}),"\n",(0,o.jsx)(n.li,{children:"Test system performance under various loads"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"sample-code-structure-6",children:"Sample Code Structure"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, DurabilityPolicy\nfrom std_msgs.msg import String\nimport time\nimport threading\n\nclass PerformanceOptimizer(Node):\n    def __init__(self):\n        super().__init__(\'performance_optimizer\')\n        \n        # Create QoS profiles for different data types\n        self.high_reliability_qos = QoSProfile(\n            depth=10,\n            reliability=ReliabilityPolicy.RELIABLE,\n            durability=DurabilityPolicy.VOLATILE\n        )\n        \n        self.low_latency_qos = QoSProfile(\n            depth=1,\n            reliability=ReliabilityPolicy.BEST_EFFORT,\n            durability=DurabilityPolicy.VOLATILE\n        )\n        \n        # Subscriptions with appropriate QoS\n        self.critical_sub = self.create_subscription(\n            String, \'critical_data\', self.critical_callback, \n            self.high_reliability_qos)\n        \n        self.performance_sub = self.create_subscription(\n            String, \'performance_data\', self.performance_callback,\n            self.low_latency_qos)\n        \n        # Publishers\n        self.metrics_pub = self.create_publisher(String, \'performance_metrics\', 10)\n        \n        # Performance tracking\n        self.message_times = {}\n        self.cpu_usage = []\n        self.memory_usage = []\n        \n        # Start performance monitoring\n        self.monitor_timer = self.create_timer(1.0, self.report_performance)\n        \n        self.get_logger().info(\'Performance Optimizer initialized\')\n\n    def critical_callback(self, msg):\n        """Handle critical messages with performance tracking"""\n        start_time = time.time()\n        \n        # Process critical message\n        self.process_critical_data(msg)\n        \n        # Track processing time\n        processing_time = time.time() - start_time\n        self.message_times[\'critical\'] = processing_time\n        \n        if processing_time > 0.1:  # More than 100ms\n            self.get_logger().warn(f\'Critical message processing took {processing_time:.3f}s\')\n\n    def performance_callback(self, msg):\n        """Handle performance data"""\n        # Track performance metrics\n        pass\n\n    def process_critical_data(self, msg):\n        """Process critical data with optimization"""\n        # Implementation would process critical data\n        pass\n\n    def report_performance(self):\n        """Report system performance metrics"""\n        metrics = {\n            \'timestamp\': time.time(),\n            \'message_processing_times\': self.message_times.copy(),\n            \'average_cpu_usage\': sum(self.cpu_usage[-10:]) / min(len(self.cpu_usage), 10) if self.cpu_usage else 0,\n            \'latest_memory_usage\': self.memory_usage[-1] if self.memory_usage else 0\n        }\n        \n        # Publish metrics\n        metrics_msg = String()\n        metrics_msg.data = json.dumps(metrics)\n        self.metrics_pub.publish(metrics_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PerformanceOptimizer()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"validation-6",children:"Validation"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Monitor system performance metrics"}),"\n",(0,o.jsx)(n.li,{children:"Verify optimization techniques are effective"}),"\n",(0,o.jsx)(n.li,{children:"Test system under various load conditions"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"How does component communication work in a ROS 2-based integrated system?"}),"\n",(0,o.jsx)(n.li,{children:"What are the key challenges in coordinating multiple robotic subsystems?"}),"\n",(0,o.jsx)(n.li,{children:"How do you handle state management in an integrated robotic system?"}),"\n",(0,o.jsx)(n.li,{children:"What safety considerations are important when integrating perception and navigation?"}),"\n",(0,o.jsx)(n.li,{children:"How would you optimize an integrated system for real-time performance?"}),"\n",(0,o.jsx)(n.li,{children:"What debugging strategies are effective for integrated robotic systems?"}),"\n",(0,o.jsx)(n.li,{children:"How do you handle error propagation between integrated components?"}),"\n",(0,o.jsx)(n.li,{children:"What role does system architecture play in successful integration?"}),"\n",(0,o.jsx)(n.li,{children:"How would you validate that all components are properly integrated?"}),"\n",(0,o.jsx)(n.li,{children:"What are the trade-offs between different integration approaches?"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"These exercises progressively build from basic component communication to a complete integrated system, providing hands-on experience with the challenges and solutions involved in system integration for the capstone project."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}},8453(e,n,t){t.d(n,{R:()=>a,x:()=>r});var s=t(6540);const o={},i=s.createContext(o);function a(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);