"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[1075],{801(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module-4-vision-language-action/voice-to-action","title":"Voice-to-Action Pipeline","description":"Learn how to create a complete pipeline from voice commands to robotic actions","source":"@site/docs/module-4-vision-language-action/voice-to-action.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/voice-to-action","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/your-organization/physical-ai-humanoid-robotics-book/edit/main/book/docs/module-4-vision-language-action/voice-to-action.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Voice-to-Action Pipeline","description":"Learn how to create a complete pipeline from voice commands to robotic actions","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"LLM Cognitive Planning for Robotics","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/llm-cognitive-planning"},"next":{"title":"OpenAI Whisper Integration Code Examples","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/openai-whisper-examples"}}');var i=t(4848),a=t(8453);const r={title:"Voice-to-Action Pipeline",description:"Learn how to create a complete pipeline from voice commands to robotic actions",sidebar_position:3},s="Voice-to-Action Pipeline",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Architecture of the Voice-to-Action System",id:"architecture-of-the-voice-to-action-system",level:2},{value:"Implementing the Complete Pipeline",id:"implementing-the-complete-pipeline",level:2},{value:"1. System Integration Node",id:"1-system-integration-node",level:3},{value:"Advanced Voice Command Processing",id:"advanced-voice-command-processing",level:2},{value:"Contextual Understanding",id:"contextual-understanding",level:3},{value:"Intent Recognition and Entity Extraction",id:"intent-recognition-and-entity-extraction",level:3},{value:"Error Handling and Recovery",id:"error-handling-and-recovery",level:2},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Caching Common Commands",id:"caching-common-commands",level:3},{value:"Asynchronous Processing",id:"asynchronous-processing",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"voice-to-action-pipeline",children:"Voice-to-Action Pipeline"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"The voice-to-action pipeline combines speech recognition and cognitive planning to create robots that respond to natural language commands. This chapter details how to integrate OpenAI Whisper for speech recognition with Large Language Models for cognitive planning, creating a complete system for voice-controlled robotics."}),"\n",(0,i.jsx)(n.h2,{id:"architecture-of-the-voice-to-action-system",children:"Architecture of the Voice-to-Action System"}),"\n",(0,i.jsx)(n.p,{children:"The complete voice-to-action pipeline consists of several interconnected components:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Voice Command\n      \u2193\n[Audio Capture]\n      \u2193\n[Preprocessing & Enhancement]\n      \u2193\n[Whisper ASR - Speech to Text]\n      \u2193\n[Natural Language Understanding]\n      \u2193\n[LLM Cognitive Planning]\n      \u2193\n[Action Validation & Safety Checks]\n      \u2193\n[Low-Level Robot Control]\n      \u2193\nRobot Action Execution\n"})}),"\n",(0,i.jsx)(n.p,{children:"Each component plays a crucial role in transforming human speech into robot behavior."}),"\n",(0,i.jsx)(n.h2,{id:"implementing-the-complete-pipeline",children:"Implementing the Complete Pipeline"}),"\n",(0,i.jsx)(n.h3,{id:"1-system-integration-node",children:"1. System Integration Node"}),"\n",(0,i.jsx)(n.p,{children:"Here's a complete ROS 2 node that integrates all components:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nimport numpy as np\nimport pyaudio\nimport whisper\nimport threading\nimport queue\nimport time\nimport json\nimport openai\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\nimport librosa\n\nclass VoiceToActionNode(Node):\n    def __init__(self):\n        super().__init__(\'voice_to_action_node\')\n        \n        # Initialize Whisper model for speech recognition\n        self.whisper_model = whisper.load_model("base")\n        \n        # Set OpenAI API key for cognitive planning\n        openai.api_key = "YOUR_API_KEY_HERE"\n        \n        # Audio parameters\n        self.rate = 16000\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        \n        # Initialize PyAudio\n        self.audio = pyaudio.PyAudio()\n        \n        # Publishers\n        self.interim_text_pub = self.create_publisher(String, \'interim_speech_text\', 10)\n        self.final_text_pub = self.create_publisher(String, \'final_speech_text\', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        \n        # Subscribers\n        self.voice_command_sub = self.create_subscription(\n            String,\n            \'final_speech_text\',\n            self.voice_command_callback,\n            10\n        )\n        \n        # Robot state\n        self.robot_state = {\n            \'position\': {\'x\': 0, \'y\': 0, \'theta\': 0},\n            \'battery_level\': 100,\n            \'attached_object\': None,\n            \'environment\': \'indoor_office\',\n            \'is_moving\': False\n        }\n        \n        # Audio buffer and control\n        self.audio_queue = queue.Queue()\n        self.is_listening = True\n        self.wake_word_detected = False\n        self.wake_word = "robot"\n        \n        # Start audio recording thread\n        self.recording_thread = threading.Thread(target=self.record_audio)\n        self.recording_thread.daemon = True\n        self.recording_thread.start()\n        \n        # Start processing thread\n        self.processing_thread = threading.Thread(target=self.process_audio_queue)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n        \n        self.get_logger().info(\'Voice-to-Action Node initialized\')\n\n    def record_audio(self):\n        """Continuously record audio and add to queue"""\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n        \n        while self.is_listening:\n            data = stream.read(self.chunk)\n            self.audio_queue.put(data)\n        \n        stream.stop_stream()\n        stream.close()\n\n    def process_audio_queue(self):\n        """Process audio chunks from the queue"""\n        audio_buffer = []\n        buffer_duration = 3  # Process every 3 seconds of audio\n        max_chunks = int(buffer_duration * self.rate / self.chunk)\n        \n        while self.is_listening:\n            try:\n                # Get audio chunk from queue (with timeout)\n                audio_chunk = self.audio_queue.get(timeout=0.1)\n                audio_buffer.append(audio_chunk)\n                \n                # If buffer is full, process the audio\n                if len(audio_buffer) >= max_chunks:\n                    # Convert buffer to numpy array\n                    audio_data = b\'\'.join(audio_buffer)\n                    audio_np = np.frombuffer(audio_data, dtype=np.int16)\n                    \n                    # Normalize to [-1, 1]\n                    audio_float = audio_np.astype(np.float32) / 32768.0\n                    \n                    # Check for wake word first\n                    if not self.wake_word_detected:\n                        self.check_wake_word(audio_float)\n                    \n                    # Process with Whisper if wake word was detected\n                    elif self.wake_word_detected:\n                        result = self.whisper_model.transcribe(audio_float)\n                        recognized_text = result[\'text\'].strip()\n                        \n                        if recognized_text:\n                            # Publish interim text\n                            interim_msg = String()\n                            interim_msg.data = recognized_text\n                            self.interim_text_pub.publish(interim_msg)\n                            \n                            # Check if this is a complete command (simple heuristic)\n                            if self.is_complete_command(recognized_text):\n                                # Publish final text for processing\n                                final_msg = String()\n                                final_msg.data = recognized_text\n                                self.final_text_pub.publish(final_msg)\n                                \n                                # Reset wake word detection\n                                self.wake_word_detected = False\n                                self.get_logger().info(f\'Command recognized: "{recognized_text}"\')\n                        \n                        # Clear buffer\n                        audio_buffer = []\n                        \n            except queue.Empty:\n                continue\n\n    def check_wake_word(self, audio_float):\n        """Check if the wake word is present in the audio"""\n        # For simplicity, we\'ll assume wake word detection is triggered externally\n        # In a real implementation, you\'d use a dedicated wake word detection model\n        # or implement keyword spotting\n        \n        # For this example, we\'ll simulate wake word detection\n        # by publishing a message to a topic when wake word is "detected"\n        self.get_logger().info(f\'Listening for wake word: {self.wake_word}\')\n        # In a real implementation, this would analyze audio_float for the wake word\n\n    def is_complete_command(self, text):\n        """Simple heuristic to determine if text is a complete command"""\n        # In a real implementation, this would be more sophisticated\n        # For now, we\'ll consider it complete if it\'s not just a wake word\n        return len(text.strip()) > len(self.wake_word) and text.lower() != self.wake_word.lower()\n\n    def voice_command_callback(self, msg):\n        """Process recognized voice command and generate robot action"""\n        command = msg.data\n        self.get_logger().info(f\'Processing command: {command}\')\n        \n        # Generate plan using LLM\n        plan = self.generate_plan(command)\n        \n        if plan:\n            # Execute the plan safely\n            self.execute_plan_safely(plan)\n        else:\n            self.get_logger().warn(\'Could not generate a plan for the command\')\n            # Provide feedback to user\n            feedback_msg = String()\n            feedback_msg.data = f"Sorry, I couldn\'t understand the command: {command}"\n            self.interim_text_pub.publish(feedback_msg)\n\n    def generate_plan(self, command):\n        """Generate a step-by-step plan using an LLM"""\n        prompt = f"""\n        You are a cognitive planner for a TurtleBot3 robot operating in an indoor office environment.\n        The robot can perform the following actions:\n        - move_forward(distance_in_meters)\n        - move_backward(distance_in_meters)\n        - turn_left(degrees)\n        - turn_right(degrees)\n        - stop()\n        - pick_up_object()\n        - place_object()\n        - detect_object(object_type)\n        - check_battery()\n        \n        Current robot state:\n        {json.dumps(self.robot_state, indent=2)}\n        \n        Human command: "{command}"\n        \n        Respond with a JSON object containing a step-by-step plan. Each step should be a dictionary with an \'action\' and \'parameters\'.\n        Format:\n        {{\n          "plan": [\n            {{"action": "move_forward", "parameters": {{"distance": 1.0}}}},\n            {{"action": "turn_left", "parameters": {{"degrees": 90}}}},\n            {{"action": "pick_up_object", "parameters": {{}}}}\n          ]\n        }}\n        \n        Be specific with parameters and ensure the plan is executable.\n        """\n        \n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-3.5-turbo",\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.3,\n                max_tokens=500\n            )\n            \n            content = response.choices[0].message[\'content\'].strip()\n            \n            # Extract JSON from the response\n            start_idx = content.find(\'{\')\n            end_idx = content.rfind(\'}\') + 1\n            \n            if start_idx != -1 and end_idx != 0:\n                json_str = content[start_idx:end_idx]\n                plan_data = json.loads(json_str)\n                return plan_data[\'plan\']\n            else:\n                self.get_logger().error(f\'Could not extract JSON from LLM response: {content}\')\n                return None\n                \n        except Exception as e:\n            self.get_logger().error(f\'Error calling LLM: {str(e)}\')\n            return None\n\n    def execute_plan_safely(self, plan):\n        """Execute the plan with safety checks"""\n        if not self.validate_plan(plan):\n            self.get_logger().error(\'Plan failed safety validation\')\n            return False\n        \n        for step in plan:\n            # Check if environment is still safe\n            if not self.check_environment_safety():\n                self.get_logger().error(\'Environment is unsafe, stopping execution\')\n                self.stop_robot()\n                return False\n            \n            # Execute the action\n            self.execute_single_action(step)\n            \n            # Small delay between actions for safety\n            time.sleep(0.5)\n        \n        return True\n\n    def validate_plan(self, plan):\n        """Validate the plan for safety before execution"""\n        for step in plan:\n            action = step[\'action\']\n            params = step.get(\'parameters\', {})\n            \n            # Check for potentially dangerous actions\n            if action in [\'move_forward\', \'move_backward\']:\n                distance = params.get(\'distance\', 0)\n                if distance > 10.0:  # Arbitrary safety limit\n                    self.get_logger().warn(f\'Plan includes unsafe movement: {distance}m\')\n                    return False\n            \n            # Check if trying to pick up when already holding an object\n            if action == \'pick_up_object\' and self.robot_state[\'attached_object\'] is not None:\n                self.get_logger().warn(\'Robot already holding an object, cannot pick up another\')\n                return False\n        \n        return True\n\n    def check_environment_safety(self):\n        """Check if the environment is safe for movement"""\n        # In a real robot, this would check sensor data\n        # for obstacles, people, etc.\n        # For this example, we\'ll return True\n        return True\n\n    def execute_single_action(self, step):\n        """Execute a single action from the plan"""\n        action = step[\'action\']\n        params = step.get(\'parameters\', {})\n        \n        self.get_logger().info(f\'Executing action: {action} with params: {params}\')\n        \n        if action == \'move_forward\':\n            self.move_forward(params.get(\'distance\', 1.0))\n        elif action == \'move_backward\':\n            self.move_backward(params.get(\'distance\', 1.0))\n        elif action == \'turn_left\':\n            self.turn_left(params.get(\'degrees\', 90))\n        elif action == \'turn_right\':\n            self.turn_right(params.get(\'degrees\', 90))\n        elif action == \'stop\':\n            self.stop_robot()\n        elif action == \'pick_up_object\':\n            self.pick_up_object()\n        elif action == \'place_object\':\n            self.place_object()\n        elif action == \'detect_object\':\n            self.detect_object(params.get(\'object_type\', \'any\'))\n        elif action == \'check_battery\':\n            self.check_battery()\n        else:\n            self.get_logger().warn(f\'Unknown action: {action}\')\n\n    def move_forward(self, distance):\n        """Move the robot forward by the specified distance"""\n        msg = Twist()\n        msg.linear.x = 0.2  # m/s\n        duration = distance / 0.2\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def move_backward(self, distance):\n        """Move the robot backward by the specified distance"""\n        msg = Twist()\n        msg.linear.x = -0.2  # m/s\n        duration = distance / 0.2\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def turn_left(self, degrees):\n        """Turn the robot left by the specified degrees"""\n        msg = Twist()\n        msg.angular.z = 0.5  # rad/s\n        duration = (degrees * 3.14159 / 180) / 0.5  # Convert degrees to radians and calculate time\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def turn_right(self, degrees):\n        """Turn the robot right by the specified degrees"""\n        msg = Twist()\n        msg.angular.z = -0.5  # rad/s\n        duration = (degrees * 3.14159 / 180) / 0.5  # Convert degrees to radians and calculate time\n        \n        start_time = self.get_clock().now()\n        while self.get_clock().now() - start_time < rclpy.duration.Duration(seconds=duration):\n            self.cmd_vel_pub.publish(msg)\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        self.stop_robot()\n\n    def stop_robot(self):\n        """Stop the robot"""\n        msg = Twist()\n        self.cmd_vel_pub.publish(msg)\n        self.robot_state[\'is_moving\'] = False\n\n    def pick_up_object(self):\n        """Simulate picking up an object"""\n        self.get_logger().info(\'Picking up object...\')\n        self.robot_state[\'attached_object\'] = \'object\'\n\n    def place_object(self):\n        """Simulate placing an object"""\n        self.get_logger().info(\'Placing object...\')\n        self.robot_state[\'attached_object\'] = None\n\n    def detect_object(self, object_type):\n        """Simulate object detection"""\n        self.get_logger().info(f\'Detecting {object_type}...\')\n\n    def check_battery(self):\n        """Check robot battery level"""\n        self.get_logger().info(f\'Battery level: {self.robot_state["battery_level"]}%\')\n\n    def destroy_node(self):\n        """Clean up resources"""\n        self.is_listening = False\n        self.audio.terminate()\n        super().destroy_node()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceToActionNode()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"advanced-voice-command-processing",children:"Advanced Voice Command Processing"}),"\n",(0,i.jsx)(n.h3,{id:"contextual-understanding",children:"Contextual Understanding"}),"\n",(0,i.jsx)(n.p,{children:"To improve the system's ability to handle context-dependent commands, implement a conversation memory:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class ConversationMemory:\n    def __init__(self, max_history=10):\n        self.history = []\n        self.max_history = max_history\n    \n    def add_interaction(self, user_input, robot_response):\n        """Add a user-robot interaction to the history"""\n        self.history.append({\n            \'user\': user_input,\n            \'robot\': robot_response,\n            \'timestamp\': time.time()\n        })\n        \n        # Keep only the most recent interactions\n        if len(self.history) > self.max_history:\n            self.history = self.history[-self.max_history:]\n    \n    def get_context(self):\n        """Get recent conversation context"""\n        return self.history[-5:]  # Return last 5 interactions\n    \n    def clear_memory(self):\n        """Clear the conversation history"""\n        self.history = []\n\nclass VoiceToActionNodeWithMemory(VoiceToActionNode):\n    def __init__(self):\n        super().__init__()\n        self.conversation_memory = ConversationMemory()\n    \n    def generate_plan(self, command):\n        """Generate a plan considering conversation context"""\n        context = self.conversation_memory.get_context()\n        \n        prompt = f"""\n        You are a cognitive planner for a TurtleBot3 robot operating in an indoor office environment.\n        The robot can perform the following actions:\n        - move_forward(distance_in_meters)\n        - move_backward(distance_in_meters)\n        - turn_left(degrees)\n        - turn_right(degrees)\n        - stop()\n        - pick_up_object()\n        - place_object()\n        - detect_object(object_type)\n        - check_battery()\n        \n        Current robot state:\n        {json.dumps(self.robot_state, indent=2)}\n        \n        Recent conversation history:\n        {json.dumps(context, indent=2)}\n        \n        Human command: "{command}"\n        \n        Respond with a JSON object containing a step-by-step plan. Each step should be a dictionary with an \'action\' and \'parameters\'.\n        Format:\n        {{\n          "plan": [\n            {{"action": "move_forward", "parameters": {{"distance": 1.0}}}},\n            {{"action": "turn_left", "parameters": {{"degrees": 90}}}},\n            {{"action": "pick_up_object", "parameters": {{}}}}\n          ]\n        }}\n        \n        Consider the conversation context when interpreting the command.\n        Be specific with parameters and ensure the plan is executable.\n        """\n        \n        # Implementation similar to parent class\n        # ... (rest of implementation)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"intent-recognition-and-entity-extraction",children:"Intent Recognition and Entity Extraction"}),"\n",(0,i.jsx)(n.p,{children:"For more robust command processing, implement intent recognition:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def extract_intent_and_entities(self, command):\n    """Extract the intent and relevant entities from a command"""\n    prompt = f"""\n    Analyze the following command and extract the intent and entities:\n    \n    Command: "{command}"\n    \n    Return a JSON object with:\n    - intent: The main action requested\n    - entities: Relevant parameters (locations, objects, people, etc.)\n    \n    Example:\n    {{\n      "intent": "navigate_and_manipulate",\n      "entities": {{\n        "action": "pick_up",\n        "object": "red cup",\n        "destination": "kitchen",\n        "recipient": "John"\n      }}\n    }}\n    """\n    \n    # Call LLM to extract intent and entities\n    # Implementation similar to other LLM calls\n    pass\n'})}),"\n",(0,i.jsx)(n.h2,{id:"error-handling-and-recovery",children:"Error Handling and Recovery"}),"\n",(0,i.jsx)(n.p,{children:"Implement robust error handling for the voice-to-action pipeline:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class VoiceToActionNodeWithRecovery(VoiceToActionNode):\n    def __init__(self):\n        super().__init__()\n        self.retry_count = 0\n        self.max_retries = 3\n        self.command_history = []\n    \n    def voice_command_callback(self, msg):\n        """Process voice command with error handling and recovery"""\n        command = msg.data\n        self.get_logger().info(f\'Processing command: {command}\')\n        \n        # Add command to history\n        self.command_history.append({\n            \'command\': command,\n            \'timestamp\': time.time()\n        })\n        \n        # Limit history size\n        if len(self.command_history) > 20:\n            self.command_history = self.command_history[-20:]\n        \n        # Generate and execute plan with error handling\n        try:\n            plan = self.generate_plan(command)\n            \n            if plan:\n                success = self.execute_plan_safely(plan)\n                if not success:\n                    self.handle_execution_failure(command, plan)\n            else:\n                self.handle_planning_failure(command)\n                \n        except Exception as e:\n            self.get_logger().error(f\'Error processing command: {str(e)}\')\n            self.handle_system_error(command, str(e))\n    \n    def handle_execution_failure(self, command, plan):\n        """Handle when plan execution fails"""\n        self.get_logger().warn(f\'Plan execution failed for command: {command}\')\n        \n        # Try to recover by simplifying the plan\n        if self.retry_count < self.max_retries:\n            self.retry_count += 1\n            self.get_logger().info(f\'Attempting recovery, retry {self.retry_count}/{self.max_retries}\')\n            \n            # Simplify the plan or try an alternative approach\n            simplified_plan = self.simplify_plan(plan)\n            if simplified_plan:\n                self.execute_plan_safely(simplified_plan)\n        else:\n            self.get_logger().error(\'Max retries reached, giving up on command\')\n            self.retry_count = 0  # Reset for next command\n    \n    def handle_planning_failure(self, command):\n        """Handle when LLM fails to generate a plan"""\n        self.get_logger().warn(f\'Could not generate plan for command: {command}\')\n        \n        # Provide feedback to user\n        feedback_msg = String()\n        feedback_msg.data = f"Sorry, I couldn\'t understand how to execute: {command}. Could you rephrase?"\n        self.interim_text_pub.publish(feedback_msg)\n    \n    def handle_system_error(self, command, error_msg):\n        """Handle general system errors"""\n        self.get_logger().error(f\'System error for command "{command}": {error_msg}\')\n        \n        # Provide feedback to user\n        feedback_msg = String()\n        feedback_msg.data = "Sorry, I encountered an error processing your command. Please try again."\n        self.interim_text_pub.publish(feedback_msg)\n        \n        # Reset for next command\n        self.retry_count = 0\n'})}),"\n",(0,i.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"caching-common-commands",children:"Caching Common Commands"}),"\n",(0,i.jsx)(n.p,{children:"To reduce API calls and improve response time, cache common command interpretations:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import hashlib\n\nclass CachedVoiceToActionNode(VoiceToActionNode):\n    def __init__(self):\n        super().__init__()\n        self.command_cache = {}\n        self.max_cache_size = 100\n    \n    def generate_plan(self, command):\n        """Generate a plan with caching for common commands"""\n        # Create a hash of the command for caching\n        command_hash = hashlib.md5(command.encode()).hexdigest()\n        \n        # Check if we have a cached plan for this command\n        if command_hash in self.command_cache:\n            self.get_logger().info(\'Using cached plan for command\')\n            return self.command_cache[command_hash]\n        \n        # Generate new plan\n        plan = super().generate_plan(command)\n        \n        # Cache the plan if successful\n        if plan:\n            # Limit cache size\n            if len(self.command_cache) >= self.max_cache_size:\n                # Remove oldest entry (in a real implementation, you might use LRU)\n                oldest_key = next(iter(self.command_cache))\n                del self.command_cache[oldest_key]\n            \n            self.command_cache[command_hash] = plan\n        \n        return plan\n'})}),"\n",(0,i.jsx)(n.h3,{id:"asynchronous-processing",children:"Asynchronous Processing"}),"\n",(0,i.jsx)(n.p,{children:"To improve responsiveness, process audio and generate plans asynchronously:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass AsyncVoiceToActionNode(VoiceToActionNode):\n    def __init__(self):\n        super().__init__()\n        self.executor = ThreadPoolExecutor(max_workers=2)\n        self.plan_queue = asyncio.Queue()\n        \n    async def process_voice_command_async(self, command):\n        """Process voice command asynchronously"""\n        # Generate plan in a separate thread\n        loop = asyncio.get_event_loop()\n        plan = await loop.run_in_executor(self.executor, self.generate_plan, command)\n        \n        if plan:\n            # Execute plan in a separate thread\n            success = await loop.run_in_executor(self.executor, self.execute_plan_safely, plan)\n            return success\n        else:\n            return False\n'})}),"\n",(0,i.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,i.jsx)(n.p,{children:"Create a testing framework to validate the voice-to-action pipeline:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import unittest\n\nclass TestVoiceToActionPipeline(unittest.TestCase):\n    def setUp(self):\n        """Set up test environment"""\n        # In a real test, you might mock the LLM and audio components\n        pass\n    \n    def test_simple_command(self):\n        """Test a simple navigation command"""\n        command = "move forward 2 meters"\n        expected_actions = [\n            {"action": "move_forward", "parameters": {"distance": 2.0}}\n        ]\n        \n        # In a real test, you would validate the generated plan\n        # against expected actions\n        # self.assertEqual(generated_plan, expected_actions)\n    \n    def test_complex_command(self):\n        """Test a complex multi-step command"""\n        command = "go to the kitchen and pick up the red cup"\n        # Validate that this generates appropriate navigation and manipulation steps\n    \n    def test_error_handling(self):\n        """Test error handling for invalid commands"""\n        command = "invalid command that should fail"\n        # Validate that the system handles this gracefully\n'})}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"The voice-to-action pipeline creates an intuitive interface for human-robot interaction by combining speech recognition with cognitive planning. When implementing such a system, consider:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Architecture"}),": Design a robust pipeline with clear separation of concerns"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety"}),": Implement comprehensive validation and safety checks"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performance"}),": Optimize for latency, especially in real-time applications"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reliability"}),": Include error handling and recovery mechanisms"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context"}),": Consider conversation history for better understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Privacy"}),": Choose between cloud and local processing based on requirements"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The next section will explore practical examples of implementing these concepts with real-world robotics applications."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>r,x:()=>s});var o=t(6540);const i={},a=o.createContext(i);function r(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);