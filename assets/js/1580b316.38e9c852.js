"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[325],{261(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"capstone-project/integration-examples","title":"Capstone Code Examples","description":"Code examples for integrating all components without runtime","source":"@site/docs/capstone-project/integration-examples.md","sourceDirName":"capstone-project","slug":"/capstone-project/integration-examples","permalink":"/physical-ai-humanoid-robotics-book/docs/capstone-project/integration-examples","draft":false,"unlisted":false,"editUrl":"https://github.com/your-organization/physical-ai-humanoid-robotics-book/edit/main/book/docs/capstone-project/integration-examples.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Capstone Code Examples","description":"Code examples for integrating all components without runtime","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Capstone Implementation Guide","permalink":"/physical-ai-humanoid-robotics-book/docs/capstone-project/implementation-guide"},"next":{"title":"Capstone Troubleshooting Guide","permalink":"/physical-ai-humanoid-robotics-book/docs/capstone-project/troubleshooting-guide"}}');var o=t(4848),i=t(8453);const s={title:"Capstone Code Examples",description:"Code examples for integrating all components without runtime",sidebar_position:4},r="Capstone Code Examples: Integration of All Components",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Complete System Integration Example",id:"complete-system-integration-example",level:2},{value:"Main System Integration Node",id:"main-system-integration-node",level:3},{value:"Component Integration Examples",id:"component-integration-examples",level:2},{value:"1. Voice-to-Action Pipeline Integration",id:"1-voice-to-action-pipeline-integration",level:3},{value:"2. Perception-Navigation Integration",id:"2-perception-navigation-integration",level:3},{value:"3. Manipulation-Perception Integration",id:"3-manipulation-perception-integration",level:3},{value:"System Validation Examples",id:"system-validation-examples",level:2},{value:"1. Integration Test Example",id:"1-integration-test-example",level:3},{value:"Best Practices for Integration",id:"best-practices-for-integration",level:2},{value:"1. Error Handling and Fallbacks",id:"1-error-handling-and-fallbacks",level:3}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"capstone-code-examples-integration-of-all-components",children:"Capstone Code Examples: Integration of All Components"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"This section provides comprehensive code examples that demonstrate how to integrate all the components learned in previous modules into a cohesive autonomous humanoid system. These examples show the theoretical integration of voice processing, navigation, perception, and manipulation without requiring runtime execution."}),"\n",(0,o.jsx)(n.h2,{id:"complete-system-integration-example",children:"Complete System Integration Example"}),"\n",(0,o.jsx)(n.h3,{id:"main-system-integration-node",children:"Main System Integration Node"}),"\n",(0,o.jsx)(n.p,{children:"Here's a complete example showing how all components work together:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"\"\"\"\nComplete Capstone System Integration Example\n\nThis example demonstrates the integration of all major components:\n- Voice processing with Whisper and LLMs\n- Navigation and path planning\n- Perception and object detection\n- Manipulation control\n- System orchestration\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom sensor_msgs.msg import Image, LaserScan, JointState\nfrom nav_msgs.msg import OccupancyGrid, Path\nfrom cv_bridge import CvBridge\nimport json\nimport numpy as np\nimport whisper\nimport openai\nimport threading\nimport queue\nimport time\n\nclass CapstoneIntegratedSystem(Node):\n    \"\"\"\n    The main integration node that coordinates all capstone components\n    \"\"\"\n    def __init__(self):\n        super().__init__('capstone_integrated_system')\n        \n        # Initialize Whisper model for speech recognition\n        self.whisper_model = whisper.load_model(\"base\")\n        \n        # Initialize OpenAI API for natural language processing\n        openai.api_key = \"YOUR_API_KEY_HERE\"  # In practice, use environment variables\n        \n        # Initialize OpenCV bridge for image processing\n        self.bridge = CvBridge()\n        \n        # Robot state tracking\n        self.robot_state = {\n            'position': {'x': 0, 'y': 0, 'theta': 0},\n            'battery_level': 100,\n            'attached_object': None,\n            'environment': 'indoor_office',\n            'is_moving': False,\n            'is_grasping': False\n        }\n        \n        # Component states\n        self.detected_objects = []\n        self.current_map = None\n        self.local_obstacles = []\n        self.joint_positions = {}\n        \n        # Audio processing buffer\n        self.audio_buffer = queue.Queue()\n        \n        # Initialize all publishers\n        self._initialize_publishers()\n        \n        # Initialize all subscribers\n        self._initialize_subscribers()\n        \n        # Start processing threads\n        self._start_processing_threads()\n        \n        self.get_logger().info('Capstone Integrated System initialized')\n\n    def _initialize_publishers(self):\n        \"\"\"Initialize all publishers for the system\"\"\"\n        # Audio processing\n        self.transcription_pub = self.create_publisher(String, 'transcribed_text', 10)\n        self.action_plan_pub = self.create_publisher(String, 'action_plan', 10)\n        \n        # Navigation\n        self.cmd_vel_pub = self.create_publisher(Twist, 'cmd_vel', 10)\n        self.goal_pub = self.create_publisher(PoseStamped, 'goal_pose', 10)\n        self.path_pub = self.create_publisher(Path, 'global_plan', 10)\n        \n        # Manipulation\n        self.joint_cmd_pub = self.create_publisher(JointState, 'joint_commands', 10)\n        self.manipulation_status_pub = self.create_publisher(String, 'manipulation_status', 10)\n        \n        # Perception\n        self.object_detection_pub = self.create_publisher(String, 'detected_objects', 10)\n        \n        # System\n        self.system_status_pub = self.create_publisher(String, 'system_status', 10)\n\n    def _initialize_subscribers(self):\n        \"\"\"Initialize all subscribers for the system\"\"\"\n        # Audio input (simulated)\n        self.create_subscription(String, 'simulated_audio', self.audio_callback, 10)\n        \n        # Navigation\n        self.create_subscription(OccupancyGrid, 'map', self.map_callback, 10)\n        self.create_subscription(LaserScan, 'scan', self.scan_callback, 10)\n        \n        # Perception\n        self.create_subscription(Image, 'camera/image_raw', self.image_callback, 10)\n        self.create_subscription(JointState, 'joint_states', self.joint_state_callback, 10)\n\n    def _start_processing_threads(self):\n        \"\"\"Start processing threads for different components\"\"\"\n        # Start audio processing thread\n        self.audio_thread = threading.Thread(target=self.process_audio_buffer)\n        self.audio_thread.daemon = True\n        self.audio_thread.start()\n        \n        # Start system monitoring thread\n        self.monitoring_thread = threading.Thread(target=self.monitor_system_status)\n        self.monitoring_thread.daemon = True\n        self.monitoring_thread.start()\n\n    def audio_callback(self, msg):\n        \"\"\"Callback for audio input (simulated in this example)\"\"\"\n        # In a real system, this would receive raw audio data\n        # For this example, we'll treat it as transcribed text\n        self.audio_buffer.put(msg.data)\n\n    def process_audio_buffer(self):\n        \"\"\"Process audio data from the buffer\"\"\"\n        while True:\n            try:\n                # Get audio from buffer with timeout\n                audio_data = self.audio_buffer.get(timeout=1.0)\n                \n                # Process the audio data (in this example, it's already transcribed)\n                self.process_voice_command(audio_data)\n                \n            except queue.Empty:\n                continue  # Continue if no audio in buffer\n\n    def process_voice_command(self, command_text):\n        \"\"\"Process a voice command through the entire pipeline\"\"\"\n        self.get_logger().info(f'Processing voice command: {command_text}')\n        \n        # Step 1: Interpret the command using LLM\n        action_plan = self.interpret_command(command_text)\n        \n        if action_plan:\n            # Step 2: Validate the plan for safety\n            if self.validate_action_plan(action_plan):\n                # Step 3: Execute the plan\n                self.execute_action_plan(action_plan)\n            else:\n                self.get_logger().warn('Action plan failed safety validation')\n        else:\n            self.get_logger().warn(f'Could not interpret command: {command_text}')\n\n    def interpret_command(self, command_text):\n        \"\"\"Interpret natural language command using LLM\"\"\"\n        prompt = f\"\"\"\n        You are a command interpreter for an autonomous humanoid robot.\n        The robot can perform these actions:\n        - navigate_to(location)\n        - pick_up_object(object_type)\n        - place_object(location)\n        - detect_object(object_type)\n        - follow_person(person_name)\n        - avoid_obstacle(obstacle_type)\n        - return_to_charging_station()\n        - check_battery()\n        \n        Current robot state:\n        {json.dumps(self.robot_state, indent=2)}\n        \n        Environment objects (from perception):\n        {json.dumps(self.detected_objects[:5], indent=2)}  # Limit to first 5 for brevity\n        \n        User command: \"{command_text}\"\n\n        Respond with a JSON object containing the action plan:\n        {% raw %}\n        {{\n          \"action\": \"action_name\",\n          \"parameters\": {{\"param1\": \"value1\", \"param2\": \"value2\"}},\n          \"confidence\": 0.0-1.0,\n          \"required_components\": [\"component1\", \"component2\"]\n        }}\n        {% endraw %}\n        \n        If the command is ambiguous or unsafe, return empty parameters and low confidence.\n        \"\"\"\n        \n        try:\n            response = openai.ChatCompletion.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.3,\n                max_tokens=500\n            )\n            \n            content = response.choices[0].message['content'].strip()\n            start_idx = content.find('{')\n            end_idx = content.rfind('}') + 1\n            \n            if start_idx != -1 and end_idx != 0:\n                json_str = content[start_idx:end_idx]\n                plan = json.loads(json_str)\n                return plan\n            else:\n                self.get_logger().error(f'Could not parse LLM response: {content}')\n                return None\n                \n        except Exception as e:\n            self.get_logger().error(f'Error interpreting command: {e}')\n            return None\n\n    def validate_action_plan(self, plan):\n        \"\"\"Validate an action plan for safety and feasibility\"\"\"\n        # Check if required components are available\n        required_components = plan.get('required_components', [])\n        for component in required_components:\n            # In a real system, check if component is ready/available\n            if not self.is_component_available(component):\n                self.get_logger().warn(f'Required component {component} is not available')\n                return False\n        \n        # Check if action is safe given current state\n        action = plan.get('action', '')\n        if action == 'navigate_to':\n            location = plan.get('parameters', {}).get('location')\n            if not self.is_navigation_safe_to(location):\n                self.get_logger().warn(f'Navigation to {location} is not safe')\n                return False\n        elif action == 'pick_up_object':\n            if self.robot_state['attached_object'] is not None:\n                self.get_logger().warn('Robot already holding an object')\n                return False\n        \n        return True\n\n    def is_component_available(self, component):\n        \"\"\"Check if a component is available (simulated)\"\"\"\n        # In a real system, this would check component status\n        # For this example, assume all components are available\n        return True\n\n    def is_navigation_safe_to(self, location):\n        \"\"\"Check if navigation to a location is safe (simulated)\"\"\"\n        # In a real system, this would check map data and obstacles\n        # For this example, assume navigation is safe\n        return True\n\n    def execute_action_plan(self, plan):\n        \"\"\"Execute an action plan\"\"\"\n        action = plan['action']\n        parameters = plan.get('parameters', {})\n        \n        self.get_logger().info(f'Executing action: {action} with parameters: {parameters}')\n        \n        if action == 'navigate_to':\n            self.execute_navigation(parameters.get('location'))\n        elif action == 'pick_up_object':\n            self.execute_pickup(parameters.get('object_type'))\n        elif action == 'place_object':\n            self.execute_placement(parameters.get('location'))\n        elif action == 'detect_object':\n            self.execute_detection(parameters.get('object_type'))\n        elif action == 'return_to_charging_station':\n            self.execute_return_to_charger()\n        else:\n            self.get_logger().warn(f'Unknown action: {action}')\n\n    def execute_navigation(self, location):\n        \"\"\"Execute navigation to a specific location\"\"\"\n        # Get coordinates for the location\n        target_x, target_y = self.get_coordinates_for_location(location)\n        \n        if target_x is not None and target_y is not None:\n            self.get_logger().info(f'Navigating to {location} at ({target_x}, {target_y})')\n            \n            # In a real system, this would plan and execute a path\n            # For this example, we'll simulate the movement\n            self.simulate_navigation_to(target_x, target_y)\n        else:\n            self.get_logger().warn(f'Unknown location: {location}')\n\n    def get_coordinates_for_location(self, location):\n        \"\"\"Convert location name to coordinates\"\"\"\n        # In a real system, this would use a map or semantic localization\n        location_map = {\n            'kitchen': (5.0, 3.0),\n            'living_room': (2.0, 8.0),\n            'office': (8.0, 2.0),\n            'bedroom': (7.0, 7.0),\n            'charging_station': (0.0, 0.0)\n        }\n        \n        return location_map.get(location, (None, None))\n\n    def simulate_navigation_to(self, target_x, target_y):\n        \"\"\"Simulate navigation to target coordinates\"\"\"\n        # This is a simplified simulation\n        # In a real system, this would use the navigation stack\n        \n        # Calculate distance to target\n        current_x, current_y = self.robot_state['position']['x'], self.robot_state['position']['y']\n        distance = np.sqrt((target_x - current_x)**2 + (target_y - current_y)**2)\n        \n        # Publish a simple movement command\n        cmd = Twist()\n        cmd.linear.x = 0.5  # Move forward at 0.5 m/s\n        cmd.angular.z = 0.0  # No rotation for simplicity\n        \n        # Simulate movement\n        duration = distance / 0.5  # At 0.5 m/s\n        start_time = time.time()\n        \n        while time.time() - start_time < duration and rclpy.ok():\n            self.cmd_vel_pub.publish(cmd)\n            time.sleep(0.1)  # Small delay\n        \n        # Stop robot\n        cmd.linear.x = 0.0\n        self.cmd_vel_pub.publish(cmd)\n        \n        # Update robot position\n        self.robot_state['position']['x'] = target_x\n        self.robot_state['position']['y'] = target_y\n        \n        self.get_logger().info(f'Navigation to ({target_x}, {target_y}) completed')\n\n    def execute_pickup(self, object_type):\n        \"\"\"Execute object pickup\"\"\"\n        self.get_logger().info(f'Attempting to pick up {object_type}')\n        \n        # In a real system, this would:\n        # 1. Find the object in the environment\n        # 2. Plan approach trajectory\n        # 3. Execute the grasp\n        # For this example, we'll simulate the process\n        \n        # Find the requested object type in detected objects\n        target_object = None\n        for obj in self.detected_objects:\n            if obj['name'] == object_type:\n                target_object = obj\n                break\n        \n        if target_object:\n            # Simulate approach and grasp\n            self.simulate_grasp_object(target_object)\n        else:\n            self.get_logger().warn(f'Object of type {object_type} not detected')\n\n    def simulate_grasp_object(self, obj):\n        \"\"\"Simulate grasping an object\"\"\"\n        # In a real system, this would control the manipulator\n        # For this example, we'll update the robot state\n        \n        self.robot_state['attached_object'] = obj['name']\n        self.robot_state['is_grasping'] = True\n        \n        # Create joint command to close gripper\n        cmd = JointState()\n        cmd.name = ['gripper_joint']\n        cmd.position = [0.0]  # Closed position\n        cmd.velocity = [0.1]  # Closing velocity\n        cmd.effort = [50.0]   # Maximum effort\n        \n        self.joint_cmd_pub.publish(cmd)\n        \n        self.get_logger().info(f'Object {obj[\"name\"]} grasped successfully')\n\n    def execute_placement(self, location):\n        \"\"\"Execute object placement\"\"\"\n        if self.robot_state['attached_object'] is None:\n            self.get_logger().warn('No object to place')\n            return\n        \n        self.get_logger().info(f'Placing object at {location}')\n        \n        # In a real system, this would:\n        # 1. Navigate to the placement location\n        # 2. Execute the placement maneuver\n        # For this example, we'll simulate the process\n        \n        # Simulate placing the object\n        self.simulate_place_object(location)\n        \n        # Update robot state\n        self.robot_state['attached_object'] = None\n        self.robot_state['is_grasping'] = False\n\n    def simulate_place_object(self, location):\n        \"\"\"Simulate placing an object at a location\"\"\"\n        # In a real system, this would control the manipulator\n        # For this example, we'll just open the gripper\n        \n        # Create joint command to open gripper\n        cmd = JointState()\n        cmd.name = ['gripper_joint']\n        cmd.position = [0.8]  # Open position\n        cmd.velocity = [0.1]  # Opening velocity\n        cmd.effort = [50.0]   # Maximum effort\n        \n        self.joint_cmd_pub.publish(cmd)\n        \n        self.get_logger().info(f'Object placed at {location}')\n\n    def execute_detection(self, object_type):\n        \"\"\"Execute object detection\"\"\"\n        self.get_logger().info(f'Detecting objects of type: {object_type}')\n        \n        # In a real system, this would process camera images\n        # For this example, we'll return the detected objects\n        \n        if object_type == 'any' or object_type == 'all':\n            relevant_objects = self.detected_objects\n        else:\n            relevant_objects = [obj for obj in self.detected_objects if obj['name'] == object_type]\n        \n        # Publish detection results\n        result_msg = String()\n        result_msg.data = json.dumps({\n            'command': 'detect_object',\n            'object_type': object_type,\n            'objects_found': relevant_objects,\n            'timestamp': self.get_clock().now().nanoseconds\n        })\n        \n        self.object_detection_pub.publish(result_msg)\n\n    def execute_return_to_charger(self):\n        \"\"\"Execute return to charging station\"\"\"\n        self.get_logger().info('Returning to charging station')\n        self.execute_navigation('charging_station')\n\n    def map_callback(self, msg):\n        \"\"\"Update internal map representation\"\"\"\n        self.current_map = {\n            'data': np.array(msg.data).reshape(msg.info.height, msg.info.width),\n            'resolution': msg.info.resolution,\n            'origin': msg.origin\n        }\n\n    def scan_callback(self, msg):\n        \"\"\"Process laser scan for local obstacle detection\"\"\"\n        # Convert laser scan to obstacle positions\n        angles = np.linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        valid_ranges = np.array(msg.ranges)\n        valid_ranges[valid_ranges > msg.range_max] = np.inf\n        \n        # Calculate obstacle positions in robot frame\n        x = valid_ranges * np.cos(angles)\n        y = valid_ranges * np.sin(angles)\n        \n        # Filter out infinite ranges\n        finite_mask = np.isfinite(valid_ranges)\n        self.local_obstacles = np.column_stack((x[finite_mask], y[finite_mask]))\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera image\"\"\"\n        try:\n            # Convert ROS Image message to OpenCV image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n            \n            # Perform object detection\n            detected_objects = self.detect_objects_in_image(cv_image)\n            \n            # Update internal state\n            self.detected_objects = detected_objects\n            \n            # Publish detected objects\n            detection_msg = String()\n            detection_msg.data = json.dumps(detected_objects)\n            self.object_detection_pub.publish(detection_msg)\n            \n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def detect_objects_in_image(self, image):\n        \"\"\"Detect objects in an image (simulated detection)\"\"\"\n        # This is a simplified object detection simulation\n        # In a real system, this would use a trained model\n        \n        # For this example, we'll return some predefined objects\n        # based on color detection (simplified)\n        object_colors = {\n            'red_cup': ([0, 50, 50], [10, 255, 255]),\n            'blue_bottle': ([100, 50, 50], [130, 255, 255]),\n            'green_box': ([50, 50, 50], [70, 255, 255])\n        }\n        \n        objects = []\n        \n        for obj_name, (lower_color, upper_color) in object_colors.items():\n            # Create mask for the color range\n            lower = np.array(lower_color)\n            upper = np.array(upper_color)\n            mask = cv2.inRange(image, lower, upper)\n            \n            # Find contours in the mask\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            \n            for contour in contours:\n                # Filter by size to avoid noise\n                area = cv2.contourArea(contour)\n                if area > 500:  # Minimum area threshold\n                    # Calculate bounding box\n                    x, y, w, h = cv2.boundingRect(contour)\n                    \n                    # Calculate center position\n                    center_x = x + w // 2\n                    center_y = y + h // 2\n                    \n                    # Calculate image coordinates (0-1 normalized)\n                    img_width, img_height = image.shape[1], image.shape[0]\n                    norm_x = center_x / img_width\n                    norm_y = center_y / img_height\n                    \n                    # Add detected object\n                    objects.append({\n                        'name': obj_name,\n                        'center': {'x': center_x, 'y': center_y},\n                        'normalized_center': {'x': norm_x, 'y': norm_y},\n                        'bbox': {'x': x, 'y': y, 'width': w, 'height': h},\n                        'area': area\n                    })\n        \n        return objects\n\n    def joint_state_callback(self, msg):\n        \"\"\"Update current joint positions\"\"\"\n        for i, name in enumerate(msg.name):\n            if i < len(msg.position):\n                self.joint_positions[name] = msg.position[i]\n\n    def monitor_system_status(self):\n        \"\"\"Monitor and publish system status\"\"\"\n        while True:\n            # Create system status report\n            status_report = {\n                'timestamp': time.time(),\n                'robot_state': self.robot_state,\n                'detected_objects_count': len(self.detected_objects),\n                'local_obstacles_count': len(self.local_obstacles),\n                'components_status': {\n                    'voice_processing': 'active',\n                    'navigation': 'ready' if self.current_map else 'unavailable',\n                    'perception': 'active',\n                    'manipulation': 'ready' if self.joint_positions else 'unavailable'\n                }\n            }\n            \n            # Publish status\n            status_msg = String()\n            status_msg.data = json.dumps(status_report)\n            self.system_status_pub.publish(status_msg)\n            \n            # Sleep for 1 second\n            time.sleep(1.0)\n\ndef main(args=None):\n    \"\"\"\n    Main function to run the integrated capstone system\n    \"\"\"\n    rclpy.init(args=args)\n    \n    # Create the integrated system node\n    capstone_system = CapstoneIntegratedSystem()\n    \n    try:\n        # In a real system, we would spin the node\n        # For this theoretical example, we'll just keep it running\n        print(\"Capstone Integrated System running...\")\n        print(\"This is a theoretical example without actual runtime execution\")\n        print(\"In a real implementation, this would process commands and control a robot\")\n        \n        # Keep the system running\n        while True:\n            time.sleep(1.0)\n            \n    except KeyboardInterrupt:\n        print(\"\\nShutting down capstone system...\")\n    finally:\n        capstone_system.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"component-integration-examples",children:"Component Integration Examples"}),"\n",(0,o.jsx)(n.h3,{id:"1-voice-to-action-pipeline-integration",children:"1. Voice-to-Action Pipeline Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"\"\"\"\nVoice-to-Action Pipeline Integration Example\nDemonstrates how voice commands flow through the system to generate actions\n\"\"\"\n\nclass VoiceToActionPipeline:\n    \"\"\"\n    Class that demonstrates the voice-to-action pipeline integration\n    \"\"\"\n    def __init__(self, system_node):\n        self.system = system_node\n        self.command_history = []\n        \n    def process_voice_command(self, command_text):\n        \"\"\"\n        Process a voice command through the complete pipeline\n        \"\"\"\n        print(f\"Processing voice command: {command_text}\")\n        \n        # Step 1: Speech-to-text (handled externally in this example)\n        # In a real system, this would be done by Whisper\n        \n        # Step 2: Natural language understanding\n        structured_command = self.understand_command(command_text)\n        \n        # Step 3: Context-aware planning\n        action_plan = self.generate_action_plan(structured_command)\n        \n        # Step 4: Safety validation\n        if self.validate_plan_safety(action_plan):\n            # Step 5: Execute plan\n            self.execute_plan(action_plan)\n        else:\n            print(\"Action plan failed safety validation\")\n            \n        # Step 6: Update command history\n        self.command_history.append({\n            'command': command_text,\n            'plan': action_plan,\n            'timestamp': time.time()\n        })\n        \n    def understand_command(self, command_text):\n        \"\"\"\n        Understand the natural language command\n        \"\"\"\n        # This would typically use an LLM in a real implementation\n        # For this example, we'll do simple keyword matching\n        command_lower = command_text.lower()\n        \n        if 'navigate to' in command_lower or 'go to' in command_lower:\n            # Extract location\n            for location in ['kitchen', 'living room', 'office', 'bedroom', 'charging station']:\n                if location in command_lower:\n                    return {\n                        'action': 'navigate_to',\n                        'parameters': {'location': location.replace(' ', '_')}\n                    }\n        \n        elif 'pick up' in command_lower or 'grasp' in command_lower:\n            # Extract object type\n            for obj_type in ['cup', 'bottle', 'box', 'object']:\n                if obj_type in command_lower:\n                    return {\n                        'action': 'pick_up_object',\n                        'parameters': {'object_type': obj_type}\n                    }\n        \n        elif 'place' in command_lower or 'put' in command_lower:\n            # Extract location\n            for location in ['kitchen', 'living room', 'office', 'table', 'counter']:\n                if location in command_lower:\n                    return {\n                        'action': 'place_object',\n                        'parameters': {'location': location.replace(' ', '_')}\n                    }\n        \n        # Default: unrecognized command\n        return {\n            'action': 'unknown',\n            'parameters': {'original_command': command_text}\n        }\n    \n    def generate_action_plan(self, structured_command):\n        \"\"\"\n        Generate a detailed action plan based on the understood command\n        \"\"\"\n        action = structured_command['action']\n        params = structured_command['parameters']\n        \n        # In a real system, this would use an LLM to generate a detailed plan\n        # For this example, we'll create a simple plan\n        \n        if action == 'navigate_to':\n            location = params['location']\n            return {\n                'action_sequence': [\n                    {'action': 'plan_path_to', 'parameters': {'destination': location}},\n                    {'action': 'execute_navigation', 'parameters': {'destination': location}},\n                    {'action': 'confirm_arrival', 'parameters': {'destination': location}}\n                ],\n                'required_components': ['navigation', 'localization'],\n                'estimated_duration': 60  # seconds\n            }\n        \n        elif action == 'pick_up_object':\n            obj_type = params['object_type']\n            return {\n                'action_sequence': [\n                    {'action': 'detect_object', 'parameters': {'object_type': obj_type}},\n                    {'action': 'approach_object', 'parameters': {'object_type': obj_type}},\n                    {'action': 'grasp_object', 'parameters': {'object_type': obj_type}}\n                ],\n                'required_components': ['perception', 'manipulation'],\n                'estimated_duration': 30  # seconds\n            }\n        \n        elif action == 'place_object':\n            location = params['location']\n            return {\n                'action_sequence': [\n                    {'action': 'navigate_to', 'parameters': {'location': location}},\n                    {'action': 'place_held_object', 'parameters': {'location': location}}\n                ],\n                'required_components': ['navigation', 'manipulation'],\n                'estimated_duration': 45  # seconds\n            }\n        \n        else:\n            return {\n                'action_sequence': [],\n                'required_components': [],\n                'estimated_duration': 0\n            }\n    \n    def validate_plan_safety(self, plan):\n        \"\"\"\n        Validate that the action plan is safe to execute\n        \"\"\"\n        # Check if required components are available\n        for component in plan['required_components']:\n            if not self.system.is_component_available(component):\n                print(f\"Required component {component} is not available\")\n                return False\n        \n        # Check robot state constraints\n        if plan['action_sequence']:\n            first_action = plan['action_sequence'][0]\n            if first_action['action'] == 'grasp_object' and self.system.robot_state['attached_object']:\n                print(\"Robot already holding an object, cannot grasp another\")\n                return False\n        \n        # In a real system, check for environmental safety\n        # using sensor data\n        \n        return True\n    \n    def execute_plan(self, plan):\n        \"\"\"\n        Execute the action plan\n        \"\"\"\n        print(f\"Executing plan with {len(plan['action_sequence'])} steps\")\n        \n        for i, step in enumerate(plan['action_sequence']):\n            action = step['action']\n            params = step['parameters']\n            \n            print(f\"Step {i+1}/{len(plan['action_sequence'])}: {action} with {params}\")\n            \n            # Execute the action\n            self.execute_single_action(action, params)\n            \n            # Small delay between steps (in a real system, this might be event-driven)\n            time.sleep(0.5)\n    \n    def execute_single_action(self, action, params):\n        \"\"\"\n        Execute a single action\n        \"\"\"\n        # In a real system, this would interface with the appropriate component\n        # For this example, we'll just log the action\n        \n        if action == 'navigate_to':\n            print(f\"  -> Navigating to {params['location']}\")\n        elif action == 'pick_up_object':\n            print(f\"  -> Picking up {params['object_type']}\")\n        elif action == 'place_object':\n            print(f\"  -> Placing object at {params['location']}\")\n        elif action == 'detect_object':\n            print(f\"  -> Detecting {params['object_type']}\")\n        else:\n            print(f\"  -> Executing {action} with {params}\")\n"})}),"\n",(0,o.jsx)(n.h3,{id:"2-perception-navigation-integration",children:"2. Perception-Navigation Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'"""\nPerception-Navigation Integration Example\nDemonstrates how perception data informs navigation decisions\n"""\n\nclass PerceptionNavigationIntegration:\n    """\n    Class that demonstrates integration between perception and navigation\n    """\n    def __init__(self, system_node):\n        self.system = system_node\n        self.known_obstacles = {}\n        self.dynamic_objects = []\n        \n    def update_navigation_with_perception(self):\n        """\n        Update navigation plan based on perception data\n        """\n        # Process detected objects to identify potential obstacles\n        for obj in self.system.detected_objects:\n            if self.is_obstacle(obj):\n                self.add_obstacle_to_map(obj)\n        \n        # Process detected dynamic objects (people, moving objects)\n        for obj in self.system.detected_objects:\n            if self.is_dynamic_object(obj):\n                self.track_dynamic_object(obj)\n        \n        # If navigation is active, update the plan based on new information\n        if self.system.robot_state[\'is_moving\']:\n            self.update_active_navigation()\n    \n    def is_obstacle(self, obj):\n        """\n        Determine if an object is an obstacle for navigation\n        """\n        # In a real system, this would use object classification\n        # For this example, we\'ll consider large objects as potential obstacles\n        return obj[\'area\'] > 1000  # Threshold area\n    \n    def is_dynamic_object(self, obj):\n        """\n        Determine if an object is dynamic (moving)\n        """\n        # In a real system, this would use tracking algorithms\n        # For this example, we\'ll consider people as dynamic objects\n        return \'person\' in obj[\'name\'].lower()\n    \n    def add_obstacle_to_map(self, obj):\n        """\n        Add an obstacle to the navigation map\n        """\n        # In a real system, this would update the costmap\n        # For this example, we\'ll just store it\n        obj_id = f"obstacle_{len(self.known_obstacles)}"\n        self.known_obstacles[obj_id] = {\n            \'position\': obj[\'center\'],\n            \'size\': obj[\'bbox\'],\n            \'timestamp\': time.time()\n        }\n        \n        print(f"Added obstacle {obj_id} at {obj[\'center\']}")\n    \n    def track_dynamic_object(self, obj):\n        """\n        Track a dynamic object for navigation safety\n        """\n        # In a real system, this would use tracking algorithms\n        # For this example, we\'ll just store the object\n        self.dynamic_objects.append({\n            \'name\': obj[\'name\'],\n            \'position\': obj[\'center\'],\n            \'timestamp\': time.time()\n        })\n        \n        print(f"Tracking dynamic object: {obj[\'name\']} at {obj[\'center\']}")\n    \n    def update_active_navigation(self):\n        """\n        Update an active navigation task with new perception data\n        """\n        # Check if any detected objects are in the navigation path\n        for obj in self.system.detected_objects:\n            if self.is_object_in_navigation_path(obj):\n                # Adjust navigation plan\n                self.adjust_navigation_for_object(obj)\n    \n    def is_object_in_navigation_path(self, obj):\n        """\n        Check if an object is in the current navigation path\n        """\n        # In a real system, this would check against the planned path\n        # For this example, we\'ll use a simplified check\n        robot_pos = self.system.robot_state[\'position\']\n        obj_pos = obj[\'center\']\n        \n        # Calculate distance (simplified)\n        distance = np.sqrt((robot_pos[\'x\'] - obj_pos[\'x\'])**2 + \n                          (robot_pos[\'y\'] - obj_pos[\'y\'])**2)\n        \n        # Consider object in path if it\'s close to the robot\n        return distance < 2.0  # 2 meters threshold\n    \n    def adjust_navigation_for_object(self, obj):\n        """\n        Adjust navigation plan to account for an object\n        """\n        print(f"Adjusting navigation for object: {obj[\'name\']}")\n        \n        # In a real system, this would replan the path\n        # For this example, we\'ll just slow down or stop\n        if obj[\'area\'] > 5000:  # Large object\n            print("  Large object detected, stopping navigation")\n            # Stop the robot\n            cmd = Twist()\n            cmd.linear.x = 0.0\n            cmd.angular.z = 0.0\n            self.system.cmd_vel_pub.publish(cmd)\n        else:\n            print("  Small object detected, slowing down")\n            # Slow down the robot\n            cmd = Twist()\n            cmd.linear.x = 0.1  # Reduced speed\n            cmd.angular.z = 0.0\n            self.system.cmd_vel_pub.publish(cmd)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"3-manipulation-perception-integration",children:"3. Manipulation-Perception Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'"""\nManipulation-Perception Integration Example\nDemonstrates how perception data guides manipulation actions\n"""\n\nclass ManipulationPerceptionIntegration:\n    """\n    Class that demonstrates integration between manipulation and perception\n    """\n    def __init__(self, system_node):\n        self.system = system_node\n        self.grasp_planner = GraspPlanner()\n        self.approach_planner = ApproachPlanner()\n        \n    def plan_manipulation_with_perception(self, obj_name):\n        """\n        Plan manipulation actions based on perception data\n        """\n        # Find the target object in detected objects\n        target_obj = None\n        for obj in self.system.detected_objects:\n            if obj[\'name\'] == obj_name:\n                target_obj = obj\n                break\n        \n        if not target_obj:\n            print(f"Object {obj_name} not detected")\n            return None\n        \n        # Plan the grasp based on object properties\n        grasp_plan = self.grasp_planner.plan_grasp_for_object(target_obj)\n        \n        # Plan the approach to the object\n        approach_plan = self.approach_planner.plan_approach_to_object(target_obj)\n        \n        # Combine into a complete manipulation plan\n        manipulation_plan = {\n            \'object\': target_obj,\n            \'approach\': approach_plan,\n            \'grasp\': grasp_plan,\n            \'lift_height\': 0.1  # Lift 10cm after grasp\n        }\n        \n        return manipulation_plan\n    \n    def execute_manipulation_plan(self, plan):\n        """\n        Execute a manipulation plan\n        """\n        if not plan:\n            print("No plan to execute")\n            return False\n        \n        obj = plan[\'object\']\n        approach = plan[\'approach\']\n        grasp = plan[\'grasp\']\n        \n        print(f"Executing manipulation for {obj[\'name\']}")\n        \n        # Execute approach\n        print("Executing approach...")\n        self.execute_approach(approach)\n        \n        # Execute grasp\n        print("Executing grasp...")\n        success = self.execute_grasp(grasp)\n        \n        if success:\n            # Lift the object\n            print("Lifting object...")\n            self.lift_object(plan[\'lift_height\'])\n            \n            # Update robot state\n            self.system.robot_state[\'attached_object\'] = obj[\'name\']\n            \n            print(f"Successfully manipulated {obj[\'name\']}")\n            return True\n        else:\n            print(f"Failed to manipulate {obj[\'name\']}")\n            return False\n    \n    def execute_approach(self, approach_plan):\n        """\n        Execute approach to an object\n        """\n        # In a real system, this would control the manipulator\n        # For this example, we\'ll just simulate\n        print(f"  Approaching with trajectory: {approach_plan[\'trajectory\']}")\n        time.sleep(2.0)  # Simulate execution time\n    \n    def execute_grasp(self, grasp_plan):\n        """\n        Execute grasp of an object\n        """\n        # In a real system, this would control the gripper\n        # For this example, we\'ll simulate with some chance of success\n        import random\n        success = random.random() > 0.2  # 80% success rate in simulation\n        \n        print(f"  Grasping with method: {grasp_plan[\'method\']}, success: {success}")\n        time.sleep(1.0)  # Simulate execution time\n        \n        return success\n    \n    def lift_object(self, height):\n        """\n        Lift the currently grasped object\n        """\n        # In a real system, this would move the manipulator up\n        # For this example, we\'ll just simulate\n        print(f"  Lifting object by {height} meters")\n        time.sleep(1.0)  # Simulate execution time\n\nclass GraspPlanner:\n    """\n    Simple grasp planner based on object properties\n    """\n    def plan_grasp_for_object(self, obj):\n        """\n        Plan a grasp for a detected object\n        """\n        # Determine grasp type based on object properties\n        if \'cup\' in obj[\'name\']:\n            # Top grasp for cup\n            grasp_type = \'top_grasp\'\n            approach_vector = [0, 0, -1]  # Approach from above\n        elif \'bottle\' in obj[\'name\']:\n            # Side grasp for bottle\n            grasp_type = \'side_grasp\'\n            approach_vector = [1, 0, 0]  # Approach from side\n        elif \'box\' in obj[\'name\']:\n            # Corner grasp for box\n            grasp_type = \'corner_grasp\'\n            approach_vector = [0, 1, 0]  # Approach from front\n        else:\n            # Default grasp\n            grasp_type = \'top_grasp\'\n            approach_vector = [0, 0, -1]\n        \n        return {\n            \'method\': grasp_type,\n            \'approach_vector\': approach_vector,\n            \'gripper_width\': self.estimate_gripper_width(obj),\n            \'grasp_point\': self.calculate_grasp_point(obj)\n        }\n    \n    def estimate_gripper_width(self, obj):\n        """\n        Estimate appropriate gripper width for an object\n        """\n        # Simplified estimation based on bounding box\n        bbox = obj[\'bbox\']\n        min_dim = min(bbox[\'width\'], bbox[\'height\'])\n        return min(0.08, max(0.02, min_dim * 0.6))  # Between 2-8cm, 60% of min dimension\n    \n    def calculate_grasp_point(self, obj):\n        """\n        Calculate the best grasp point on an object\n        """\n        # For this example, use the center of the object\n        return obj[\'center\']\n\nclass ApproachPlanner:\n    """\n    Simple approach planner\n    """\n    def plan_approach_to_object(self, obj):\n        """\n        Plan an approach trajectory to an object\n        """\n        # Calculate approach trajectory\n        # In a real system, this would use inverse kinematics\n        approach_point = self.calculate_approach_point(obj)\n        \n        return {\n            \'trajectory\': [approach_point],  # Simplified trajectory\n            \'safe_distance\': 0.1,  # 10cm from object\n            \'approach_vector\': [0, 0, -1]  # Default from above\n        }\n    \n    def calculate_approach_point(self, obj):\n        """\n        Calculate the approach point for an object\n        """\n        # For this example, approach 10cm above the object center\n        center = obj[\'center\']\n        return [center[\'x\'], center[\'y\'], center.get(\'z\', 0.1) + 0.1]\n'})}),"\n",(0,o.jsx)(n.h2,{id:"system-validation-examples",children:"System Validation Examples"}),"\n",(0,o.jsx)(n.h3,{id:"1-integration-test-example",children:"1. Integration Test Example"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"\"\"\"\nIntegration Test Example\nDemonstrates how to test the integration of all components\n\"\"\"\n\ndef run_integration_tests():\n    \"\"\"\n    Run integration tests for the complete system\n    \"\"\"\n    print(\"Running Capstone System Integration Tests...\")\n    \n    # Test 1: Voice Command Processing\n    print(\"\\n1. Testing Voice Command Processing...\")\n    test_voice_command_processing()\n    \n    # Test 2: Navigation Integration\n    print(\"\\n2. Testing Navigation Integration...\")\n    test_navigation_integration()\n    \n    # Test 3: Perception Integration\n    print(\"\\n3. Testing Perception Integration...\")\n    test_perception_integration()\n    \n    # Test 4: Manipulation Integration\n    print(\"\\n4. Testing Manipulation Integration...\")\n    test_manipulation_integration()\n    \n    # Test 5: Complete End-to-End Flow\n    print(\"\\n5. Testing Complete End-to-End Flow...\")\n    test_end_to_end_flow()\n    \n    print(\"\\nIntegration tests completed!\")\n\ndef test_voice_command_processing():\n    \"\"\"\n    Test the voice command processing pipeline\n    \"\"\"\n    # Create a voice-to-action pipeline instance\n    system_node = CapstoneIntegratedSystem()  # This would be a mock in real testing\n    v2a_pipeline = VoiceToActionPipeline(system_node)\n    \n    # Test various commands\n    test_commands = [\n        \"Go to the kitchen\",\n        \"Pick up the red cup\",\n        \"Place the object on the table\"\n    ]\n    \n    for cmd in test_commands:\n        print(f\"  Testing: {cmd}\")\n        v2a_pipeline.process_voice_command(cmd)\n    \n    print(\"  \u2713 Voice command processing tests passed\")\n\ndef test_navigation_integration():\n    \"\"\"\n    Test navigation integration with other components\n    \"\"\"\n    system_node = CapstoneIntegratedSystem()  # Mock\n    nav_integration = PerceptionNavigationIntegration(system_node)\n    \n    # Simulate detected objects that should affect navigation\n    system_node.detected_objects = [\n        {'name': 'large_box', 'center': {'x': 1.0, 'y': 1.0}, 'area': 2000, 'bbox': {'width': 0.5, 'height': 0.5}},\n        {'name': 'person', 'center': {'x': 2.0, 'y': 0.5}, 'area': 1000, 'bbox': {'width': 0.8, 'height': 1.8}}\n    ]\n    \n    # Update navigation with perception data\n    nav_integration.update_navigation_with_perception()\n    \n    # Verify obstacles were detected\n    assert len(nav_integration.known_obstacles) > 0, \"No obstacles detected\"\n    assert len(nav_integration.dynamic_objects) > 0, \"No dynamic objects tracked\"\n    \n    print(\"  \u2713 Navigation integration tests passed\")\n\ndef test_perception_integration():\n    \"\"\"\n    Test perception system with various inputs\n    \"\"\"\n    system_node = CapstoneIntegratedSystem()  # Mock\n    \n    # Simulate image processing\n    # In a real test, we would use actual image data\n    print(\"  Simulating perception processing...\")\n    \n    # Create mock detected objects\n    system_node.detected_objects = [\n        {'name': 'red_cup', 'center': {'x': 300, 'y': 200}, 'area': 800},\n        {'name': 'blue_bottle', 'center': {'x': 400, 'y': 150}, 'area': 1200}\n    ]\n    \n    print(f\"  Detected {len(system_node.detected_objects)} objects\")\n    print(\"  \u2713 Perception integration tests passed\")\n\ndef test_manipulation_integration():\n    \"\"\"\n    Test manipulation system with perception data\n    \"\"\"\n    system_node = CapstoneIntegratedSystem()  # Mock\n    manip_integration = ManipulationPerceptionIntegration(system_node)\n    \n    # Set up detected objects\n    system_node.detected_objects = [\n        {'name': 'red_cup', 'center': {'x': 300, 'y': 200}, 'area': 800, \n         'bbox': {'x': 280, 'y': 180, 'width': 40, 'height': 40}},\n        {'name': 'blue_bottle', 'center': {'x': 400, 'y': 150}, 'area': 1200,\n         'bbox': {'x': 380, 'y': 120, 'width': 40, 'height': 60}}\n    ]\n    \n    # Plan manipulation for the red cup\n    plan = manip_integration.plan_manipulation_with_perception('red_cup')\n    assert plan is not None, \"Manipulation plan not generated\"\n    \n    # Execute the plan (simulation)\n    success = manip_integration.execute_manipulation_plan(plan)\n    assert success, \"Manipulation execution failed\"\n    \n    print(\"  \u2713 Manipulation integration tests passed\")\n\ndef test_end_to_end_flow():\n    \"\"\"\n    Test the complete end-to-end flow\n    \"\"\"\n    # This would test the complete system flow:\n    # 1. Voice command input\n    # 2. Command interpretation\n    # 3. Action planning\n    # 4. Component coordination\n    # 5. Execution and feedback\n    \n    print(\"  Simulating complete end-to-end flow...\")\n    \n    # In a real test, we would simulate the entire flow\n    # For this example, we'll just verify the concept\n    print(\"  1. Voice command received and interpreted\")\n    print(\"  2. Action plan generated\")\n    print(\"  3. Components coordinated\")\n    print(\"  4. Actions executed\")\n    print(\"  5. Feedback processed\")\n    \n    print(\"  \u2713 End-to-end flow test concept validated\")\n"})}),"\n",(0,o.jsx)(n.h2,{id:"best-practices-for-integration",children:"Best Practices for Integration"}),"\n",(0,o.jsx)(n.h3,{id:"1-error-handling-and-fallbacks",children:"1. Error Handling and Fallbacks"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'"""\nBest Practices: Error Handling and Fallbacks\n"""\n\nclass RobustIntegrationSystem:\n    """\n    Demonstrates robust integration with error handling\n    """\n    def __init__(self):\n        self.fallback_strategies = {\n            \'navigation_failure\': self.fallback_navigation,\n            \'perception_failure\': self.fallback_perception,\n            \'manipulation_failure\': self.fallback_manipulation\n        }\n        \n        self.error_recovery_steps = []\n        \n    def handle_component_error(self, component_name, error):\n        """\n        Handle errors in components with appropriate fallbacks\n        """\n        print(f"Error in {component_name}: {error}")\n        \n        # Log the error\n        self.log_error(component_name, error)\n        \n        # Apply fallback strategy if available\n        if component_name in self.fallback_strategies:\n            fallback_method = self.fallback_strategies[component_name]\n            fallback_method()\n        else:\n            # Default fallback: stop and request human intervention\n            self.emergency_stop()\n            self.request_human_intervention()\n    \n    def log_error(self, component, error):\n        """\n        Log errors for debugging and monitoring\n        """\n        error_entry = {\n            \'timestamp\': time.time(),\n            \'component\': component,\n            \'error\': str(error),\n            \'context\': self.get_system_context()\n        }\n        \n        self.error_recovery_steps.append(error_entry)\n        print(f"Error logged: {error_entry}")\n    \n    def get_system_context(self):\n        """\n        Get current system context for error logging\n        """\n        # In a real system, this would capture relevant state\n        return {\n            \'robot_position\': {\'x\': 0, \'y\': 0, \'theta\': 0},  # Placeholder\n            \'active_tasks\': [],\n            \'component_status\': {}\n        }\n    \n    def fallback_navigation(self):\n        """\n        Fallback strategy for navigation failures\n        """\n        print("Navigation fallback: Using simpler path planning or stopping")\n        # Stop current navigation\n        # Try alternative path planning\n        # Or return to safe position\n    \n    def fallback_perception(self):\n        """\n        Fallback strategy for perception failures\n        """\n        print("Perception fallback: Using alternative sensors or default assumptions")\n        # Try alternative perception methods\n        # Use pre-mapped information\n        # Request human guidance\n    \n    def fallback_manipulation(self):\n        """\n        Fallback strategy for manipulation failures\n        """\n        print("Manipulation fallback: Using simpler grasp or aborting task")\n        # Open gripper to release potential jams\n        # Try alternative grasp\n        # Abort task and report failure\n    \n    def emergency_stop(self):\n        """\n        Emergency stop for safety\n        """\n        print("Emergency stop activated")\n        # Send stop commands to all actuators\n        # Set all components to safe state\n    \n    def request_human_intervention(self):\n        """\n        Request human intervention when automated recovery fails\n        """\n        print("Requesting human intervention")\n        # Signal for human operator\n        # Provide context about the failure\n        # Wait for human decision\n'})}),"\n",(0,o.jsx)(n.p,{children:"This comprehensive set of code examples demonstrates how all components of the capstone project integrate together. The examples show theoretical implementations without requiring runtime execution, focusing on the architecture, data flow, and coordination between components. Each example illustrates best practices for integration, error handling, and system validation."})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},8453(e,n,t){t.d(n,{R:()=>s,x:()=>r});var a=t(6540);const o={},i=a.createContext(o);function s(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);