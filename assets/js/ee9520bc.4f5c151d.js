"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[1748],{8441(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>i,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"capstone-project/implementation-guide","title":"Capstone Implementation Guide","description":"Step-by-step guide for implementing the capstone project","source":"@site/docs/capstone-project/implementation-guide.md","sourceDirName":"capstone-project","slug":"/capstone-project/implementation-guide","permalink":"/physical-ai-humanoid-robotics-book/docs/capstone-project/implementation-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/your-organization/physical-ai-humanoid-robotics-book/edit/main/book/docs/capstone-project/implementation-guide.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Capstone Implementation Guide","description":"Step-by-step guide for implementing the capstone project","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Capstone Architecture Design","permalink":"/physical-ai-humanoid-robotics-book/docs/capstone-project/capstone-architecture"},"next":{"title":"Capstone Code Examples","permalink":"/physical-ai-humanoid-robotics-book/docs/capstone-project/integration-examples"}}');var s=t(4848),a=t(8453);const i={title:"Capstone Implementation Guide",description:"Step-by-step guide for implementing the capstone project",sidebar_position:3},r="Capstone Implementation Guide",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Phase 1: Project Setup and Environment Configuration",id:"phase-1-project-setup-and-environment-configuration",level:2},{value:"Step 1: Create the ROS 2 Workspace",id:"step-1-create-the-ros-2-workspace",level:3},{value:"Step 2: Install Dependencies",id:"step-2-install-dependencies",level:3},{value:"Step 3: Set Up OpenAI API",id:"step-3-set-up-openai-api",level:3},{value:"Phase 2: Core System Components",id:"phase-2-core-system-components",level:2},{value:"Step 4: Implement Voice Command Processor",id:"step-4-implement-voice-command-processor",level:3},{value:"Step 5: Implement Navigation Planner",id:"step-5-implement-navigation-planner",level:3},{value:"Step 6: Implement Perception System",id:"step-6-implement-perception-system",level:3},{value:"Step 7: Implement Manipulation Controller",id:"step-7-implement-manipulation-controller",level:3},{value:"Step 8: Create Main Orchestration Node",id:"step-8-create-main-orchestration-node",level:3},{value:"Phase 3: Simulation Environment",id:"phase-3-simulation-environment",level:2},{value:"Step 9: Create Gazebo World File",id:"step-9-create-gazebo-world-file",level:3},{value:"Step 10: Create Launch File",id:"step-10-create-launch-file",level:3},{value:"Phase 4: Testing and Validation",id:"phase-4-testing-and-validation",level:2},{value:"Step 11: Create Unit Tests",id:"step-11-create-unit-tests",level:3},{value:"Step 12: Create Integration Tests",id:"step-12-create-integration-tests",level:3},{value:"Phase 5: Documentation and Finalization",id:"phase-5-documentation-and-finalization",level:2},{value:"Step 13: Update Setup Scripts",id:"step-13-update-setup-scripts",level:3},{value:"Step 14: Create README",id:"step-14-create-readme",level:3},{value:"Usage",id:"usage",level:2},{value:"Testing",id:"testing",level:2},{value:"Simulation",id:"simulation",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Phase 7: Final System Integration",id:"phase-7-final-system-integration",level:2},{value:"Step 16: Create Final Launch Script",id:"step-16-create-final-launch-script",level:3},{value:"Summary",id:"summary",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"capstone-implementation-guide",children:"Capstone Implementation Guide"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This guide provides a detailed, step-by-step approach to implementing the Autonomous Humanoid Capstone Project. Following this guide will help you build and integrate all components of the system systematically."}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before starting the implementation, ensure you have:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Development Environment:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Ubuntu 22.04 LTS"}),"\n",(0,s.jsx)(n.li,{children:"ROS 2 Humble Hawksbill"}),"\n",(0,s.jsx)(n.li,{children:"Python 3.10+"}),"\n",(0,s.jsx)(n.li,{children:"Git version control"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Required Software:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Gazebo simulation environment"}),"\n",(0,s.jsx)(n.li,{children:"OpenAI Whisper for speech recognition"}),"\n",(0,s.jsx)(n.li,{children:"Docusaurus for documentation"}),"\n",(0,s.jsx)(n.li,{children:"NVIDIA Isaac SDK (if available)"}),"\n",(0,s.jsx)(n.li,{children:"Standard ROS 2 packages for navigation and manipulation"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Hardware (for deployment):"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Robotic platform with mobile base"}),"\n",(0,s.jsx)(n.li,{children:"Robotic arm with end-effector"}),"\n",(0,s.jsx)(n.li,{children:"RGB-D camera"}),"\n",(0,s.jsx)(n.li,{children:"LIDAR sensor"}),"\n",(0,s.jsx)(n.li,{children:"Microphone array"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"phase-1-project-setup-and-environment-configuration",children:"Phase 1: Project Setup and Environment Configuration"}),"\n",(0,s.jsx)(n.h3,{id:"step-1-create-the-ros-2-workspace",children:"Step 1: Create the ROS 2 Workspace"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create workspace directory\nmkdir -p ~/capstone_ws/src\ncd ~/capstone_ws\n\n# Create the package for the capstone project\ncd src\nros2 pkg create --build-type ament_python capstone_robot --dependencies rclpy std_msgs geometry_msgs sensor_msgs nav_msgs\n\n# Navigate back to workspace root\ncd ~/capstone_ws\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-install-dependencies",children:"Step 2: Install Dependencies"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Install Python dependencies\npip3 install openai whisperSpeech numpy scipy opencv-python cv-bridge\n\n# Install ROS 2 navigation stack\nsudo apt update\nsudo apt install ros-humble-navigation2 ros-humble-nav2-bringup\n\n# Install ROS 2 manipulation packages\nsudo apt install ros-humble-moveit ros-humble-moveit-ros ros-humble-moveit-ros-planners ros-humble-moveit-ros-warehouse\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-set-up-openai-api",children:"Step 3: Set Up OpenAI API"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create environment file for API keys\necho \"export OPENAI_API_KEY='your-api-key-here'\" >> ~/.bashrc\nsource ~/.bashrc\n"})}),"\n",(0,s.jsx)(n.h2,{id:"phase-2-core-system-components",children:"Phase 2: Core System Components"}),"\n",(0,s.jsx)(n.h3,{id:"step-4-implement-voice-command-processor",children:"Step 4: Implement Voice Command Processor"}),"\n",(0,s.jsx)(n.p,{children:"Create the voice command processing node:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# File: ~/capstone_ws/src/capstone_robot/capstone_robot/voice_processor.py\n\nimport rclpy\nfrom rclpy.node import Node\nimport openai\nimport whisper\nimport json\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import Image, LaserScan\n\nclass VoiceCommandProcessor(Node):\n    def __init__(self):\n        super().__init__(\'voice_command_processor\')\n        \n        # Initialize Whisper model for speech recognition\n        self.whisper_model = whisper.load_model("base")\n        \n        # Initialize OpenAI API for natural language processing\n        # Note: In production, use environment variables\n        openai.api_key = "YOUR_API_KEY_HERE"\n        \n        # Subscriptions\n        self.command_sub = self.create_subscription(\n            String,\n            \'voice_command\',\n            self.command_callback,\n            10\n        )\n        \n        # Publishers\n        self.action_plan_pub = self.create_publisher(String, \'action_plan\', 10)\n        self.speech_pub = self.create_publisher(String, \'robot_speech\', 10)\n        \n        # Robot state tracking\n        self.robot_state = {\n            \'position\': {\'x\': 0, \'y\': 0, \'theta\': 0},\n            \'battery_level\': 100,\n            \'attached_object\': None,\n            \'environment\': \'indoor_office\',\n            \'last_command_time\': 0\n        }\n        \n        self.get_logger().info(\'Voice Command Processor initialized\')\n\n    def command_callback(self, msg):\n        """Process voice command and generate action plan"""\n        command_text = msg.data\n        self.get_logger().info(f\'Received command: {command_text}\')\n        \n        # Process with LLM for command interpretation\n        command_plan = self.interpret_command(command_text)\n        \n        if command_plan and command_plan.get(\'confidence\', 0) > 0.5:\n            # Publish the interpreted command\n            cmd_msg = String()\n            cmd_msg.data = json.dumps(command_plan)\n            self.action_plan_pub.publish(cmd_msg)\n            \n            self.get_logger().info(f\'Command plan generated: {command_plan}\')\n        else:\n            # Request clarification\n            self.request_clarification(command_text)\n\n    def interpret_command(self, command_text):\n        """Interpret natural language command using LLM"""\n        prompt = f"""\n        You are a command interpreter for an autonomous humanoid robot.\n        The robot can perform these actions:\n        - navigate_to(location)\n        - pick_up_object(object_type)\n        - place_object(location)\n        - detect_object(object_type)\n        - follow_person(person_name)\n        - avoid_obstacle(obstacle_type)\n        - return_to_charging_station()\n        \n        Current robot state:\n        {json.dumps(self.robot_state, indent=2)}\n        \n        User command: "{command_text}"\n\n        Respond with a JSON object containing the action plan:\n        {% raw %}\n        {{\n          "action": "action_name",\n          "parameters": {{"param1": "value1", "param2": "value2"}},\n          "confidence": 0.0-1.0\n        }}\n        {% endraw %}\n        \n        If the command is ambiguous, return empty parameters and low confidence.\n        """\n        \n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-3.5-turbo",\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.3,\n                max_tokens=300\n            )\n            \n            content = response.choices[0].message[\'content\'].strip()\n            start_idx = content.find(\'{\')\n            end_idx = content.rfind(\'}\') + 1\n            \n            if start_idx != -1 and end_idx != 0:\n                json_str = content[start_idx:end_idx]\n                plan = json.loads(json_str)\n                return plan\n            else:\n                self.get_logger().error(f\'Could not parse LLM response: {content}\')\n                return None\n                \n        except Exception as e:\n            self.get_logger().error(f\'Error interpreting command: {e}\')\n            return None\n\n    def request_clarification(self, original_command):\n        """Request clarification for ambiguous commands"""\n        clarification_msg = String()\n        clarification_msg.data = f"I didn\'t understand your command: \'{original_command}\'. Could you please rephrase?"\n        self.speech_pub.publish(clarification_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceCommandProcessor()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-5-implement-navigation-planner",children:"Step 5: Implement Navigation Planner"}),"\n",(0,s.jsx)(n.p,{children:"Create the navigation and path planning node:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# File: ~/capstone_ws/src/capstone_robot/capstone_robot/navigation_planner.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom nav_msgs.msg import OccupancyGrid, Path\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom sensor_msgs.msg import LaserScan\nfrom std_msgs.msg import String\nimport numpy as np\nimport json\nfrom scipy.spatial import distance\n\nclass NavigationPlanner(Node):\n    def __init__(self):\n        super().__init__(\'navigation_planner\')\n        \n        # Subscriptions\n        self.map_sub = self.create_subscription(\n            OccupancyGrid,\n            \'map\',\n            self.map_callback,\n            10\n        )\n        \n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            \'scan\',\n            self.scan_callback,\n            10\n        )\n        \n        self.action_plan_sub = self.create_subscription(\n            String,\n            \'action_plan\',\n            self.action_plan_callback,\n            10\n        )\n        \n        # Publishers\n        self.path_pub = self.create_publisher(Path, \'global_plan\', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.goal_pub = self.create_publisher(PoseStamped, \'goal_pose\', 10)\n        \n        # Navigation state\n        self.map_data = None\n        self.current_pose = None\n        self.goal_pose = None\n        self.local_obstacles = []\n        \n        # Navigation parameters\n        self.linear_vel = 0.5  # m/s\n        self.angular_vel = 0.5  # rad/s\n        self.safe_distance = 0.5  # meters\n        \n        self.get_logger().info(\'Navigation Planner initialized\')\n\n    def map_callback(self, msg):\n        """Update internal map representation"""\n        self.map_data = {\n            \'data\': np.array(msg.data).reshape(msg.info.height, msg.info.width),\n            \'resolution\': msg.info.resolution,\n            \'origin\': msg.origin\n        }\n\n    def scan_callback(self, msg):\n        """Process laser scan for local obstacle detection"""\n        # Convert laser scan to obstacle positions\n        angles = np.linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        valid_ranges = np.array(msg.ranges)\n        valid_ranges[valid_ranges > msg.range_max] = np.inf\n        \n        # Calculate obstacle positions in robot frame\n        x = valid_ranges * np.cos(angles)\n        y = valid_ranges * np.sin(angles)\n        \n        # Filter out infinite ranges\n        finite_mask = np.isfinite(valid_ranges)\n        self.local_obstacles = np.column_stack((x[finite_mask], y[finite_mask]))\n\n    def action_plan_callback(self, msg):\n        """Process navigation commands from action plan"""\n        try:\n            command_plan = json.loads(msg.data)\n            action = command_plan[\'action\']\n            \n            if action == \'navigate_to\':\n                location = command_plan[\'parameters\'].get(\'location\')\n                self.navigate_to_location(location)\n            elif action == \'return_to_charging_station\':\n                self.return_to_charging_station()\n                \n        except json.JSONDecodeError as e:\n            self.get_logger().error(f\'Error parsing command: {e}\')\n\n    def navigate_to_location(self, location):\n        """Navigate to a specified location"""\n        # Get coordinates for the location\n        target_x, target_y = self.get_coordinates_for_location(location)\n        \n        if target_x is not None and target_y is not None:\n            self.get_logger().info(f\'Navigating to {location} at ({target_x}, {target_y})\')\n            self.move_to_position(target_x, target_y)\n        else:\n            self.get_logger().warn(f\'Unknown location: {location}\')\n\n    def get_coordinates_for_location(self, location):\n        """Convert location name to coordinates"""\n        # In a real system, this would use a map or semantic localization\n        location_map = {\n            \'kitchen\': (5.0, 3.0),\n            \'living_room\': (2.0, 8.0),\n            \'office\': (8.0, 2.0),\n            \'bedroom\': (7.0, 7.0),\n            \'charging_station\': (0.0, 0.0)\n        }\n        \n        return location_map.get(location, (None, None))\n\n    def move_to_position(self, target_x, target_y):\n        """Move robot to target position with obstacle avoidance"""\n        # Simple proportional controller for demonstration\n        while rclpy.ok():\n            # Get current position (in practice, from localization system)\n            current_x, current_y = self.get_current_position()\n            \n            # Calculate distance and angle to target\n            dx = target_x - current_x\n            dy = target_y - current_y\n            distance_to_target = np.sqrt(dx**2 + dy**2)\n            \n            # Check if we\'re close enough to target\n            if distance_to_target < 0.2:  # 20 cm tolerance\n                self.get_logger().info(\'Reached target position\')\n                self.stop_robot()\n                break\n            \n            # Calculate desired heading\n            desired_theta = np.arctan2(dy, dx)\n            current_theta = self.get_current_orientation()\n            \n            # Calculate heading error\n            heading_error = desired_theta - current_theta\n            # Normalize angle to [-\u03c0, \u03c0]\n            while heading_error > np.pi:\n                heading_error -= 2 * np.pi\n            while heading_error < -np.pi:\n                heading_error += 2 * np.pi\n            \n            # Create velocity command\n            cmd = Twist()\n            \n            # Proportional controller for linear velocity\n            cmd.linear.x = min(self.linear_vel, distance_to_target * 0.5)\n            \n            # Proportional controller for angular velocity\n            cmd.angular.z = min(self.angular_vel, max(-self.angular_vel, heading_error * 1.0))\n            \n            # Check for obstacles before moving\n            if self.has_obstacle_ahead():\n                cmd.linear.x = 0.0  # Stop if obstacle detected\n                self.get_logger().warn(\'Obstacle detected ahead, stopping\')\n            \n            # Publish command\n            self.cmd_vel_pub.publish(cmd)\n            \n            # Small delay\n            self.get_clock().sleep_for(rclpy.duration.Duration(seconds=0.1))\n\n    def has_obstacle_ahead(self):\n        """Check if there\'s an obstacle in front of the robot"""\n        # Check if any local obstacle is within safe distance and in front of robot\n        for obs_x, obs_y in self.local_obstacles:\n            # Only consider obstacles in front of the robot (positive x direction)\n            if obs_x > 0 and abs(obs_y) < 0.5:  # Within 50cm laterally\n                if obs_x < self.safe_distance:  # Within safe distance\n                    return True\n        return False\n\n    def get_current_position(self):\n        """Get current robot position (implement with localization system)"""\n        # Placeholder implementation\n        # In a real system, this would come from AMCL, odometry, or other localization\n        return (0.0, 0.0)\n\n    def get_current_orientation(self):\n        """Get current robot orientation (implement with localization system)"""\n        # Placeholder implementation\n        return 0.0\n\n    def stop_robot(self):\n        """Stop robot movement"""\n        cmd = Twist()\n        self.cmd_vel_pub.publish(cmd)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = NavigationPlanner()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-6-implement-perception-system",children:"Step 6: Implement Perception System"}),"\n",(0,s.jsx)(n.p,{children:"Create the perception and object detection node:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# File: ~/capstone_ws/src/capstone_robot/capstone_robot/perception_system.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport json\n\nclass PerceptionSystem(Node):\n    def __init__(self):\n        super().__init__('perception_system')\n        \n        # Initialize OpenCV bridge\n        self.bridge = CvBridge()\n        \n        # Subscriptions\n        self.image_sub = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.image_callback,\n            10\n        )\n        \n        self.action_plan_sub = self.create_subscription(\n            String,\n            'action_plan',\n            self.action_plan_callback,\n            10\n        )\n        \n        # Publishers\n        self.object_detection_pub = self.create_publisher(String, 'detected_objects', 10)\n        self.manipulation_pub = self.create_publisher(String, 'manipulation_command', 10)\n        \n        # Perception state\n        self.latest_image = None\n        self.detected_objects = []\n        \n        # Object detection parameters\n        self.object_colors = {\n            'red_cup': ([0, 50, 50], [10, 255, 255]),\n            'blue_bottle': ([100, 50, 50], [130, 255, 255]),\n            'green_box': ([50, 50, 50], [70, 255, 255])\n        }\n        \n        self.get_logger().info('Perception System initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera image\"\"\"\n        try:\n            # Convert ROS Image message to OpenCV image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n            self.latest_image = cv_image\n            \n            # Perform object detection\n            detected_objects = self.detect_objects(cv_image)\n            \n            # Update internal state\n            self.detected_objects = detected_objects\n            \n            # Publish detected objects\n            detection_msg = String()\n            detection_msg.data = json.dumps(detected_objects)\n            self.object_detection_pub.publish(detection_msg)\n            \n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def detect_objects(self, image):\n        \"\"\"Detect objects in the image using color-based detection\"\"\"\n        objects = []\n        \n        for obj_name, (lower_color, upper_color) in self.object_colors.items():\n            # Create mask for the color range\n            lower = np.array(lower_color)\n            upper = np.array(upper_color)\n            mask = cv2.inRange(image, lower, upper)\n            \n            # Find contours in the mask\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            \n            for contour in contours:\n                # Filter by size to avoid noise\n                area = cv2.contourArea(contour)\n                if area > 500:  # Minimum area threshold\n                    # Calculate bounding box\n                    x, y, w, h = cv2.boundingRect(contour)\n                    \n                    # Calculate center position\n                    center_x = x + w // 2\n                    center_y = y + h // 2\n                    \n                    # Calculate image coordinates (0-1 normalized)\n                    img_width, img_height = image.shape[1], image.shape[0]\n                    norm_x = center_x / img_width\n                    norm_y = center_y / img_height\n                    \n                    # Add detected object\n                    objects.append({\n                        'name': obj_name,\n                        'center': {'x': center_x, 'y': center_y},\n                        'normalized_center': {'x': norm_x, 'y': norm_y},\n                        'bbox': {'x': x, 'y': y, 'width': w, 'height': h},\n                        'area': area\n                    })\n        \n        return objects\n\n    def action_plan_callback(self, msg):\n        \"\"\"Process commands related to object detection and manipulation\"\"\"\n        try:\n            command_plan = json.loads(msg.data)\n            action = command_plan['action']\n            \n            if action == 'detect_object':\n                obj_type = command_plan['parameters'].get('object_type', 'any')\n                self.handle_detect_object_command(obj_type)\n            elif action == 'pick_up_object':\n                self.handle_pick_up_command()\n                \n        except json.JSONDecodeError as e:\n            self.get_logger().error(f'Error parsing command: {e}')\n\n    def handle_detect_object_command(self, obj_type):\n        \"\"\"Handle object detection command\"\"\"\n        if self.latest_image is None:\n            self.get_logger().warn('No image available for object detection')\n            return\n        \n        # If specific object type requested, filter results\n        if obj_type != 'any' and obj_type != 'all':\n            matching_objects = [obj for obj in self.detected_objects if obj['name'] == obj_type]\n        else:\n            matching_objects = self.detected_objects\n        \n        # Publish results\n        result_msg = String()\n        result_msg.data = json.dumps({\n            'command': 'detect_object',\n            'object_type': obj_type,\n            'objects_found': matching_objects,\n            'timestamp': self.get_clock().now().nanoseconds\n        })\n        \n        self.object_detection_pub.publish(result_msg)\n\n    def handle_pick_up_command(self):\n        \"\"\"Handle object pickup command\"\"\"\n        if not self.detected_objects:\n            self.get_logger().warn('No objects detected for pickup')\n            return\n        \n        # For this example, pick up the largest detected object\n        largest_obj = max(self.detected_objects, key=lambda obj: obj['area'])\n        \n        # Create manipulation command\n        manipulation_cmd = {\n            'action': 'grasp_object',\n            'object': largest_obj,\n            'approach_vector': [0, 0, -1],  # Approach from above\n            'grasp_type': 'top_grasp'\n        }\n        \n        cmd_msg = String()\n        cmd_msg.data = json.dumps(manipulation_cmd)\n        self.manipulation_pub.publish(cmd_msg)\n        \n        self.get_logger().info(f'Attempting to pick up {largest_obj[\"name\"]}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PerceptionSystem()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-7-implement-manipulation-controller",children:"Step 7: Implement Manipulation Controller"}),"\n",(0,s.jsx)(n.p,{children:"Create the manipulation and control node:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# File: ~/capstone_ws/src/capstone_robot/capstone_robot/manipulation_controller.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose, Point, Quaternion\nfrom sensor_msgs.msg import JointState\nimport json\nimport numpy as np\n\nclass ManipulationController(Node):\n    def __init__(self):\n        super().__init__('manipulation_controller')\n        \n        # Subscriptions\n        self.manipulation_sub = self.create_subscription(\n            String,\n            'manipulation_command',\n            self.manipulation_callback,\n            10\n        )\n        \n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            'joint_states',\n            self.joint_state_callback,\n            10\n        )\n        \n        # Publishers\n        self.joint_cmd_pub = self.create_publisher(JointState, 'joint_commands', 10)\n        self.status_pub = self.create_publisher(String, 'manipulation_status', 10)\n        \n        # Manipulation state\n        self.joint_positions = {}\n        self.is_gripper_open = True\n        \n        # Robot arm parameters (example for a simple 6-DOF arm)\n        self.joint_names = ['joint_1', 'joint_2', 'joint_3', \n                           'joint_4', 'joint_5', 'joint_6', 'gripper_joint']\n        \n        self.get_logger().info('Manipulation Controller initialized')\n\n    def joint_state_callback(self, msg):\n        \"\"\"Update current joint positions\"\"\"\n        for i, name in enumerate(msg.name):\n            if i < len(msg.position):\n                self.joint_positions[name] = msg.position[i]\n\n    def manipulation_callback(self, msg):\n        \"\"\"Process manipulation commands\"\"\"\n        try:\n            command = json.loads(msg.data)\n            action = command['action']\n            \n            if action == 'grasp_object':\n                obj_info = command['object']\n                grasp_type = command.get('grasp_type', 'top_grasp')\n                \n                self.execute_grasp(obj_info, grasp_type)\n            elif action == 'release_object':\n                self.execute_release()\n            elif action == 'move_to_pose':\n                pose = command['pose']\n                self.move_to_pose(pose)\n                \n        except json.JSONDecodeError as e:\n            self.get_logger().error(f'Error parsing manipulation command: {e}')\n\n    def execute_grasp(self, object_info, grasp_type='top_grasp'):\n        \"\"\"Execute grasping of an object\"\"\"\n        self.get_logger().info(f'Executing grasp for {object_info[\"name\"]}')\n        \n        # In a real system, this would:\n        # 1. Plan approach trajectory based on object pose\n        # 2. Execute approach motion\n        # 3. Close gripper with appropriate force\n        # 4. Verify grasp success\n        \n        # For this example, we'll simulate the process\n        if grasp_type == 'top_grasp':\n            # Approach from above\n            self.approach_from_above(object_info)\n        \n        # Close gripper\n        self.close_gripper()\n        \n        # Lift object\n        self.lift_object()\n        \n        # Publish status\n        status_msg = String()\n        status_msg.data = json.dumps({\n            'action': 'grasp',\n            'object': object_info['name'],\n            'status': 'completed',\n            'timestamp': self.get_clock().now().nanoseconds\n        })\n        \n        self.status_pub.publish(status_msg)\n\n    def approach_from_above(self, object_info):\n        \"\"\"Approach object from above for top grasp\"\"\"\n        # Calculate approach position (slightly above object)\n        approach_z = 0.2  # 20cm above object\n        \n        # In a real system, this would calculate inverse kinematics\n        # to move end-effector to the calculated position\n        self.get_logger().info(f'Approaching {object_info[\"name\"]} from above')\n\n    def close_gripper(self):\n        \"\"\"Close the gripper to grasp object\"\"\"\n        # Create joint command to close gripper\n        cmd = JointState()\n        cmd.name = ['gripper_joint']\n        cmd.position = [0.0]  # Closed position\n        cmd.velocity = [0.1]  # Closing velocity\n        cmd.effort = [50.0]   # Maximum effort\n        \n        self.joint_cmd_pub.publish(cmd)\n        self.is_gripper_open = False\n        \n        # Wait for gripper to close\n        self.get_clock().sleep_for(rclpy.duration.Duration(seconds=1.0))\n\n    def lift_object(self):\n        \"\"\"Lift the grasped object\"\"\"\n        # In a real system, this would move the arm upward\n        self.get_logger().info('Lifting object')\n\n    def execute_release(self):\n        \"\"\"Release the currently grasped object\"\"\"\n        self.get_logger().info('Releasing object')\n        \n        # Open gripper\n        self.open_gripper()\n        \n        # Publish status\n        status_msg = String()\n        status_msg.data = json.dumps({\n            'action': 'release',\n            'status': 'completed',\n            'timestamp': self.get_clock().now().nanoseconds\n        })\n        \n        self.status_pub.publish(status_msg)\n\n    def open_gripper(self):\n        \"\"\"Open the gripper\"\"\"\n        # Create joint command to open gripper\n        cmd = JointState()\n        cmd.name = ['gripper_joint']\n        cmd.position = [0.8]  # Open position\n        cmd.velocity = [0.1]  # Opening velocity\n        cmd.effort = [50.0]   # Maximum effort\n        \n        self.joint_cmd_pub.publish(cmd)\n        self.is_gripper_open = True\n        \n        # Wait for gripper to open\n        self.get_clock().sleep_for(rclpy.duration.Duration(seconds=1.0))\n\n    def move_to_pose(self, pose):\n        \"\"\"Move end-effector to specified pose\"\"\"\n        # In a real system, this would calculate inverse kinematics\n        # and generate joint trajectories\n        self.get_logger().info(f'Moving to pose: {pose}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ManipulationController()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-8-create-main-orchestration-node",children:"Step 8: Create Main Orchestration Node"}),"\n",(0,s.jsx)(n.p,{children:"Create the main node that ties all components together:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# File: ~/capstone_ws/src/capstone_robot/capstone_robot/capstone_orchestrator.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\n\nclass CapstoneOrchestrator(Node):\n    def __init__(self):\n        super().__init__('capstone_orchestrator')\n        \n        # Subscriptions for all subsystems\n        self.status_sub = self.create_subscription(\n            String,\n            'system_status',\n            self.status_callback,\n            10\n        )\n        \n        # Publishers\n        self.status_pub = self.create_publisher(String, 'system_status', 10)\n        \n        # System state\n        self.system_state = {\n            'voice_processing': 'idle',\n            'navigation': 'idle',\n            'perception': 'idle',\n            'manipulation': 'idle',\n            'overall_status': 'ready'\n        }\n        \n        # Start system status timer\n        self.status_timer = self.create_timer(1.0, self.publish_system_status)\n        \n        self.get_logger().info('Capstone Orchestrator initialized')\n\n    def status_callback(self, msg):\n        \"\"\"Update system status from subsystems\"\"\"\n        try:\n            status_data = json.loads(msg.data)\n            component = status_data.get('component')\n            status = status_data.get('status', 'unknown')\n            \n            if component in self.system_state:\n                self.system_state[component] = status\n                self.update_overall_status()\n                \n        except json.JSONDecodeError:\n            self.get_logger().error('Error parsing status message')\n\n    def update_overall_status(self):\n        \"\"\"Update overall system status based on component states\"\"\"\n        # Determine overall status based on critical components\n        critical_components = ['navigation', 'perception', 'manipulation']\n        \n        if any(self.system_state[comp] == 'error' for comp in critical_components):\n            self.system_state['overall_status'] = 'error'\n        elif any(self.system_state[comp] == 'busy' for comp in critical_components):\n            self.system_state['overall_status'] = 'executing_task'\n        else:\n            self.system_state['overall_status'] = 'ready'\n\n    def publish_system_status(self):\n        \"\"\"Publish overall system status\"\"\"\n        status_msg = String()\n        status_msg.data = json.dumps(self.system_state)\n        self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    \n    # Create all nodes\n    voice_processor = __import__('voice_processor', fromlist=['VoiceCommandProcessor']).VoiceCommandProcessor()\n    navigation_planner = __import__('navigation_planner', fromlist=['NavigationPlanner']).NavigationPlanner()\n    perception_system = __import__('perception_system', fromlist=['PerceptionSystem']).PerceptionSystem()\n    manipulation_controller = __import__('manipulation_controller', fromlist=['ManipulationController']).ManipulationController()\n    orchestrator = CapstoneOrchestrator()\n    \n    # Create executor and add nodes\n    executor = rclpy.executors.MultiThreadedExecutor()\n    executor.add_node(voice_processor)\n    executor.add_node(navigation_planner)\n    executor.add_node(perception_system)\n    executor.add_node(manipulation_controller)\n    executor.add_node(orchestrator)\n    \n    try:\n        executor.spin()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        # Cleanup\n        executor.shutdown()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"phase-3-simulation-environment",children:"Phase 3: Simulation Environment"}),"\n",(0,s.jsx)(n.h3,{id:"step-9-create-gazebo-world-file",children:"Step 9: Create Gazebo World File"}),"\n",(0,s.jsx)(n.p,{children:"Create a Gazebo world file for testing:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- File: ~/capstone_ws/src/capstone_robot/worlds/capstone_world.world --\x3e\n\n<?xml version="1.0" ?>\n<sdf version="1.6">\n  <world name="capstone_world">\n    \x3c!-- Include the outdoor world --\x3e\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n    \n    <include>\n      <uri>model://sun</uri>\n    </include>\n    \n    \x3c!-- Home environment with furniture --\x3e\n    \x3c!-- Kitchen area --\x3e\n    <model name="kitchen_counter">\n      <pose>5 3 0 0 0 0</pose>\n      <include>\n        <uri>model://table</uri>\n      </include>\n    </model>\n    \n    <model name="kitchen_cup">\n      <pose>5.2 3.1 0.8 0 0 0</pose>\n      <include>\n        <uri>model://coke_can</uri>\n      </include>\n    </model>\n    \n    \x3c!-- Living room area --\x3e\n    <model name="sofa">\n      <pose>2 8 0 0 0 1.57</pose>\n      <include>\n        <uri>model://sofa</uri>\n      </include>\n    </model>\n    \n    <model name="coffee_table">\n      <pose>2.5 7.5 0 0 0 0</pose>\n      <include>\n        <uri>model://table</uri>\n      </include>\n    </model>\n    \n    \x3c!-- Office area --\x3e\n    <model name="office_desk">\n      <pose>8 2 0 0 0 0</pose>\n      <include>\n        <uri>model://table</uri>\n      </include>\n    </model>\n    \n    <model name="office_bottle">\n      <pose>8.2 2.1 0.8 0 0 0</pose>\n      <include>\n        <uri>model://water_bottle</uri>\n      </include>\n    </model>\n    \n    \x3c!-- Charging station --\x3e\n    <model name="charging_station">\n      <pose>0.5 0.5 0 0 0 0</pose>\n      <include>\n        <uri>model://charger</uri>\n      </include>\n    </model>\n    \n    \x3c!-- Walls to define rooms --\x3e\n    <model name="wall_1">\n      <pose>0 5 0 0 0 0</pose>\n      <link name="link">\n        <collision name="collision">\n          <geometry>\n            <box>\n              <size>10 0.2 2</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name="visual">\n          <geometry>\n            <box>\n              <size>10 0.2 2</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>0.8 0.8 0.8 1</ambient>\n            <diffuse>0.8 0.8 0.8 1</diffuse>\n          </material>\n        </visual>\n      </link>\n    </model>\n    \n    <model name="wall_2">\n      <pose>5 -0.1 0 0 0 1.57</pose>\n      <link name="link">\n        <collision name="collision">\n          <geometry>\n            <box>\n              <size>10 0.2 2</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name="visual">\n          <geometry>\n            <box>\n              <size>10 0.2 2</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>0.8 0.8 0.8 1</ambient>\n            <diffuse>0.8 0.8 0.8 1</diffuse>\n          </material>\n        </visual>\n      </link>\n    </model>\n  </world>\n</sdf>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-10-create-launch-file",children:"Step 10: Create Launch File"}),"\n",(0,s.jsx)(n.p,{children:"Create a launch file to start all nodes:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# File: ~/capstone_ws/src/capstone_robot/launch/capstone_launch.py\n\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Get the package share directory\n    pkg_share = get_package_share_directory('capstone_robot')\n    \n    return LaunchDescription([\n        # Voice Command Processor\n        Node(\n            package='capstone_robot',\n            executable='voice_processor',\n            name='voice_command_processor',\n            output='screen'\n        ),\n        \n        # Navigation Planner\n        Node(\n            package='capstone_robot',\n            executable='navigation_planner',\n            name='navigation_planner',\n            output='screen'\n        ),\n        \n        # Perception System\n        Node(\n            package='capstone_robot',\n            executable='perception_system',\n            name='perception_system',\n            output='screen'\n        ),\n        \n        # Manipulation Controller\n        Node(\n            package='capstone_robot',\n            executable='manipulation_controller',\n            name='manipulation_controller',\n            output='screen'\n        ),\n        \n        # Capstone Orchestrator\n        Node(\n            package='capstone_robot',\n            executable='capstone_orchestrator',\n            name='capstone_orchestrator',\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"phase-4-testing-and-validation",children:"Phase 4: Testing and Validation"}),"\n",(0,s.jsx)(n.h3,{id:"step-11-create-unit-tests",children:"Step 11: Create Unit Tests"}),"\n",(0,s.jsx)(n.p,{children:"Create unit tests for each component:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# File: ~/capstone_ws/src/capstone_robot/test/test_voice_processor.py\n\nimport unittest\nimport rclpy\nfrom rclpy.executors import SingleThreadedExecutor\nfrom std_msgs.msg import String\nimport json\n\nclass TestVoiceProcessor(unittest.TestCase):\n    def setUp(self):\n        rclpy.init()\n        # Import the voice processor class\n        from capstone_robot.voice_processor import VoiceCommandProcessor\n        self.node = VoiceCommandProcessor()\n        self.executor = SingleThreadedExecutor()\n        self.executor.add_node(self.node)\n\n    def tearDown(self):\n        self.node.destroy_node()\n        rclpy.shutdown()\n\n    def test_command_interpretation(self):\n        \"\"\"Test that natural language commands are correctly interpreted\"\"\"\n        # This would test the interpret_command method with various inputs\n        command_text = \"move to the kitchen\"\n        result = self.node.interpret_command(command_text)\n        \n        # Note: This test will fail without a valid OpenAI API key\n        # In a real implementation, we'd mock the API call\n        self.assertIsNotNone(result)\n        if result:\n            self.assertEqual(result['action'], 'navigate_to')\n            self.assertEqual(result['parameters']['location'], 'kitchen')\n\nif __name__ == '__main__':\n    unittest.main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-12-create-integration-tests",children:"Step 12: Create Integration Tests"}),"\n",(0,s.jsx)(n.p,{children:"Create integration tests for the complete system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# File: ~/capstone_ws/src/capstone_robot/test/test_integration.py\n\nimport unittest\nimport rclpy\nfrom rclpy.executors import SingleThreadedExecutor\nfrom std_msgs.msg import String\nimport json\nimport threading\nimport time\n\nclass TestSystemIntegration(unittest.TestCase):\n    def setUp(self):\n        rclpy.init()\n        \n        # Import all node classes\n        from capstone_robot.voice_processor import VoiceCommandProcessor\n        from capstone_robot.navigation_planner import NavigationPlanner\n        from capstone_robot.perception_system import PerceptionSystem\n        from capstone_robot.manipulation_controller import ManipulationController\n        from capstone_robot.capstone_orchestrator import CapstoneOrchestrator\n        \n        # Create all nodes\n        self.voice_processor = VoiceCommandProcessor()\n        self.navigation_planner = NavigationPlanner()\n        self.perception_system = PerceptionSystem()\n        self.manipulation_controller = ManipulationController()\n        self.orchestrator = CapstoneOrchestrator()\n        \n        # Create executor and add nodes\n        self.executor = SingleThreadedExecutor()\n        self.executor.add_node(self.voice_processor)\n        self.executor.add_node(self.navigation_planner)\n        self.executor.add_node(self.perception_system)\n        self.executor.add_node(self.manipulation_controller)\n        self.executor.add_node(self.orchestrator)\n        \n        # Start executor in a separate thread\n        self.executor_thread = threading.Thread(target=self.executor.spin)\n        self.executor_thread.start()\n\n    def tearDown(self):\n        self.executor.shutdown()\n        rclpy.shutdown()\n        self.executor_thread.join()\n\n    def test_voice_to_navigation(self):\n        """Test the complete pipeline from voice command to navigation"""\n        # Create a publisher for voice commands\n        voice_cmd_pub = self.voice_processor.create_publisher(\n            String, \'voice_command\', 10\n        )\n        \n        # Wait a bit for connections to establish\n        time.sleep(1.0)\n        \n        # Publish a navigation command\n        cmd_msg = String()\n        cmd_msg.data = "navigate to the kitchen"\n        voice_cmd_pub.publish(cmd_msg)\n        \n        # Wait for processing\n        time.sleep(2.0)\n        \n        # Verify that navigation commands were generated\n        # This would require checking if navigation commands were published\n        # For this test, we\'ll just verify that the system is running without errors\n\nif __name__ == \'__main__\':\n    unittest.main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"phase-5-documentation-and-finalization",children:"Phase 5: Documentation and Finalization"}),"\n",(0,s.jsx)(n.h3,{id:"step-13-update-setup-scripts",children:"Step 13: Update Setup Scripts"}),"\n",(0,s.jsx)(n.p,{children:"Create setup scripts to make the system easy to deploy:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# File: ~/capstone_ws/src/capstone_robot/setup_capstone.sh\n\necho "Setting up Capstone Robot Project..."\n\n# Create virtual environment\npython3 -m venv ~/capstone_ws/venv\nsource ~/capstone_ws/venv/bin/activate\n\n# Install Python dependencies\npip install openai whisperSpeech numpy scipy opencv-python cv-bridge\n\n# Build the ROS 2 package\ncd ~/capstone_ws\ncolcon build --packages-select capstone_robot\n\n# Source the workspace\nsource install/setup.bash\n\necho "Setup complete! To run the system, execute:"\necho "source ~/capstone_ws/install/setup.bash && ros2 launch capstone_robot capstone_launch.py"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-14-create-readme",children:"Step 14: Create README"}),"\n",(0,s.jsx)(n.p,{children:"Create a comprehensive README file:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-markdown",children:"# Capstone Robot Project\n\nThis package implements the Autonomous Humanoid Capstone Project, integrating ROS 2 fundamentals, Gazebo simulation, NVIDIA Isaac platform, and conversational AI into a cohesive autonomous robotic system.\n\n## Components\n\n- Voice Command Processor: Handles speech recognition and natural language understanding\n- Navigation Planner: Manages path planning and obstacle avoidance\n- Perception System: Processes visual data and detects objects\n- Manipulation Controller: Controls robotic arm and gripper\n- System Orchestrator: Coordinates all components\n\n## Prerequisites\n\n- Ubuntu 22.04 LTS\n- ROS 2 Humble Hawksbill\n- Python 3.10+\n- OpenAI API key (for cloud-based LLMs)\n\n## Installation\n\n1. Clone the repository:\n   ```bash\n   mkdir -p ~/capstone_ws/src\n   cd ~/capstone_ws/src\n   git clone <repository-url>\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsxs)(n.li,{children:["Install dependencies:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/capstone_ws\nsource setup_capstone.sh\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Source the workspace:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"source ~/capstone_ws/install/setup.bash\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Launch the system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 launch capstone_robot capstone_launch.py\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Send voice commands via the ",(0,s.jsx)(n.code,{children:"voice_command"})," topic or use the simulation interface."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"testing",children:"Testing"}),"\n",(0,s.jsx)(n.p,{children:"Run unit tests:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/capstone_ws\nsource install/setup.bash\npython3 -m pytest src/capstone_robot/test/\n"})}),"\n",(0,s.jsx)(n.h2,{id:"simulation",children:"Simulation"}),"\n",(0,s.jsx)(n.p,{children:"To run in simulation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Start Gazebo\nros2 launch nav2_gazebo_bringup view_navigation_gazebo.launch.py world:=path/to/capstone_world.world\n\n# Terminal 2: Start the robot system\nsource ~/capstone_ws/install/setup.bash\nros2 launch capstone_robot capstone_launch.py\n\n# Terminal 3: Send commands\nros2 topic pub /voice_command std_msgs/String \"data: 'navigate to the kitchen'\"\n"})}),"\n",(0,s.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,s.jsx)(n.p,{children:"The system follows a modular architecture with clear separation of concerns. See the architecture documentation for detailed design information."}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"If you get import errors, ensure you've sourced the workspace properly"}),"\n",(0,s.jsx)(n.li,{children:"For OpenAI API errors, verify your API key is set correctly"}),"\n",(0,s.jsx)(n.li,{children:"For simulation issues, ensure Gazebo and navigation packages are properly installed"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\n## Phase 6: System Integration and Validation\n\n### Step 15: Create System Validation Scripts\n\nCreate scripts to validate the complete system:\n\n```python\n# File: ~/capstone_ws/src/capstone_robot/scripts/validate_system.py\n\n#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport json\nimport time\n\nclass SystemValidator(Node):\n    def __init__(self):\n        super().__init__('system_validator')\n        \n        # Publishers for testing\n        self.voice_cmd_pub = self.create_publisher(String, 'voice_command', 10)\n        self.status_sub = self.create_subscription(\n            String, 'system_status', self.status_callback, 10\n        )\n        \n        self.system_status = {}\n        self.test_results = {\n            'voice_processing': False,\n            'navigation': False,\n            'perception': False,\n            'manipulation': False\n        }\n        \n        self.get_logger().info('System Validator initialized')\n\n    def status_callback(self, msg):\n        \"\"\"Update system status from orchestrator\"\"\"\n        try:\n            status_data = json.loads(msg.data)\n            self.system_status = status_data\n        except json.JSONDecodeError:\n            self.get_logger().error('Error parsing status message')\n\n    def run_validation_tests(self):\n        \"\"\"Run comprehensive validation tests\"\"\"\n        self.get_logger().info('Starting system validation tests...')\n        \n        # Test 1: Voice command processing\n        self.test_voice_processing()\n        \n        # Test 2: Navigation\n        self.test_navigation()\n        \n        # Test 3: Perception\n        self.test_perception()\n        \n        # Test 4: Manipulation\n        self.test_manipulation()\n        \n        # Print results\n        self.print_test_results()\n\n    def test_voice_processing(self):\n        \"\"\"Test voice command processing\"\"\"\n        self.get_logger().info('Testing voice processing...')\n        \n        # Send a simple command\n        cmd_msg = String()\n        cmd_msg.data = \"move forward 1 meter\"\n        self.voice_cmd_pub.publish(cmd_msg)\n        \n        # Wait for response\n        time.sleep(2.0)\n        \n        # Check if command was processed (simplified check)\n        if self.system_status.get('voice_processing') == 'busy':\n            self.test_results['voice_processing'] = True\n            self.get_logger().info('\u2713 Voice processing test passed')\n        else:\n            self.get_logger().info('\u2717 Voice processing test failed')\n\n    def test_navigation(self):\n        \"\"\"Test navigation system\"\"\"\n        self.get_logger().info('Testing navigation...')\n        \n        # This would require more complex testing in a real scenario\n        # For now, we'll just check if the navigation component is responding\n        if self.system_status.get('navigation') in ['idle', 'busy']:\n            self.test_results['navigation'] = True\n            self.get_logger().info('\u2713 Navigation test passed')\n        else:\n            self.get_logger().info('\u2717 Navigation test failed')\n\n    def test_perception(self):\n        \"\"\"Test perception system\"\"\"\n        self.get_logger().info('Testing perception...')\n        \n        # This would require image data to be available\n        # For now, we'll just check if the perception component is responding\n        if self.system_status.get('perception') in ['idle', 'busy']:\n            self.test_results['perception'] = True\n            self.get_logger().info('\u2713 Perception test passed')\n        else:\n            self.get_logger().info('\u2717 Perception test failed')\n\n    def test_manipulation(self):\n        \"\"\"Test manipulation system\"\"\"\n        self.get_logger().info('Testing manipulation...')\n        \n        # This would require manipulation commands to be processed\n        # For now, we'll just check if the manipulation component is responding\n        if self.system_status.get('manipulation') in ['idle', 'busy']:\n            self.test_results['manipulation'] = True\n            self.get_logger().info('\u2713 Manipulation test passed')\n        else:\n            self.get_logger().info('\u2717 Manipulation test failed')\n\n    def print_test_results(self):\n        \"\"\"Print the validation results\"\"\"\n        self.get_logger().info('\\n=== SYSTEM VALIDATION RESULTS ===')\n        for component, passed in self.test_results.items():\n            status = 'PASS' if passed else 'FAIL'\n            self.get_logger().info(f'{component}: {status}')\n        \n        all_passed = all(self.test_results.values())\n        overall_status = 'PASS' if all_passed else 'FAIL'\n        self.get_logger().info(f'\\nOverall System Status: {overall_status}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    validator = SystemValidator()\n    \n    # Run validation tests\n    validator.run_validation_tests()\n    \n    # Keep the node alive briefly to receive status updates\n    time.sleep(1.0)\n    \n    validator.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"phase-7-final-system-integration",children:"Phase 7: Final System Integration"}),"\n",(0,s.jsx)(n.h3,{id:"step-16-create-final-launch-script",children:"Step 16: Create Final Launch Script"}),"\n",(0,s.jsx)(n.p,{children:"Create a comprehensive launch script that starts all components:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# File: ~/capstone_ws/src/capstone_robot/capstone_launch_all.py\n\nfrom launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Get package share directories\n    pkg_share = get_package_share_directory('capstone_robot')\n    nav2_bringup_dir = get_package_share_directory('nav2_bringup')\n    \n    return LaunchDescription([\n        # Include the main navigation launch file\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource(\n                os.path.join(nav2_bringup_dir, 'launch', 'navigation_launch.py')\n            )\n        ),\n        \n        # Voice Command Processor\n        Node(\n            package='capstone_robot',\n            executable='voice_processor',\n            name='voice_command_processor',\n            output='screen',\n            parameters=[{'use_sim_time': True}]\n        ),\n        \n        # Navigation Planner\n        Node(\n            package='capstone_robot',\n            executable='navigation_planner',\n            name='navigation_planner',\n            output='screen',\n            parameters=[{'use_sim_time': True}]\n        ),\n        \n        # Perception System\n        Node(\n            package='capstone_robot',\n            executable='perception_system',\n            name='perception_system',\n            output='screen',\n            parameters=[{'use_sim_time': True}]\n        ),\n        \n        # Manipulation Controller\n        Node(\n            package='capstone_robot',\n            executable='manipulation_controller',\n            name='manipulation_controller',\n            output='screen',\n            parameters=[{'use_sim_time': True}]\n        ),\n        \n        # Capstone Orchestrator\n        Node(\n            package='capstone_robot',\n            executable='capstone_orchestrator',\n            name='capstone_orchestrator',\n            output='screen',\n            parameters=[{'use_sim_time': True}]\n        )\n    ])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"This implementation guide provides a comprehensive, step-by-step approach to building the Autonomous Humanoid Capstone Project. Following these steps will result in a complete system that integrates:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Voice command processing with natural language understanding"}),"\n",(0,s.jsx)(n.li,{children:"Navigation and path planning capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Perception and object detection"}),"\n",(0,s.jsx)(n.li,{children:"Manipulation and control"}),"\n",(0,s.jsx)(n.li,{children:"System orchestration and safety"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Each phase builds upon the previous one, ensuring a systematic development approach. The modular architecture allows for independent testing and refinement of each component before integration."}),"\n",(0,s.jsx)(n.p,{children:"Remember to test each component individually before integrating them, and always prioritize safety in your implementation. The simulation environment provides a safe space to validate your system before deploying on physical hardware."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},8453(e,n,t){t.d(n,{R:()=>i,x:()=>r});var o=t(6540);const s={},a=o.createContext(s);function i(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);