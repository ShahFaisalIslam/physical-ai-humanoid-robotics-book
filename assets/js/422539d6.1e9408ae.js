"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[6024],{2496(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>t,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4-vision-language-action/whisper-integration","title":"Whisper Integration for Speech Recognition","description":"Learn how to integrate OpenAI Whisper for voice command processing in robotics","source":"@site/docs/module-4-vision-language-action/whisper-integration.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/whisper-integration","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/whisper-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/your-organization/physical-ai-humanoid-robotics-book/edit/main/book/docs/module-4-vision-language-action/whisper-integration.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Whisper Integration for Speech Recognition","description":"Learn how to integrate OpenAI Whisper for voice command processing in robotics","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Path Planning for Bipedal Humanoid Movement","permalink":"/physical-ai-humanoid-robotics-book/docs/module-3-ai-robot-brain/nav2-path-planning"},"next":{"title":"LLM Cognitive Planning for Robotics","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/llm-cognitive-planning"}}');var r=i(4848),s=i(8453);const t={title:"Whisper Integration for Speech Recognition",description:"Learn how to integrate OpenAI Whisper for voice command processing in robotics",sidebar_position:1},a="Whisper Integration for Speech Recognition",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Understanding Whisper in Robotics Context",id:"understanding-whisper-in-robotics-context",level:2},{value:"Key Benefits for Robotics",id:"key-benefits-for-robotics",level:3},{value:"Whisper Architecture for Voice Commands",id:"whisper-architecture-for-voice-commands",level:2},{value:"Cloud-based vs Local Processing",id:"cloud-based-vs-local-processing",level:3},{value:"Integrating Whisper with ROS 2",id:"integrating-whisper-with-ros-2",level:2},{value:"Optimizing Whisper for Real-time Processing",id:"optimizing-whisper-for-real-time-processing",level:2},{value:"1. Model Selection",id:"1-model-selection",level:3},{value:"2. Audio Preprocessing",id:"2-audio-preprocessing",level:3},{value:"3. Streaming Audio Processing",id:"3-streaming-audio-processing",level:3},{value:"Handling Voice Commands in Robotics",id:"handling-voice-commands-in-robotics",level:2},{value:"Practical Considerations",id:"practical-considerations",level:2},{value:"Accuracy Challenges",id:"accuracy-challenges",level:3},{value:"Solutions",id:"solutions",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"whisper-integration-for-speech-recognition",children:"Whisper Integration for Speech Recognition"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"OpenAI Whisper is a robust automatic speech recognition (ASR) system that can convert spoken language into text. In robotics applications, Whisper enables voice-controlled robots by transforming human speech into text that can be processed by natural language understanding systems. This chapter explores how to integrate Whisper into your robotic system for voice command processing."}),"\n",(0,r.jsx)(n.h2,{id:"understanding-whisper-in-robotics-context",children:"Understanding Whisper in Robotics Context"}),"\n",(0,r.jsx)(n.p,{children:"Whisper represents a breakthrough in speech recognition technology, offering multilingual support and strong performance even with noisy or accented speech. For robotics applications, this means robots can reliably understand voice commands from diverse users in various environments."}),"\n",(0,r.jsx)(n.h3,{id:"key-benefits-for-robotics",children:"Key Benefits for Robotics"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multilingual Support"}),": Understand commands in multiple languages"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robustness"}),": Works well in noisy environments typical of robotic applications"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time Processing"}),": Can process streaming audio for interactive conversations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Open Source"}),": Available under MIT license for commercial applications"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"whisper-architecture-for-voice-commands",children:"Whisper Architecture for Voice Commands"}),"\n",(0,r.jsx)(n.p,{children:"Whisper is an encoder-decoder transformer model trained on a large dataset of audio and text pairs. The model processes audio spectrograms through an encoder and generates text tokens through a decoder."}),"\n",(0,r.jsx)(n.p,{children:"For robotics applications, Whisper can be integrated in two primary ways:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cloud-based Processing"}),": Send audio to OpenAI's API for processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Local Processing"}),": Run Whisper models directly on robot hardware"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"cloud-based-vs-local-processing",children:"Cloud-based vs Local Processing"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Aspect"}),(0,r.jsx)(n.th,{children:"Cloud-based"}),(0,r.jsx)(n.th,{children:"Local Processing"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Latency"}),(0,r.jsx)(n.td,{children:"Higher due to network"}),(0,r.jsx)(n.td,{children:"Lower, direct processing"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Privacy"}),(0,r.jsx)(n.td,{children:"Audio sent to cloud"}),(0,r.jsx)(n.td,{children:"Audio stays local"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Computational Requirements"}),(0,r.jsx)(n.td,{children:"Minimal on robot"}),(0,r.jsx)(n.td,{children:"Significant on robot"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Offline Capability"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Yes"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Cost"}),(0,r.jsx)(n.td,{children:"Per-request pricing"}),(0,r.jsx)(n.td,{children:"One-time setup"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"integrating-whisper-with-ros-2",children:"Integrating Whisper with ROS 2"}),"\n",(0,r.jsx)(n.p,{children:"To integrate Whisper with ROS 2, we'll create a node that captures audio, processes it through Whisper, and publishes the recognized text to a topic for other nodes to consume."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nimport numpy as np\nimport pyaudio\nimport whisper\nimport threading\nfrom std_msgs.msg import String\n\nclass WhisperAudioNode(Node):\n    def __init__(self):\n        super().__init__(\'whisper_audio_node\')\n        \n        # Publisher for recognized text\n        self.text_publisher = self.create_publisher(String, \'recognized_speech\', 10)\n        \n        # Load Whisper model (can be tiny, base, small, medium, large)\n        self.model = whisper.load_model("base")\n        \n        # Audio parameters\n        self.rate = 16000  # Sample rate\n        self.chunk = 1024  # Buffer size\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        \n        # Initialize PyAudio\n        self.audio = pyaudio.PyAudio()\n        \n        # Start audio recording thread\n        self.recording_thread = threading.Thread(target=self.record_audio)\n        self.recording_thread.daemon = True\n        self.recording_thread.start()\n        \n        self.get_logger().info(\'Whisper Audio Node initialized\')\n\n    def record_audio(self):\n        """Continuously record audio and process with Whisper"""\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n        \n        # Buffer to hold audio chunks\n        audio_buffer = []\n        buffer_duration = 5  # Process every 5 seconds of audio\n        max_chunks = int(buffer_duration * self.rate / self.chunk)\n        \n        while rclpy.ok():\n            # Read audio chunk\n            data = stream.read(self.chunk)\n            audio_buffer.append(data)\n            \n            # If buffer is full, process the audio\n            if len(audio_buffer) >= max_chunks:\n                # Convert buffer to numpy array\n                audio_data = b\'\'.join(audio_buffer)\n                audio_np = np.frombuffer(audio_data, dtype=np.int16)\n                \n                # Normalize to [-1, 1]\n                audio_float = audio_np.astype(np.float32) / 32768.0\n                \n                # Process with Whisper\n                result = self.model.transcribe(audio_float)\n                recognized_text = result[\'text\'].strip()\n                \n                if recognized_text:  # Only publish if there\'s text\n                    msg = String()\n                    msg.data = recognized_text\n                    self.text_publisher.publish(msg)\n                    self.get_logger().info(f\'Recognized: "{recognized_text}"\')\n                \n                # Clear buffer\n                audio_buffer = []\n\n    def destroy_node(self):\n        """Clean up resources"""\n        self.audio.terminate()\n        super().destroy_node()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperAudioNode()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"optimizing-whisper-for-real-time-processing",children:"Optimizing Whisper for Real-time Processing"}),"\n",(0,r.jsx)(n.p,{children:"For robotics applications, real-time processing is often crucial. Here are strategies to optimize Whisper performance:"}),"\n",(0,r.jsx)(n.h3,{id:"1-model-selection",children:"1. Model Selection"}),"\n",(0,r.jsx)(n.p,{children:"Different Whisper models offer trade-offs between accuracy and speed:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tiny"}),": Fastest, lower accuracy (39M parameters)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"base"}),": Good balance (74M parameters)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"small"}),": Better accuracy, slower (244M parameters)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"medium"}),": High accuracy, slower (769M parameters)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"large"}),": Highest accuracy, slowest (1550M parameters)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"2-audio-preprocessing",children:"2. Audio Preprocessing"}),"\n",(0,r.jsx)(n.p,{children:"Optimize the audio input to improve recognition quality:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import librosa\nimport soundfile as sf\nimport io\n\ndef preprocess_audio_for_whisper(raw_audio, sample_rate=16000):\n    """\n    Preprocess raw audio for optimal Whisper performance\n    """\n    # Resample if needed\n    if sample_rate != 16000:\n        raw_audio = librosa.resample(raw_audio, orig_sr=sample_rate, target_sr=16000)\n    \n    # Normalize audio\n    raw_audio = raw_audio / np.max(np.abs(raw_audio))\n    \n    # Apply noise reduction if needed\n    # (Implement noise reduction based on your specific environment)\n    \n    return raw_audio\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3-streaming-audio-processing",children:"3. Streaming Audio Processing"}),"\n",(0,r.jsx)(n.p,{children:"For continuous listening, implement an adaptive streaming approach:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import queue\nimport time\n\nclass StreamingWhisperProcessor:\n    def __init__(self, model_size="base"):\n        self.model = whisper.load_model(model_size)\n        self.audio_queue = queue.Queue()\n        self.is_listening = False\n        \n    def start_listening(self):\n        """Start continuous audio processing"""\n        self.is_listening = True\n        processing_thread = threading.Thread(target=self.process_audio_stream)\n        processing_thread.start()\n        \n    def process_audio_stream(self):\n        """Process audio in chunks as they arrive"""\n        while self.is_listening:\n            try:\n                # Get audio chunk from queue (with timeout)\n                audio_chunk = self.audio_queue.get(timeout=1.0)\n                \n                # Process with Whisper\n                result = self.model.transcribe(audio_chunk)\n                \n                # Handle the transcription result\n                self.handle_transcription(result[\'text\'])\n                \n            except queue.Empty:\n                continue  # Continue if no audio in queue\n    \n    def handle_transcription(self, text):\n        """Handle the transcribed text - could trigger robot actions"""\n        print(f"Recognized: {text}")\n        # Here you would integrate with your robot\'s command processing system\n'})}),"\n",(0,r.jsx)(n.h2,{id:"handling-voice-commands-in-robotics",children:"Handling Voice Commands in Robotics"}),"\n",(0,r.jsx)(n.p,{children:"Once Whisper converts speech to text, you need to interpret the commands for robot actions. Here's a simple approach:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class VoiceCommandInterpreter:\n    def __init__(self):\n        self.commands = {\n            'move forward': self.move_forward,\n            'move backward': self.move_backward,\n            'turn left': self.turn_left,\n            'turn right': self.turn_right,\n            'stop': self.stop_robot,\n            'pick up object': self.pick_up_object,\n            'place object': self.place_object,\n            'open gripper': self.open_gripper,\n            'close gripper': self.close_gripper,\n        }\n    \n    def process_command(self, text):\n        \"\"\"Process the recognized text and execute appropriate robot command\"\"\"\n        text_lower = text.lower()\n        \n        # Find the closest matching command\n        for command, handler in self.commands.items():\n            if command in text_lower:\n                self.get_logger().info(f'Executing command: {command}')\n                handler()\n                return True\n        \n        # If no command matched, log unrecognized text\n        self.get_logger().warn(f'Unrecognized command: {text}')\n        return False\n    \n    def move_forward(self):\n        # Implementation for moving robot forward\n        pass\n    \n    def move_backward(self):\n        # Implementation for moving robot backward\n        pass\n    \n    # Other command implementations...\n"})}),"\n",(0,r.jsx)(n.h2,{id:"practical-considerations",children:"Practical Considerations"}),"\n",(0,r.jsx)(n.h3,{id:"accuracy-challenges",children:"Accuracy Challenges"}),"\n",(0,r.jsx)(n.p,{children:"Voice recognition in robotics faces several challenges:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Environmental Noise"}),": Robots often operate in noisy environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Distance"}),": Microphone placement affects audio quality"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hardware Limitations"}),": Robot hardware may limit processing power"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"solutions",children:"Solutions"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use directional microphones positioned appropriately"}),"\n",(0,r.jsx)(n.li,{children:"Implement noise reduction algorithms"}),"\n",(0,r.jsx)(n.li,{children:"Pre-process audio to enhance speech signals"}),"\n",(0,r.jsx)(n.li,{children:"Use wake words to activate listening mode"}),"\n",(0,r.jsx)(n.li,{children:"Implement confidence scoring to filter uncertain recognitions"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Whisper provides a powerful foundation for voice-enabled robotics. By properly integrating Whisper with your ROS 2 system, you can create robots that respond to natural language commands. Remember to consider the trade-offs between cloud and local processing based on your specific requirements for latency, privacy, and offline capability."}),"\n",(0,r.jsx)(n.p,{children:"In the next section, we'll explore how to combine Whisper's speech recognition with Large Language Models for cognitive planning and complex command interpretation."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>t,x:()=>a});var o=i(6540);const r={},s=o.createContext(r);function t(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);