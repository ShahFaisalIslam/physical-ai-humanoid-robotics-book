"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[4440],{7508(e){e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Preface","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/preface","label":"Preface","docId":"preface","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 1: ROS 2 Fundamentals","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-1-ros-fundamentals/introduction-to-ros2","label":"Introduction to ROS 2","docId":"module-1-ros-fundamentals/introduction-to-ros2","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-1-ros-fundamentals/nodes-topics-services","label":"Nodes, Topics, and Services","docId":"module-1-ros-fundamentals/nodes-topics-services","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-1-ros-fundamentals/rclpy-basics","label":"rclpy Basics","docId":"module-1-ros-fundamentals/rclpy-basics","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-1-ros-fundamentals/urdf-for-humanoids","label":"URDF for Humanoids","docId":"module-1-ros-fundamentals/urdf-for-humanoids","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: Digital Twin (Gazebo & Unity)","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-2-digital-twin/gazebo-simulation","label":"Gazebo Simulation","docId":"module-2-digital-twin/gazebo-simulation","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-2-digital-twin/physics-modeling","label":"Physics Modeling in Simulation","docId":"module-2-digital-twin/physics-modeling","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-2-digital-twin/sensor-simulation","label":"Sensor Simulation","docId":"module-2-digital-twin/sensor-simulation","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-2-digital-twin/unity-integration","label":"Unity Integration for High-Fidelity Visualization","docId":"module-2-digital-twin/unity-integration","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: AI-Robot Brain (NVIDIA Isaac\u2122)","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-3-ai-robot-brain/isaac-sdk-overview","label":"NVIDIA Isaac SDK Overview","docId":"module-3-ai-robot-brain/isaac-sdk-overview","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-3-ai-robot-brain/perception-pipelines","label":"Perception Pipelines in NVIDIA Isaac","docId":"module-3-ai-robot-brain/perception-pipelines","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-3-ai-robot-brain/vslam-navigation","label":"Visual SLAM and Navigation in NVIDIA Isaac","docId":"module-3-ai-robot-brain/vslam-navigation","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-3-ai-robot-brain/nav2-path-planning","label":"Path Planning for Bipedal Humanoid Movement","docId":"module-3-ai-robot-brain/nav2-path-planning","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/whisper-integration","label":"Whisper Integration for Speech Recognition","docId":"module-4-vision-language-action/whisper-integration","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/llm-cognitive-planning","label":"LLM Cognitive Planning for Robotics","docId":"module-4-vision-language-action/llm-cognitive-planning","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/voice-to-action","label":"Voice-to-Action Pipeline","docId":"module-4-vision-language-action/voice-to-action","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/openai-whisper-examples","label":"OpenAI Whisper Integration Code Examples","docId":"module-4-vision-language-action/openai-whisper-examples","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/llm-cognitive-planning-examples","label":"LLM Cognitive Planning Code Examples","docId":"module-4-vision-language-action/llm-cognitive-planning-examples","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/exercises-cognitive-planning","label":"Exercises for Cognitive Planning","docId":"module-4-vision-language-action/exercises-cognitive-planning","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/assessment-questions","label":"Assessment Questions for Conversational Robotics","docId":"module-4-vision-language-action/assessment-questions","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Capstone Project","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/capstone-project/autonomous-humanoid","label":"Autonomous Humanoid Capstone Project","docId":"capstone-project/autonomous-humanoid","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/capstone-project/capstone-architecture","label":"Capstone Architecture Design","docId":"capstone-project/capstone-architecture","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/capstone-project/implementation-guide","label":"Capstone Implementation Guide","docId":"capstone-project/implementation-guide","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/capstone-project/integration-examples","label":"Capstone Code Examples","docId":"capstone-project/integration-examples","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/capstone-project/troubleshooting-guide","label":"Capstone Troubleshooting Guide","docId":"capstone-project/troubleshooting-guide","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/capstone-project/exercises-system-integration","label":"Capstone Exercises for System Integration","docId":"capstone-project/exercises-system-integration","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/capstone-project/assessment-capstone","label":"Capstone Assessment Questions","docId":"capstone-project/assessment-capstone","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Appendices","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/hardware-requirements","label":"Hardware Requirements","docId":"hardware-requirements","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/software-requirements","label":"Software Requirements","docId":"software-requirements","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/performance-benchmarks","label":"Performance Benchmarks and System Specifications","docId":"performance-benchmarks","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/installation-guides","label":"Installation Guides","docId":"installation-guides","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/troubleshooting-guides","label":"Troubleshooting Common Setup Issues","docId":"troubleshooting-guides","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/robotics-glossary","label":"Robotics Glossary","docId":"robotics-glossary","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/references-further-reading","label":"References and Further Reading","docId":"references-further-reading","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"accessibility-features":{"id":"accessibility-features","title":"Accessibility Features","description":"Accessibility features and guidelines for the robotics book content"},"capstone-project/assessment-capstone":{"id":"capstone-project/assessment-capstone","title":"Capstone Assessment Questions","description":"Assessment questions for capstone project concepts","sidebar":"tutorialSidebar"},"capstone-project/autonomous-humanoid":{"id":"capstone-project/autonomous-humanoid","title":"Autonomous Humanoid Capstone Project","description":"A comprehensive capstone project integrating all concepts from the book","sidebar":"tutorialSidebar"},"capstone-project/capstone-architecture":{"id":"capstone-project/capstone-architecture","title":"Capstone Architecture Design","description":"Design of the capstone architecture integrating all modules","sidebar":"tutorialSidebar"},"capstone-project/exercises-system-integration":{"id":"capstone-project/exercises-system-integration","title":"Capstone Exercises for System Integration","description":"Exercises to understand system integration in the capstone project","sidebar":"tutorialSidebar"},"capstone-project/implementation-guide":{"id":"capstone-project/implementation-guide","title":"Capstone Implementation Guide","description":"Step-by-step guide for implementing the capstone project","sidebar":"tutorialSidebar"},"capstone-project/integration-examples":{"id":"capstone-project/integration-examples","title":"Capstone Code Examples","description":"Code examples for integrating all components without runtime","sidebar":"tutorialSidebar"},"capstone-project/troubleshooting-guide":{"id":"capstone-project/troubleshooting-guide","title":"Capstone Troubleshooting Guide","description":"Troubleshooting guide for common challenges in the capstone project","sidebar":"tutorialSidebar"},"content-analytics":{"id":"content-analytics","title":"Content Analytics and Reader Engagement","description":"Analytics strategies for tracking reader engagement and content performance in the robotics book"},"content-versioning":{"id":"content-versioning","title":"Content Versioning Strategy","description":"Strategy for managing content versions and updates in the robotics book"},"contributing":{"id":"contributing","title":"Content Contribution Guide","description":"A comprehensive guide for contributing content to the Physical AI & Humanoid Robotics book."},"final-review-editing":{"id":"final-review-editing","title":"Final Review and Editing","description":"Comprehensive review and editing process for the robotics book content"},"hardware-requirements":{"id":"hardware-requirements","title":"Hardware Requirements","description":"Detailed hardware specifications for workstations, edge platforms, sensors, and robot platforms.","sidebar":"tutorialSidebar"},"hardware-software-requirements":{"id":"hardware-software-requirements","title":"Hardware and Software Requirements","description":"Comprehensive guide to hardware and software requirements for implementing Physical AI & Humanoid Robotics concepts."},"installation-guides":{"id":"installation-guides","title":"Installation Guides","description":"Step-by-step setup instructions for all required components in Physical AI & Humanoid Robotics.","sidebar":"tutorialSidebar"},"interactive-playground":{"id":"interactive-playground","title":"Interactive Code Playground","description":"Interactive components for experimenting with robotics code examples"},"intro":{"id":"intro","title":"Redirect Notice","description":"You are viewing a legacy redirect page. Please navigate to the Preface for the book introduction."},"module-1-ros-fundamentals/assessments":{"id":"module-1-ros-fundamentals/assessments","title":"Assessment Questions: ROS 2 Fundamentals","description":"This section provides assessment questions to test your understanding of ROS 2 fundamentals."},"module-1-ros-fundamentals/best-practices":{"id":"module-1-ros-fundamentals/best-practices","title":"Best Practices for ROS 2 Development","description":"Following best practices in ROS 2 development is essential for creating robust, maintainable, and efficient robotic applications. This section outlines key principles and approaches for effective ROS 2 development."},"module-1-ros-fundamentals/diagrams":{"id":"module-1-ros-fundamentals/diagrams","title":"ROS 2 Architecture Diagrams","description":"This section describes key ROS 2 architecture diagrams that help visualize the concepts discussed in this module."},"module-1-ros-fundamentals/exercises":{"id":"module-1-ros-fundamentals/exercises","title":"Exercises: ROS 2 Fundamentals","description":"This section provides exercises to reinforce your understanding of ROS 2 fundamentals. These exercises are designed to help you apply the concepts learned in the previous chapters."},"module-1-ros-fundamentals/introduction-to-ros2":{"id":"module-1-ros-fundamentals/introduction-to-ros2","title":"Introduction to ROS 2","description":"ROS 2 (Robot Operating System 2) is a flexible framework for writing robot software. It\'s a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behavior across a wide variety of robot platforms.","sidebar":"tutorialSidebar"},"module-1-ros-fundamentals/launch-params":{"id":"module-1-ros-fundamentals/launch-params","title":"ROS 2 Launch Files and Parameter Management","description":"Launch files and parameter management are essential for configuring and starting complex robotic systems in ROS 2. This section covers how to create launch files and manage parameters effectively."},"module-1-ros-fundamentals/lifecycle-execution":{"id":"module-1-ros-fundamentals/lifecycle-execution","title":"ROS 2 Lifecycle and Execution Model","description":"Understanding the ROS 2 lifecycle and execution model is crucial for developing robust robotic applications. This section explains how ROS 2 nodes are managed, executed, and how they interact with the system."},"module-1-ros-fundamentals/nodes-topics-services":{"id":"module-1-ros-fundamentals/nodes-topics-services","title":"Nodes, Topics, and Services","description":"In ROS 2, the fundamental concepts of Nodes, Topics, and Services form the backbone of robot communication. Understanding these concepts is crucial for developing any robotic system.","sidebar":"tutorialSidebar"},"module-1-ros-fundamentals/rclpy-basics":{"id":"module-1-ros-fundamentals/rclpy-basics","title":"rclpy Basics","description":"rclpy is the Python client library for ROS 2. It provides the standard interface for Python programs to interact with ROS 2, allowing you to create nodes, publish and subscribe to topics, provide and use services, and more.","sidebar":"tutorialSidebar"},"module-1-ros-fundamentals/urdf-for-humanoids":{"id":"module-1-ros-fundamentals/urdf-for-humanoids","title":"URDF for Humanoids","description":"URDF (Unified Robot Description Format) is an XML format used in ROS to describe robot models. For humanoid robots, URDF is essential for defining the physical structure, kinematic properties, and visual representation of the robot.","sidebar":"tutorialSidebar"},"module-2-digital-twin/assessments":{"id":"module-2-digital-twin/assessments","title":"Assessment Questions: Gazebo Simulation","description":"This section provides assessment questions to test your understanding of Gazebo simulation concepts."},"module-2-digital-twin/best-practices":{"id":"module-2-digital-twin/best-practices","title":"Best Practices for Gazebo Simulation","description":"Following best practices in Gazebo simulation is essential for creating efficient, realistic, and maintainable robotic simulations. This section outlines key principles and approaches for effective Gazebo simulation development."},"module-2-digital-twin/diagrams":{"id":"module-2-digital-twin/diagrams","title":"Gazebo Simulation Architecture Diagrams","description":"This section describes key Gazebo simulation architecture diagrams that help visualize the concepts discussed in this module."},"module-2-digital-twin/exercises":{"id":"module-2-digital-twin/exercises","title":"Exercises: Gazebo Simulation","description":"This section provides exercises to reinforce your understanding of Gazebo simulation concepts. These exercises are designed to help you apply the simulation concepts learned in this module."},"module-2-digital-twin/gazebo-plugins":{"id":"module-2-digital-twin/gazebo-plugins","title":"Gazebo Plugin Development","description":"Gazebo plugins extend the simulator\'s functionality, enabling custom behaviors, sensors, and integration with external systems like ROS. This section covers the fundamentals of Gazebo plugin development with practical examples."},"module-2-digital-twin/gazebo-simulation":{"id":"module-2-digital-twin/gazebo-simulation","title":"Gazebo Simulation","description":"Gazebo is a powerful 3D simulation environment that plays a crucial role in robotics development. It provides realistic physics simulation, high-quality graphics, and convenient programmatic interfaces that make it ideal for testing robotic algorithms before deploying them to real hardware.","sidebar":"tutorialSidebar"},"module-2-digital-twin/physics-modeling":{"id":"module-2-digital-twin/physics-modeling","title":"Physics Modeling in Simulation","description":"Physics modeling is fundamental to creating realistic robot simulations in Gazebo. Accurate physics simulation enables robots to interact with their environment in ways that closely match real-world behavior, making simulation an invaluable tool for robot development and testing.","sidebar":"tutorialSidebar"},"module-2-digital-twin/sdf-format":{"id":"module-2-digital-twin/sdf-format","title":"Simulation Description Format (SDF)","description":"The Simulation Description Format (SDF) is an XML-based format used by Gazebo to describe simulation environments, robot models, and objects. SDF provides a complete description of the physics, visual properties, and plugins needed for simulation."},"module-2-digital-twin/sensor-processing":{"id":"module-2-digital-twin/sensor-processing","title":"Sensor Data Processing in Simulation","description":"Processing sensor data in simulation is a critical aspect of robotic systems. While simulated sensors provide idealized data, processing this data effectively requires understanding the characteristics and limitations of both the sensors and the simulated environment."},"module-2-digital-twin/sensor-simulation":{"id":"module-2-digital-twin/sensor-simulation","title":"Sensor Simulation","description":"Sensor simulation is a critical component of robotic simulation, enabling robots to perceive their environment in a realistic way. Accurate sensor simulation allows for testing perception algorithms, sensor fusion, and robot behaviors before deployment to real hardware.","sidebar":"tutorialSidebar"},"module-2-digital-twin/unity-integration":{"id":"module-2-digital-twin/unity-integration","title":"Unity Integration for High-Fidelity Visualization","description":"Unity is a powerful 3D development platform that can be used alongside traditional robotics simulation tools for high-fidelity visualization and human-robot interaction design. While Gazebo provides robust physics simulation, Unity excels at creating visually rich environments that can enhance the development and testing of humanoid robots.","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/assessments":{"id":"module-3-ai-robot-brain/assessments","title":"Assessment Questions: NVIDIA Isaac Platform","description":"This section provides assessment questions to test your understanding of NVIDIA Isaac platform concepts."},"module-3-ai-robot-brain/best-practices":{"id":"module-3-ai-robot-brain/best-practices","title":"Best Practices for NVIDIA Isaac Platform","description":"Following best practices in NVIDIA Isaac development is essential for creating efficient, robust, and maintainable robotic applications. This section outlines key principles and approaches for effective Isaac platform utilization."},"module-3-ai-robot-brain/diagrams":{"id":"module-3-ai-robot-brain/diagrams","title":"Isaac Platform Architecture Diagrams","description":"This section describes key NVIDIA Isaac platform architecture diagrams that help visualize the concepts discussed in this module."},"module-3-ai-robot-brain/exercises":{"id":"module-3-ai-robot-brain/exercises","title":"Exercises: NVIDIA Isaac Platform","description":"This section provides exercises to reinforce your understanding of NVIDIA Isaac platform concepts. These exercises are designed to help you apply the Isaac SDK, perception, and navigation concepts learned in this module."},"module-3-ai-robot-brain/isaac-ros-components":{"id":"module-3-ai-robot-brain/isaac-ros-components","title":"Isaac ROS Components and Python Code Examples","description":"NVIDIA Isaac ROS provides a collection of GPU-accelerated packages that enable efficient perception, navigation, and manipulation tasks for robotics applications. This section covers key Isaac ROS components with practical Python code examples."},"module-3-ai-robot-brain/isaac-sdk-overview":{"id":"module-3-ai-robot-brain/isaac-sdk-overview","title":"NVIDIA Isaac SDK Overview","description":"The NVIDIA Isaac SDK is a comprehensive software development kit designed for building, simulating, and deploying AI-powered robotics applications. It provides a complete ecosystem for developing advanced robotic systems with a focus on perception, navigation, and manipulation capabilities.","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/nav2-path-planning":{"id":"module-3-ai-robot-brain/nav2-path-planning","title":"Path Planning for Bipedal Humanoid Movement","description":"Path planning for bipedal humanoid robots presents unique challenges compared to wheeled or simpler mobile robots. Humanoid robots must navigate with dynamic balance, consider their complex kinematics, and plan paths that account for their distinctive locomotion patterns. NVIDIA Isaac provides specialized tools for this complex path planning task.","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/perception-algorithms":{"id":"module-3-ai-robot-brain/perception-algorithms","title":"Perception Algorithms in NVIDIA Isaac","description":"Perception algorithms form the core of robotic intelligence, enabling robots to understand and interpret their environment. NVIDIA Isaac provides GPU-accelerated implementations of state-of-the-art perception algorithms, making real-time processing of sensor data possible for complex robotic applications."},"module-3-ai-robot-brain/perception-pipelines":{"id":"module-3-ai-robot-brain/perception-pipelines","title":"Perception Pipelines in NVIDIA Isaac","description":"Perception pipelines in NVIDIA Isaac are designed to process sensor data using AI-powered algorithms to understand the robot\'s environment. These pipelines leverage NVIDIA\'s GPU acceleration to perform real-time perception tasks such as object detection, segmentation, and tracking.","sidebar":"tutorialSidebar"},"module-3-ai-robot-brain/synthetic-data":{"id":"module-3-ai-robot-brain/synthetic-data","title":"Synthetic Data Generation Concepts in Isaac Sim","description":"Synthetic data generation is a critical capability of Isaac Sim that enables the creation of large, diverse datasets for training AI models without the need for real-world data collection. This approach accelerates development and enables scenarios that would be difficult or dangerous to capture in the real world."},"module-3-ai-robot-brain/vslam-navigation":{"id":"module-3-ai-robot-brain/vslam-navigation","title":"Visual SLAM and Navigation in NVIDIA Isaac","description":"Visual SLAM (Simultaneous Localization and Mapping) and navigation are critical capabilities for autonomous robots. NVIDIA Isaac provides GPU-accelerated implementations of these algorithms, enabling robots to understand their position in the world and plan paths to navigate effectively.","sidebar":"tutorialSidebar"},"module-4-vision-language-action/assessment-questions":{"id":"module-4-vision-language-action/assessment-questions","title":"Assessment Questions for Conversational Robotics","description":"Assessment questions to evaluate understanding of conversational robotics concepts","sidebar":"tutorialSidebar"},"module-4-vision-language-action/exercises-cognitive-planning":{"id":"module-4-vision-language-action/exercises-cognitive-planning","title":"Exercises for Cognitive Planning","description":"Practical exercises to understand LLM-based cognitive planning in robotics","sidebar":"tutorialSidebar"},"module-4-vision-language-action/llm-cognitive-planning":{"id":"module-4-vision-language-action/llm-cognitive-planning","title":"LLM Cognitive Planning for Robotics","description":"Learn how to use Large Language Models for cognitive planning in robotics applications","sidebar":"tutorialSidebar"},"module-4-vision-language-action/llm-cognitive-planning-examples":{"id":"module-4-vision-language-action/llm-cognitive-planning-examples","title":"LLM Cognitive Planning Code Examples","description":"Practical code examples for implementing LLM-based cognitive planning in robotics","sidebar":"tutorialSidebar"},"module-4-vision-language-action/openai-whisper-examples":{"id":"module-4-vision-language-action/openai-whisper-examples","title":"OpenAI Whisper Integration Code Examples","description":"Practical code examples for integrating OpenAI Whisper in robotics applications","sidebar":"tutorialSidebar"},"module-4-vision-language-action/voice-to-action":{"id":"module-4-vision-language-action/voice-to-action","title":"Voice-to-Action Pipeline","description":"Learn how to create a complete pipeline from voice commands to robotic actions","sidebar":"tutorialSidebar"},"module-4-vision-language-action/whisper-integration":{"id":"module-4-vision-language-action/whisper-integration","title":"Whisper Integration for Speech Recognition","description":"Learn how to integrate OpenAI Whisper for voice command processing in robotics","sidebar":"tutorialSidebar"},"performance-benchmarks":{"id":"performance-benchmarks","title":"Performance Benchmarks and System Specifications","description":"Minimum and recommended performance standards for Physical AI & Humanoid Robotics applications.","sidebar":"tutorialSidebar"},"performance-optimization":{"id":"performance-optimization","title":"Performance Optimization","description":"Techniques for optimizing the performance of the robotics book platform and content"},"preface":{"id":"preface","title":"Preface","description":"Welcome to Physical AI & Humanoid Robotics: A Comprehensive Guide \u2013 your journey into the exciting frontier where artificial intelligence meets the physical world. This book bridges the gap between digital algorithms and embodied intelligence, preparing you to design, simulate, and deploy intelligent humanoid robots capable of natural human interactions.","sidebar":"tutorialSidebar"},"quickstart":{"id":"quickstart","title":"Quickstart Guide","description":"Get started with the Physical AI & Humanoid Robotics book in minutes."},"reader-feedback":{"id":"reader-feedback","title":"Reader Feedback System","description":"Mechanism for readers to report issues and provide feedback on the robotics book content"},"references-further-reading":{"id":"references-further-reading","title":"References and Further Reading","description":"Comprehensive list of references and resources for further learning in robotics","sidebar":"tutorialSidebar"},"responsive-design":{"id":"responsive-design","title":"Responsive Design for Mobile","description":"Responsive design principles and implementation for mobile viewing of the robotics book"},"robotics-glossary":{"id":"robotics-glossary","title":"Robotics Glossary","description":"Comprehensive glossary of robotics terms and definitions","sidebar":"tutorialSidebar"},"search-optimization":{"id":"search-optimization","title":"Search Optimization and Content Indexing","description":"Strategies for optimizing search and indexing robotics book content"},"software-requirements":{"id":"software-requirements","title":"Software Requirements","description":"Operating systems, development frameworks, and tools needed for Physical AI & Humanoid Robotics.","sidebar":"tutorialSidebar"},"troubleshooting-guides":{"id":"troubleshooting-guides","title":"Troubleshooting Common Setup Issues","description":"Solutions to common problems encountered during setup of Physical AI & Humanoid Robotics environment.","sidebar":"tutorialSidebar"},"tutorial-basics/congratulations":{"id":"tutorial-basics/congratulations","title":"Congratulations!","description":"You have just learned the basics of Docusaurus and made some changes to the initial template."},"tutorial-basics/create-a-blog-post":{"id":"tutorial-basics/create-a-blog-post","title":"Create a Blog Post","description":"Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed..."},"tutorial-basics/create-a-document":{"id":"tutorial-basics/create-a-document","title":"Create a Document","description":"Documents are groups of pages connected through:"},"tutorial-basics/create-a-page":{"id":"tutorial-basics/create-a-page","title":"Create a Page","description":"Add Markdown or React files to src/pages to create a standalone page:"},"tutorial-basics/deploy-your-site":{"id":"tutorial-basics/deploy-your-site","title":"Deploy your site","description":"Docusaurus is a static-site-generator (also called Jamstack)."},"tutorial-basics/markdown-features":{"id":"tutorial-basics/markdown-features","title":"Markdown Features","description":"Docusaurus supports Markdown and a few additional features."},"tutorial-extras/manage-docs-versions":{"id":"tutorial-extras/manage-docs-versions","title":"Manage Docs Versions","description":"Docusaurus can manage multiple versions of your docs."},"tutorial-extras/translate-your-site":{"id":"tutorial-extras/translate-your-site","title":"Translate your site","description":"Let\'s translate docs/intro.md to French."}}}}')}}]);